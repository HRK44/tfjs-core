{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/package.json","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1524062920943},{"name":"./util","loc":{"line":3,"column":19}}],"generated":{"js":"\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar util = require(\"./util\");\nfunction getFilteredNodesXToY(tape, xs, y) {\n    var tensorsFromX = {};\n    var nodesFromX = {};\n    for (var i = 0; i < xs.length; i++) {\n        tensorsFromX[xs[i].id] = true;\n    }\n    for (var i = 0; i < tape.length; i++) {\n        var node = tape[i];\n        var nodeInputs = node.inputs;\n        for (var inputName in nodeInputs) {\n            var input = nodeInputs[inputName];\n            var anyInputFromX = false;\n            for (var j = 0; j < xs.length; j++) {\n                if (tensorsFromX[input.id]) {\n                    tensorsFromX[node.output.id] = true;\n                    anyInputFromX = true;\n                    nodesFromX[node.id] = true;\n                    break;\n                }\n            }\n            if (anyInputFromX) {\n                break;\n            }\n        }\n    }\n    var tensorsLeadToY = {};\n    tensorsLeadToY[y.id] = true;\n    var nodesToY = {};\n    for (var i = tape.length - 1; i >= 0; i--) {\n        var node = tape[i];\n        var nodeInputs = node.inputs;\n        var outputs = [];\n        outputs.push(node.output);\n        for (var j = 0; j < outputs.length; j++) {\n            if (tensorsLeadToY[outputs[j].id]) {\n                for (var inputName in nodeInputs) {\n                    tensorsLeadToY[nodeInputs[inputName].id] = true;\n                    nodesToY[node.id] = true;\n                }\n                break;\n            }\n        }\n    }\n    var filteredTape = [];\n    for (var i = 0; i < tape.length; i++) {\n        var node = tape[i];\n        if (nodesFromX[node.id] && nodesToY[node.id]) {\n            var prunedInputs = {};\n            for (var inputName in node.inputs) {\n                var nodeInput = node.inputs[inputName];\n                if (tensorsFromX[nodeInput.id]) {\n                    prunedInputs[inputName] = nodeInput;\n                }\n            }\n            var prunedNode = Object.assign({}, node);\n            prunedNode.inputs = prunedInputs;\n            prunedNode.output = node.output;\n            filteredTape.push(prunedNode);\n        }\n    }\n    return filteredTape;\n}\nexports.getFilteredNodesXToY = getFilteredNodesXToY;\nfunction backpropagateGradients(tensorAccumulatedGradientMap, filteredTape) {\n    for (var i = filteredTape.length - 1; i >= 0; i--) {\n        var node = filteredTape[i];\n        var dy = tensorAccumulatedGradientMap[node.output.id];\n        if (node.gradient == null) {\n            throw new Error(\"Cannot compute gradient: gradient function not found \" +\n                (\"for \" + node.name + \".\"));\n        }\n        var inputGradients = node.gradient(dy);\n        for (var inputName in node.inputs) {\n            if (!(inputName in inputGradients)) {\n                throw new Error(\"Cannot backprop through input \" + inputName + \". \" +\n                    (\"Available gradients found: \" + Object.keys(inputGradients) + \".\"));\n            }\n            var dx = inputGradients[inputName]();\n            var x = node.inputs[inputName];\n            if (!util.arraysEqual(dx.shape, x.shape)) {\n                throw new Error(\"Error in gradient for op \" + node.name + \". The gradient of input \" +\n                    (\"'\" + inputName + \"' has shape '\" + dx.shape + \"', which does not match \") +\n                    (\"the shape of the input '\" + x.shape + \"'\"));\n            }\n            if (tensorAccumulatedGradientMap[x.id] == null) {\n                tensorAccumulatedGradientMap[x.id] = dx;\n            }\n            else {\n                var curGradient = tensorAccumulatedGradientMap[x.id];\n                tensorAccumulatedGradientMap[x.id] = curGradient.add(dx);\n                curGradient.dispose();\n            }\n        }\n    }\n}\nexports.backpropagateGradients = backpropagateGradients;\n","map":{"version":3,"file":"tape.js","sourceRoot":"","sources":["../src/tape.ts"],"names":[],"mappings":";;AAmBA,6BAA+B;AAsB/B,8BACI,IAAgB,EAAE,EAAY,EAAE,CAAS;IAG3C,IAAM,YAAY,GAAkC,EAAE,CAAC;IACvD,IAAM,UAAU,GAAgC,EAAE,CAAC;IACnD,GAAG,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;QACnC,YAAY,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC;IAChC,CAAC;IAED,GAAG,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;QACrC,IAAM,IAAI,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;QACrB,IAAM,UAAU,GAAG,IAAI,CAAC,MAAM,CAAC;QAC/B,GAAG,CAAC,CAAC,IAAM,SAAS,IAAI,UAAU,CAAC,CAAC,CAAC;YACnC,IAAM,KAAK,GAAG,UAAU,CAAC,SAAS,CAAC,CAAC;YAEpC,IAAI,aAAa,GAAG,KAAK,CAAC;YAC1B,GAAG,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;gBACnC,EAAE,CAAC,CAAC,YAAY,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;oBAC3B,YAAY,CAAC,IAAI,CAAC,MAAM,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC;oBACpC,aAAa,GAAG,IAAI,CAAC;oBACrB,UAAU,CAAC,IAAI,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC;oBAC3B,KAAK,CAAC;gBACR,CAAC;YACH,CAAC;YAED,EAAE,CAAC,CAAC,aAAa,CAAC,CAAC,CAAC;gBAClB,KAAK,CAAC;YACR,CAAC;QACH,CAAC;IACH,CAAC;IAGD,IAAM,cAAc,GAAkC,EAAE,CAAC;IACzD,cAAc,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC;IAC5B,IAAM,QAAQ,GAAgC,EAAE,CAAC;IAEjD,GAAG,CAAC,CAAC,IAAI,CAAC,GAAG,IAAI,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC,IAAI,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC;QAC1C,IAAM,IAAI,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;QACrB,IAAM,UAAU,GAAG,IAAI,CAAC,MAAM,CAAC;QAE/B,IAAM,OAAO,GAAa,EAAE,CAAC;QAC7B,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;QAG1B,GAAG,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,OAAO,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;YACxC,EAAE,CAAC,CAAC,cAAc,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;gBAClC,GAAG,CAAC,CAAC,IAAM,SAAS,IAAI,UAAU,CAAC,CAAC,CAAC;oBACnC,cAAc,CAAC,UAAU,CAAC,SAAS,CAAC,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC;oBAChD,QAAQ,CAAC,IAAI,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC;gBAC3B,CAAC;gBACD,KAAK,CAAC;YACR,CAAC;QACH,CAAC;IACH,CAAC;IAGD,IAAM,YAAY,GAAe,EAAE,CAAC;IACpC,GAAG,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;QACrC,IAAM,IAAI,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;QAErB,EAAE,CAAC,CAAC,UAAU,CAAC,IAAI,CAAC,EAAE,CAAC,IAAI,QAAQ,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;YAE7C,IAAM,YAAY,GAAkC,EAAE,CAAC;YACvD,GAAG,CAAC,CAAC,IAAM,SAAS,IAAI,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC;gBACpC,IAAM,SAAS,GAAG,IAAI,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC;gBACzC,EAAE,CAAC,CAAC,YAAY,CAAC,SAAS,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;oBAC/B,YAAY,CAAC,SAAS,CAAC,GAAG,SAAS,CAAC;gBACtC,CAAC;YACH,CAAC;YAGD,IAAM,UAAU,GAAG,MAAM,CAAC,MAAM,CAAC,EAAE,EAAE,IAAI,CAAa,CAAC;YACvD,UAAU,CAAC,MAAM,GAAG,YAAY,CAAC;YACjC,UAAU,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,CAAC;YAEhC,YAAY,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;QAChC,CAAC;IACH,CAAC;IAED,MAAM,CAAC,YAAY,CAAC;AACtB,CAAC;AAjFD,oDAiFC;AAQD,gCACI,4BAA0D,EAC1D,YAAwB;IAE1B,GAAG,CAAC,CAAC,IAAI,CAAC,GAAG,YAAY,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC,IAAI,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC;QAClD,IAAM,IAAI,GAAG,YAAY,CAAC,CAAC,CAAC,CAAC;QAE7B,IAAM,EAAE,GAAG,4BAA4B,CAAC,IAAI,CAAC,MAAM,CAAC,EAAE,CAAC,CAAC;QAExD,EAAE,CAAC,CAAC,IAAI,CAAC,QAAQ,IAAI,IAAI,CAAC,CAAC,CAAC;YAC1B,MAAM,IAAI,KAAK,CACX,uDAAuD;iBACvD,SAAO,IAAI,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAC3B,CAAC;QAGD,IAAM,cAAc,GAAG,IAAI,CAAC,QAAQ,CAAC,EAAE,CAAC,CAAC;QACzC,GAAG,CAAC,CAAC,IAAM,SAAS,IAAI,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC;YACpC,EAAE,CAAC,CAAC,CAAC,CAAC,SAAS,IAAI,cAAc,CAAC,CAAC,CAAC,CAAC;gBACnC,MAAM,IAAI,KAAK,CACX,mCAAiC,SAAS,OAAI;qBAC9C,gCAA8B,MAAM,CAAC,IAAI,CAAC,cAAc,CAAC,MAAG,CAAA,CAAC,CAAC;YACpE,CAAC;YAGD,IAAM,EAAE,GAAG,cAAc,CAAC,SAAS,CAAC,EAAE,CAAC;YACvC,IAAM,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC;YACjC,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,WAAW,CAAC,EAAE,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;gBACzC,MAAM,IAAI,KAAK,CACX,8BAA4B,IAAI,CAAC,IAAI,6BAA0B;qBAC/D,MAAI,SAAS,qBAAgB,EAAE,CAAC,KAAK,6BAA0B,CAAA;qBAC/D,6BAA2B,CAAC,CAAC,KAAK,MAAG,CAAA,CAAC,CAAC;YAC7C,CAAC;YAED,EAAE,CAAC,CAAC,4BAA4B,CAAC,CAAC,CAAC,EAAE,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC;gBAC/C,4BAA4B,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG,EAAE,CAAC;YAC1C,CAAC;YAAC,IAAI,CAAC,CAAC;gBACN,IAAM,WAAW,GAAG,4BAA4B,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC;gBACvD,4BAA4B,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG,WAAW,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC;gBACzD,WAAW,CAAC,OAAO,EAAE,CAAC;YACxB,CAAC;QACH,CAAC;IACH,CAAC;AACH,CAAC;AA3CD,wDA2CC","sourcesContent":["/**\n * @license\n * Copyright 2017 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Tensor} from './tensor';\nimport {NamedTensorMap} from './types';\nimport * as util from './util';\n\nexport interface TapeNode {\n  id: number;\n  name: string;\n  output: Tensor;\n  inputs: NamedTensorMap;\n  // Optional params, defined only for ops with gradient impl.\n  gradient?: (dy: Tensor|NamedTensorMap) => NamedGradientMap;\n}\n\nexport type NamedGradientMap = {\n  [inputName: string]: () => Tensor;\n};\n\n/**\n * Computes a list of TapeNodes that connect x to y, filtering everything else\n * out and preserving the order of the original tape elements.\n * @param tape The tape elements to filter.\n * @param xs The input Tensors.\n * @param y The output Tensor.\n */\nexport function getFilteredNodesXToY(\n    tape: TapeNode[], xs: Tensor[], y: Tensor): TapeNode[] {\n  // Forward pass to compute all the nodes and Tensors that are transitively a\n  // function of x.\n  const tensorsFromX: {[tensorId: number]: boolean} = {};\n  const nodesFromX: {[nodeId: number]: boolean} = {};\n  for (let i = 0; i < xs.length; i++) {\n    tensorsFromX[xs[i].id] = true;\n  }\n\n  for (let i = 0; i < tape.length; i++) {\n    const node = tape[i];\n    const nodeInputs = node.inputs;\n    for (const inputName in nodeInputs) {\n      const input = nodeInputs[inputName];\n\n      let anyInputFromX = false;\n      for (let j = 0; j < xs.length; j++) {\n        if (tensorsFromX[input.id]) {\n          tensorsFromX[node.output.id] = true;\n          anyInputFromX = true;\n          nodesFromX[node.id] = true;\n          break;\n        }\n      }\n\n      if (anyInputFromX) {\n        break;\n      }\n    }\n  }\n\n  // Backwards pass to find all of the nodes and Tensors that lead to y.\n  const tensorsLeadToY: {[tensorId: number]: boolean} = {};\n  tensorsLeadToY[y.id] = true;\n  const nodesToY: {[nodeId: number]: boolean} = {};\n\n  for (let i = tape.length - 1; i >= 0; i--) {\n    const node = tape[i];\n    const nodeInputs = node.inputs;\n\n    const outputs: Tensor[] = [];\n    outputs.push(node.output);\n\n    // If any of the outputs lead to y, mark all of the inputs as leading to y.\n    for (let j = 0; j < outputs.length; j++) {\n      if (tensorsLeadToY[outputs[j].id]) {\n        for (const inputName in nodeInputs) {\n          tensorsLeadToY[nodeInputs[inputName].id] = true;\n          nodesToY[node.id] = true;\n        }\n        break;\n      }\n    }\n  }\n\n  // Return the paths that come from x and lead to y.\n  const filteredTape: TapeNode[] = [];\n  for (let i = 0; i < tape.length; i++) {\n    const node = tape[i];\n\n    if (nodesFromX[node.id] && nodesToY[node.id]) {\n      // Prune the inputs from the node that aren't a function of x.\n      const prunedInputs: {[inputName: string]: Tensor} = {};\n      for (const inputName in node.inputs) {\n        const nodeInput = node.inputs[inputName];\n        if (tensorsFromX[nodeInput.id]) {\n          prunedInputs[inputName] = nodeInput;\n        }\n      }\n\n      // Copy the node and overwrite inputsAndArgs to the pruned version.\n      const prunedNode = Object.assign({}, node) as TapeNode;\n      prunedNode.inputs = prunedInputs;\n      prunedNode.output = node.output;\n\n      filteredTape.push(prunedNode);\n    }\n  }\n\n  return filteredTape;\n}\n\n/**\n * Backpropagate gradients through the filtered TapeNodes.\n * @param tensorAccumulatedGradientMap A map of Tensor to its gradient. This map\n * is mutated by this method.\n * @param filteredTape The filtered TapeNodes to backprop through.\n */\nexport function backpropagateGradients(\n    tensorAccumulatedGradientMap: {[tensorId: number]: Tensor},\n    filteredTape: TapeNode[]) {\n  // Walk the tape backwards and keep a map of Tensor to its gradient.\n  for (let i = filteredTape.length - 1; i >= 0; i--) {\n    const node = filteredTape[i];\n\n    const dy = tensorAccumulatedGradientMap[node.output.id];\n\n    if (node.gradient == null) {\n      throw new Error(\n          `Cannot compute gradient: gradient function not found ` +\n          `for ${node.name}.`);\n    }\n\n    // Backprop dy through this node and accumulate gradients over the inputs.\n    const inputGradients = node.gradient(dy);\n    for (const inputName in node.inputs) {\n      if (!(inputName in inputGradients)) {\n        throw new Error(\n            `Cannot backprop through input ${inputName}. ` +\n            `Available gradients found: ${Object.keys(inputGradients)}.`);\n      }\n\n      // Call the gradient function.\n      const dx = inputGradients[inputName]();\n      const x = node.inputs[inputName];\n      if (!util.arraysEqual(dx.shape, x.shape)) {\n        throw new Error(\n            `Error in gradient for op ${node.name}. The gradient of input ` +\n            `'${inputName}' has shape '${dx.shape}', which does not match ` +\n            `the shape of the input '${x.shape}'`);\n      }\n\n      if (tensorAccumulatedGradientMap[x.id] == null) {\n        tensorAccumulatedGradientMap[x.id] = dx;\n      } else {\n        const curGradient = tensorAccumulatedGradientMap[x.id];\n        tensorAccumulatedGradientMap[x.id] = curGradient.add(dx);\n        curGradient.dispose();\n      }\n    }\n  }\n}\n"]}},"hash":"94e1b1e121d62d4eb8733124cb09ea88","cacheData":{"env":{}}}