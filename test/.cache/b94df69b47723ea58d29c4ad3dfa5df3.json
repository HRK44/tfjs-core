{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/package.json","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1524062920943},{"name":"./doc","loc":{"line":9,"column":20}},{"name":"./environment","loc":{"line":10,"column":28}},{"name":"./globals","loc":{"line":11,"column":24}},{"name":"./tensor","loc":{"line":12,"column":23}},{"name":"./util","loc":{"line":13,"column":19}}],"generated":{"js":"\"use strict\";\nvar __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar doc_1 = require(\"./doc\");\nvar environment_1 = require(\"./environment\");\nvar globals_1 = require(\"./globals\");\nvar tensor_1 = require(\"./tensor\");\nvar util = require(\"./util\");\nvar Gradients = (function () {\n    function Gradients() {\n    }\n    Gradients.gradScope = function (nameOrScopeFn, scopeFn) {\n        return globals_1.tidy(nameOrScopeFn, scopeFn, true);\n    };\n    Gradients.grad = function (f) {\n        util.assert(util.isFunction(f), 'The f passed in grad(f) must be a function');\n        return function (x, dy) {\n            util.assert(x instanceof tensor_1.Tensor, 'The x passed in grad(f)(x) must be a tensor');\n            util.assert(dy == null || dy instanceof tensor_1.Tensor, 'The dy passed in grad(f)(x, dy) must be a tensor');\n            var _a = environment_1.ENV.engine.gradients(function () { return f(x); }, [x], dy), value = _a.value, grads = _a.grads;\n            if (dy != null) {\n                util.assertShapesMatch(value.shape, dy.shape, 'The shape of dy passed in grad(f)(x, dy) must match the shape ' +\n                    'returned by f(x)');\n            }\n            value.dispose();\n            checkGrads(grads);\n            return grads[0];\n        };\n    };\n    Gradients.grads = function (f) {\n        util.assert(util.isFunction(f), 'The f passed in grads(f) must be a function');\n        return function (args, dy) {\n            util.assert(Array.isArray(args) && args.every(function (arg) { return arg instanceof tensor_1.Tensor; }), 'The args passed in grads(f)(args) must be an array of tensors');\n            util.assert(dy == null || dy instanceof tensor_1.Tensor, 'The dy passed in grads(f)(args, dy) must be a tensor');\n            var _a = environment_1.ENV.engine.gradients(function () { return f.apply(void 0, args); }, args, dy), value = _a.value, grads = _a.grads;\n            if (dy != null) {\n                util.assertShapesMatch(value.shape, dy.shape, 'The shape of dy passed in grads(f)([x1,...], dy) must match the ' +\n                    'shape returned by f([x1,...])');\n            }\n            value.dispose();\n            checkGrads(grads);\n            return grads;\n        };\n    };\n    Gradients.valueAndGrad = function (f) {\n        util.assert(util.isFunction(f), 'The f passed in valueAndGrad(f) must be a function');\n        return function (x, dy) {\n            util.assert(x instanceof tensor_1.Tensor, 'The x passed in valueAndGrad(f)(x) must be a tensor');\n            util.assert(dy == null || dy instanceof tensor_1.Tensor, 'The dy passed in valueAndGrad(f)(x, dy) must be a tensor');\n            var _a = environment_1.ENV.engine.gradients(function () { return f(x); }, [x], dy), grads = _a.grads, value = _a.value;\n            checkGrads(grads);\n            return { grad: grads[0], value: value };\n        };\n    };\n    Gradients.valueAndGrads = function (f) {\n        util.assert(util.isFunction(f), 'The f passed in valueAndGrads(f) must be a function');\n        return function (args, dy) {\n            util.assert(Array.isArray(args) && args.every(function (arg) { return arg instanceof tensor_1.Tensor; }), 'The args passed in valueAndGrads(f)(args) must be array of tensors');\n            util.assert(dy == null || dy instanceof tensor_1.Tensor, 'The dy passed in valueAndGrads(f)(args, dy) must be a tensor');\n            var res = environment_1.ENV.engine.gradients(function () { return f.apply(void 0, args); }, args, dy);\n            if (dy != null) {\n                util.assertShapesMatch(res.value.shape, dy.shape, 'The shape of dy passed in valueAndGrads(f)([x1,...], dy) must ' +\n                    'match the shape returned by f([x1,...])');\n            }\n            checkGrads(res.grads);\n            return res;\n        };\n    };\n    Gradients.variableGrads = function (f, varList) {\n        util.assert(util.isFunction(f), 'The f passed in variableGrads(f) must be a function');\n        util.assert(varList == null ||\n            Array.isArray(varList) && varList.every(function (v) { return v instanceof tensor_1.Variable; }), 'The varList passed in variableGrads(f, varList) must be an array ' +\n            'of variables');\n        if (varList == null) {\n            varList = [];\n            for (var varName in environment_1.ENV.engine.registeredVariables) {\n                varList.push(environment_1.ENV.engine.registeredVariables[varName]);\n            }\n        }\n        var originalVarCount = varList.length;\n        varList = varList.filter(function (variable) { return variable.trainable; });\n        util.assert(varList.length > 0, \"variableGrads() expects at least one of the input variables to be \" +\n            (\"trainable, but none of the \" + originalVarCount + \" variables is \") +\n            \"trainable.\");\n        var allowNoGradients = true;\n        var _a = environment_1.ENV.engine.gradients(f, varList, null, allowNoGradients), value = _a.value, grads = _a.grads;\n        util.assert(grads.some(function (g) { return g != null; }), 'Cannot find a connection between any variable and the result of the ' +\n            'loss function y=f(x). Please make sure the operations that use ' +\n            'variables are inside the function f passed to minimize().');\n        util.assert(value.rank === 0, \"The f passed in variableGrads(f) must return a scalar, but it \" +\n            (\"returned a rank-\" + value.rank + \" tensor\"));\n        var namedGrads = {};\n        varList.forEach(function (v, i) {\n            if (grads[i] != null) {\n                namedGrads[v.name] = grads[i];\n            }\n        });\n        return { value: value, grads: namedGrads };\n    };\n    Gradients.customGrad = function (f) {\n        return environment_1.ENV.engine.customGrad(f);\n    };\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Gradients' })\n    ], Gradients, \"grad\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Gradients' })\n    ], Gradients, \"grads\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Gradients' })\n    ], Gradients, \"valueAndGrad\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Gradients' })\n    ], Gradients, \"valueAndGrads\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Gradients' })\n    ], Gradients, \"variableGrads\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Gradients' })\n    ], Gradients, \"customGrad\", null);\n    return Gradients;\n}());\nexports.Gradients = Gradients;\nfunction checkGrads(grads) {\n    var numNullGradients = grads.filter(function (g) { return g == null; }).length;\n    if (numNullGradients > 0) {\n        throw new Error(\"Cannot compute gradient of y=f(x) with respect to x. Make sure that\\n    the f you passed encloses all operations that lead from x to y.\");\n    }\n}\n","map":{"version":3,"file":"gradients.js","sourceRoot":"","sources":["../src/gradients.ts"],"names":[],"mappings":";;;;;;;;AAiBA,6BAA0B;AAG1B,6CAAkC;AAClC,qCAA+B;AAC/B,mCAAkD;AAElD,6BAA+B;AAE/B;IAAA;IA2UA,CAAC;IA/TQ,mBAAS,GAAhB,UACI,aAAgC,EAAE,OAAoB;QACxD,MAAM,CAAC,cAAI,CAAC,aAAa,EAAE,OAAO,EAAE,IAAI,CAAiB,CAAC;IAC5D,CAAC;IAmCM,cAAI,GAAX,UAAgD,CAAc;QAE5D,IAAI,CAAC,MAAM,CACP,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC,EAAE,4CAA4C,CAAC,CAAC;QACtE,MAAM,CAAC,UAAC,CAAI,EAAE,EAAM;YAClB,IAAI,CAAC,MAAM,CACP,CAAC,YAAY,eAAM,EAAE,6CAA6C,CAAC,CAAC;YACxE,IAAI,CAAC,MAAM,CACP,EAAE,IAAI,IAAI,IAAI,EAAE,YAAY,eAAM,EAClC,kDAAkD,CAAC,CAAC;YAClD,IAAA,8EAA0D,EAAzD,gBAAK,EAAE,gBAAK,CAA8C;YACjE,EAAE,CAAC,CAAC,EAAE,IAAI,IAAI,CAAC,CAAC,CAAC;gBACf,IAAI,CAAC,iBAAiB,CAClB,KAAK,CAAC,KAAK,EAAE,EAAE,CAAC,KAAK,EACrB,gEAAgE;oBAC5D,kBAAkB,CAAC,CAAC;YAC9B,CAAC;YACD,KAAK,CAAC,OAAO,EAAE,CAAC;YAChB,UAAU,CAAC,KAAK,CAAC,CAAC;YAClB,MAAM,CAAC,KAAK,CAAC,CAAC,CAAM,CAAC;QACvB,CAAC,CAAC;IACJ,CAAC;IA8BM,eAAK,GAAZ,UAA+B,CAA2B;QAExD,IAAI,CAAC,MAAM,CACP,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC,EAAE,6CAA6C,CAAC,CAAC;QACvE,MAAM,CAAC,UAAC,IAAc,EAAE,EAAM;YAC5B,IAAI,CAAC,MAAM,CACP,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,IAAI,CAAC,KAAK,CAAC,UAAA,GAAG,IAAI,OAAA,GAAG,YAAY,eAAM,EAArB,CAAqB,CAAC,EAC/D,+DAA+D,CAAC,CAAC;YACrE,IAAI,CAAC,MAAM,CACP,EAAE,IAAI,IAAI,IAAI,EAAE,YAAY,eAAM,EAClC,sDAAsD,CAAC,CAAC;YACtD,IAAA,gGAAiE,EAAhE,gBAAK,EAAE,gBAAK,CAAqD;YACxE,EAAE,CAAC,CAAC,EAAE,IAAI,IAAI,CAAC,CAAC,CAAC;gBACf,IAAI,CAAC,iBAAiB,CAClB,KAAK,CAAC,KAAK,EAAE,EAAE,CAAC,KAAK,EACrB,kEAAkE;oBAC9D,+BAA+B,CAAC,CAAC;YAC3C,CAAC;YACD,KAAK,CAAC,OAAO,EAAE,CAAC;YAChB,UAAU,CAAC,KAAK,CAAC,CAAC;YAClB,MAAM,CAAC,KAAK,CAAC;QACf,CAAC,CAAC;IACJ,CAAC;IA0BM,sBAAY,GAAnB,UAAwD,CAAc;QAKpE,IAAI,CAAC,MAAM,CACP,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC,EAClB,oDAAoD,CAAC,CAAC;QAC1D,MAAM,CAAC,UAAC,CAAI,EAAE,EAAM;YAClB,IAAI,CAAC,MAAM,CACP,CAAC,YAAY,eAAM,EACnB,qDAAqD,CAAC,CAAC;YAC3D,IAAI,CAAC,MAAM,CACP,EAAE,IAAI,IAAI,IAAI,EAAE,YAAY,eAAM,EAClC,0DAA0D,CAAC,CAAC;YAC1D,IAAA,8EAA0D,EAAzD,gBAAK,EAAE,gBAAK,CAA8C;YACjE,UAAU,CAAC,KAAK,CAAC,CAAC;YAClB,MAAM,CAAC,EAAC,IAAI,EAAE,KAAK,CAAC,CAAC,CAAM,EAAE,KAAK,EAAE,KAAU,EAAC,CAAC;QAClD,CAAC,CAAC;IACJ,CAAC;IAgCM,uBAAa,GAApB,UAAuC,CAA2B;QAKhE,IAAI,CAAC,MAAM,CACP,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC,EAClB,qDAAqD,CAAC,CAAC;QAC3D,MAAM,CAAC,UAAC,IAAc,EAAE,EAAM;YAC5B,IAAI,CAAC,MAAM,CACP,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,IAAI,CAAC,KAAK,CAAC,UAAA,GAAG,IAAI,OAAA,GAAG,YAAY,eAAM,EAArB,CAAqB,CAAC,EAC/D,oEAAoE,CAAC,CAAC;YAC1E,IAAI,CAAC,MAAM,CACP,EAAE,IAAI,IAAI,IAAI,EAAE,YAAY,eAAM,EAClC,8DAA8D,CAAC,CAAC;YACpE,IAAM,GAAG,GAAG,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,cAAM,OAAA,CAAC,eAAI,IAAI,GAAT,CAAU,EAAE,IAAI,EAAE,EAAE,CAAC,CAAC;YAC7D,EAAE,CAAC,CAAC,EAAE,IAAI,IAAI,CAAC,CAAC,CAAC;gBACf,IAAI,CAAC,iBAAiB,CAClB,GAAG,CAAC,KAAK,CAAC,KAAK,EAAE,EAAE,CAAC,KAAK,EACzB,gEAAgE;oBAC5D,yCAAyC,CAAC,CAAC;YACrD,CAAC;YACD,UAAU,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC;YACtB,MAAM,CAAC,GAAG,CAAC;QACb,CAAC,CAAC;IACJ,CAAC;IAwBM,uBAAa,GAApB,UAAqB,CAAe,EAAE,OAAoB;QAExD,IAAI,CAAC,MAAM,CACP,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC,EAClB,qDAAqD,CAAC,CAAC;QAC3D,IAAI,CAAC,MAAM,CACP,OAAO,IAAI,IAAI;YACX,KAAK,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,OAAO,CAAC,KAAK,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,YAAY,iBAAQ,EAArB,CAAqB,CAAC,EACvE,mEAAmE;YAC/D,cAAc,CAAC,CAAC;QACxB,EAAE,CAAC,CAAC,OAAO,IAAI,IAAI,CAAC,CAAC,CAAC;YAEpB,OAAO,GAAG,EAAE,CAAC;YACb,GAAG,CAAC,CAAC,IAAM,OAAO,IAAI,iBAAG,CAAC,MAAM,CAAC,mBAAmB,CAAC,CAAC,CAAC;gBACrD,OAAO,CAAC,IAAI,CAAC,iBAAG,CAAC,MAAM,CAAC,mBAAmB,CAAC,OAAO,CAAC,CAAC,CAAC;YACxD,CAAC;QACH,CAAC;QAED,IAAM,gBAAgB,GAAG,OAAO,CAAC,MAAM,CAAC;QACxC,OAAO,GAAG,OAAO,CAAC,MAAM,CAAC,UAAA,QAAQ,IAAI,OAAA,QAAQ,CAAC,SAAS,EAAlB,CAAkB,CAAC,CAAC;QACzD,IAAI,CAAC,MAAM,CACP,OAAO,CAAC,MAAM,GAAG,CAAC,EAClB,oEAAoE;aAChE,gCAA8B,gBAAgB,mBAAgB,CAAA;YAC9D,YAAY,CAAC,CAAC;QAEtB,IAAM,gBAAgB,GAAG,IAAI,CAAC;QACxB,IAAA,2EACsD,EADrD,gBAAK,EAAE,gBAAK,CAC0C;QAE7D,IAAI,CAAC,MAAM,CACP,KAAK,CAAC,IAAI,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,IAAI,IAAI,EAAT,CAAS,CAAC,EAC1B,sEAAsE;YAClE,iEAAiE;YACjE,2DAA2D,CAAC,CAAC;QACrE,IAAI,CAAC,MAAM,CACP,KAAK,CAAC,IAAI,KAAK,CAAC,EAChB,gEAAgE;aAC5D,qBAAmB,KAAK,CAAC,IAAI,YAAS,CAAA,CAAC,CAAC;QAEhD,IAAM,UAAU,GAAmB,EAAE,CAAC;QACtC,OAAO,CAAC,OAAO,CAAC,UAAC,CAAC,EAAE,CAAC;YACnB,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC;gBACrB,UAAU,CAAC,CAAC,CAAC,IAAI,CAAC,GAAG,KAAK,CAAC,CAAC,CAAC,CAAC;YAChC,CAAC;QACH,CAAC,CAAC,CAAC;QACH,MAAM,CAAC,EAAC,KAAK,OAAA,EAAE,KAAK,EAAE,UAAU,EAAC,CAAC;IACpC,CAAC;IA+BM,oBAAU,GAAjB,UAAoC,CAAwB;QAE1D,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC;IAClC,CAAC;IAxRD;QADC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,WAAW,EAAC,CAAC;+BAsBnD;IA8BD;QADC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,WAAW,EAAC,CAAC;gCAuBnD;IA0BD;QADC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,WAAW,EAAC,CAAC;uCAoBnD;IAgCD;QADC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,WAAW,EAAC,CAAC;wCA0BnD;IAwBD;QADC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,WAAW,EAAC,CAAC;wCAgDnD;IA+BD;QADC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,WAAW,EAAC,CAAC;qCAInD;IACH,gBAAC;CAAA,AA3UD,IA2UC;AA3UY,8BAAS;AA6UtB,oBAAoB,KAAe;IACjC,IAAM,gBAAgB,GAAG,KAAK,CAAC,MAAM,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,IAAI,IAAI,EAAT,CAAS,CAAC,CAAC,MAAM,CAAC;IAC7D,EAAE,CAAC,CAAC,gBAAgB,GAAG,CAAC,CAAC,CAAC,CAAC;QACzB,MAAM,IAAI,KAAK,CACX,0IAC4D,CAAC,CAAC;IACpE,CAAC;AACH,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {doc} from './doc';\nimport {CustomGradientFunc} from './engine';\nimport {ScopeFn} from './engine';\nimport {ENV} from './environment';\nimport {tidy} from './globals';\nimport {Scalar, Tensor, Variable} from './tensor';\nimport {NamedTensorMap, TensorContainer} from './types';\nimport * as util from './util';\n\nexport class Gradients {\n  /**\n   * Create a new gradient scope. Similar to scope, but forces all inner scopes\n   * to not clean up so that gradient operations can be used inside of this\n   * scope.\n   * @param nameOrScopeFn The name of the scope, or the function to execute.\n   *     If a name is provided, the 2nd argument should be the function.\n   *     If a name is provided, and debug mode is on, the timing and the memory\n   *     usage of the function will be tracked and displayed on the console\n   *     using the provided name.\n   * @param scopeFn The function to execute.\n   */\n  static gradScope<T extends TensorContainer>(\n      nameOrScopeFn: string|ScopeFn<T>, scopeFn?: ScopeFn<T>): T {\n    return tidy(nameOrScopeFn, scopeFn, true /* gradScope */);\n  }\n\n  /**\n   * Provided `f(x)`, returns another function `g(x, dy?)`, which gives the\n   * gradient of `f(x)` with respect to `x`.\n   *\n   * If `dy` is provided, the gradient of `f(x).mul(dy).sum()` with respect to\n   * `x` is computed instead. `f(x)` must take a single tensor `x` and return a\n   * single tensor `y`. If `f()` takes multiple inputs, use `grads` instead.\n   *\n   * ```js\n   * // f(x) = x ^ 2\n   * const f = x => x.square();\n   * // f'(x) = 2x\n   * const g = tf.grad(f);\n   *\n   * const x = tf.tensor1d([2, 3]);\n   * g(x).print();\n   * ```\n   *\n   * ```js\n   * // f(x) = x ^ 3\n   * const f = x => x.pow(tf.scalar(3, 'int32'));\n   * // f'(x) = 3x ^ 2\n   * const g = tf.grad(f);\n   * // f''(x) = 6x\n   * const gg = tf.grad(g);\n   *\n   * const x = tf.tensor1d([2, 3]);\n   * gg(x).print();\n   * ```\n   *\n   * @param f The function f(x), to compute gradient for.\n   */\n  @doc({heading: 'Training', subheading: 'Gradients'})\n  static grad<I extends Tensor, O extends Tensor>(f: (x: I) => O):\n      (x: I, dy?: O) => I {\n    util.assert(\n        util.isFunction(f), 'The f passed in grad(f) must be a function');\n    return (x: I, dy?: O): I => {\n      util.assert(\n          x instanceof Tensor, 'The x passed in grad(f)(x) must be a tensor');\n      util.assert(\n          dy == null || dy instanceof Tensor,\n          'The dy passed in grad(f)(x, dy) must be a tensor');\n      const {value, grads} = ENV.engine.gradients(() => f(x), [x], dy);\n      if (dy != null) {\n        util.assertShapesMatch(\n            value.shape, dy.shape,\n            'The shape of dy passed in grad(f)(x, dy) must match the shape ' +\n                'returned by f(x)');\n      }\n      value.dispose();\n      checkGrads(grads);\n      return grads[0] as I;\n    };\n  }\n\n  /**\n   * Provided `f(x1, x2,...)`, returns another function `g([x1, x2,...], dy?)`,\n   * which gives an array of gradients of `f()` with respect to each input\n   * [`x1`,`x2`,...].\n   *\n   * If `dy` is passed when calling `g()`, the gradient of\n   * `f(x1,...).mul(dy).sum()` with respect to each input is computed instead.\n   * The provided `f` must take one or more tensors and return a single tensor\n   * `y`. If `f()` takes a single input, we recommend using `grad` instead.\n   *\n   * ```js\n   * // f(a, b) = a * b\n   * const f = (a, b) => a.mul(b);\n   * // df / da = b, df / db = a\n   * const g = tf.grads(f);\n   *\n   * const a = tf.tensor1d([2, 3]);\n   * const b = tf.tensor1d([-2, -3]);\n   * const [da, db] = g([a, b]);\n   * console.log('da');\n   * da.print();\n   * console.log('db');\n   * db.print();\n   * ```\n   *\n   * @param f The function `f(x1, x2,...)` to compute gradients for.\n   */\n  @doc({heading: 'Training', subheading: 'Gradients'})\n  static grads<O extends Tensor>(f: (...args: Tensor[]) => O):\n      (args: Tensor[], dy?: O) => Tensor[] {\n    util.assert(\n        util.isFunction(f), 'The f passed in grads(f) must be a function');\n    return (args: Tensor[], dy?: O): Tensor[] => {\n      util.assert(\n          Array.isArray(args) && args.every(arg => arg instanceof Tensor),\n          'The args passed in grads(f)(args) must be an array of tensors');\n      util.assert(\n          dy == null || dy instanceof Tensor,\n          'The dy passed in grads(f)(args, dy) must be a tensor');\n      const {value, grads} = ENV.engine.gradients(() => f(...args), args, dy);\n      if (dy != null) {\n        util.assertShapesMatch(\n            value.shape, dy.shape,\n            'The shape of dy passed in grads(f)([x1,...], dy) must match the ' +\n                'shape returned by f([x1,...])');\n      }\n      value.dispose();\n      checkGrads(grads);\n      return grads;\n    };\n  }\n\n  /**\n   * Like `grad`, but also returns the value of `f()`. Useful when `f()`\n   * returns a metric you want to show.\n   *\n   * The result is a rich object with the following properties:\n   * - grad: The gradient of `f(x)` w.r.t `x` (result of `grad`).\n   * - value: The value returned by `f(x)`.\n   *\n   * ```js\n   * // f(x) = x ^ 2\n   * const f = x => x.square();\n   * // f'(x) = 2x\n   * const g = tf.valueAndGrad(f);\n   *\n   * const x = tf.tensor1d([2, 3]);\n   * const {value, grad} = g(x);\n   *\n   * console.log('value');\n   * value.print();\n   * console.log('grad');\n   * grad.print();\n   * ```\n   */\n  @doc({heading: 'Training', subheading: 'Gradients'})\n  static valueAndGrad<I extends Tensor, O extends Tensor>(f: (x: I) => O):\n      (x: I, dy?: O) => {\n        value: O;\n        grad: I;\n      } {\n    util.assert(\n        util.isFunction(f),\n        'The f passed in valueAndGrad(f) must be a function');\n    return (x: I, dy?: O) => {\n      util.assert(\n          x instanceof Tensor,\n          'The x passed in valueAndGrad(f)(x) must be a tensor');\n      util.assert(\n          dy == null || dy instanceof Tensor,\n          'The dy passed in valueAndGrad(f)(x, dy) must be a tensor');\n      const {grads, value} = ENV.engine.gradients(() => f(x), [x], dy);\n      checkGrads(grads);\n      return {grad: grads[0] as I, value: value as O};\n    };\n  }\n\n  /**\n   * Like `grads`, but returns also the value of `f()`. Useful when `f()`\n   * returns a metric you want to show.\n   *\n   * The result is a rich object with the following properties:\n   * - grads: The gradients of `f()` w.r.t each input (result of `grads`).\n   * - value: The value returned by `f(x)`.\n   *\n   * ```js\n   * // f(a, b) = a * b\n   * const f = (a, b) => a.mul(b);\n   * // df/da = b, df/db = a\n   * const g = tf.valueAndGrads(f);\n   *\n   * const a = tf.tensor1d([2, 3]);\n   * const b = tf.tensor1d([-2, -3]);\n   * const {value, grads} = g([a, b]);\n   *\n   * const [da, db] = grads;\n   *\n   * console.log('value');\n   * value.print();\n   *\n   * console.log('da');\n   * da.print();\n   * console.log('db');\n   * db.print();\n   * ```\n   */\n  @doc({heading: 'Training', subheading: 'Gradients'})\n  static valueAndGrads<O extends Tensor>(f: (...args: Tensor[]) => O):\n      (args: Tensor[], dy?: O) => {\n        grads: Tensor[];\n        value: O;\n      } {\n    util.assert(\n        util.isFunction(f),\n        'The f passed in valueAndGrads(f) must be a function');\n    return (args: Tensor[], dy?: O) => {\n      util.assert(\n          Array.isArray(args) && args.every(arg => arg instanceof Tensor),\n          'The args passed in valueAndGrads(f)(args) must be array of tensors');\n      util.assert(\n          dy == null || dy instanceof Tensor,\n          'The dy passed in valueAndGrads(f)(args, dy) must be a tensor');\n      const res = ENV.engine.gradients(() => f(...args), args, dy);\n      if (dy != null) {\n        util.assertShapesMatch(\n            res.value.shape, dy.shape,\n            'The shape of dy passed in valueAndGrads(f)([x1,...], dy) must ' +\n                'match the shape returned by f([x1,...])');\n      }\n      checkGrads(res.grads);\n      return res;\n    };\n  }\n\n  /**\n   * Computes and returns the gradient of f(x) with respect to the list of\n   * trainable variables provided by `varList`. If no list is provided, it\n   * defaults to all trainable variables.\n   *\n   * ```js\n   * const a = tf.variable(tf.tensor1d([3, 4]));\n   * const b = tf.variable(tf.tensor1d([5, 6]));\n   * const x = tf.tensor1d([1, 2]);\n   *\n   * // f(a, b) = a * x ^ 2 + b * x\n   * const f = () => a.mul(x.square()).add(b.mul(x)).sum();\n   * // df/da = x ^ 2, df/db = x\n   * const {value, grads} = tf.variableGrads(f);\n   *\n   * Object.keys(grads).forEach(varName => grads[varName].print());\n   * ```\n   *\n   * @param f The function to execute. f() should return a scalar.\n   * @param varList The list of trainable variables. Defaults to all variables.\n   */\n  @doc({heading: 'Training', subheading: 'Gradients'})\n  static variableGrads(f: () => Scalar, varList?: Variable[]):\n      {value: Scalar, grads: NamedTensorMap} {\n    util.assert(\n        util.isFunction(f),\n        'The f passed in variableGrads(f) must be a function');\n    util.assert(\n        varList == null ||\n            Array.isArray(varList) && varList.every(v => v instanceof Variable),\n        'The varList passed in variableGrads(f, varList) must be an array ' +\n            'of variables');\n    if (varList == null) {\n      // Get all of the trainable variables.\n      varList = [];\n      for (const varName in ENV.engine.registeredVariables) {\n        varList.push(ENV.engine.registeredVariables[varName]);\n      }\n    }\n    // Prune non-trainable variables.\n    const originalVarCount = varList.length;\n    varList = varList.filter(variable => variable.trainable);\n    util.assert(\n        varList.length > 0,\n        `variableGrads() expects at least one of the input variables to be ` +\n            `trainable, but none of the ${originalVarCount} variables is ` +\n            `trainable.`);\n\n    const allowNoGradients = true;\n    const {value, grads} =\n        ENV.engine.gradients(f, varList, null, allowNoGradients);\n\n    util.assert(\n        grads.some(g => g != null),\n        'Cannot find a connection between any variable and the result of the ' +\n            'loss function y=f(x). Please make sure the operations that use ' +\n            'variables are inside the function f passed to minimize().');\n    util.assert(\n        value.rank === 0,\n        `The f passed in variableGrads(f) must return a scalar, but it ` +\n            `returned a rank-${value.rank} tensor`);\n\n    const namedGrads: NamedTensorMap = {};\n    varList.forEach((v, i) => {\n      if (grads[i] != null) {\n        namedGrads[v.name] = grads[i];\n      }\n    });\n    return {value, grads: namedGrads};\n  }\n\n  /**\n   * Overrides the gradient computation of a function `f`.\n   *\n   * Takes a function\n   * `f(...inputs) => {value: Tensor, gradFunc: dy => Tensor[]}` and returns\n   * another function `g(...inputs)` which takes the same inputs as `f`. When\n   * called, `g` returns `f().value`. In backward mode, custom gradients with\n   * respect to each input of `f` are computed using `f().gradFunc`.\n   *\n   * ```js\n   * const customOp = tf.customGrad(x => {\n   *   // Override gradient of our custom x ^ 2 op to be dy * abs(x);\n   *   return {value: x.square(), gradFunc: dy => [dy.mul(x.abs())]};\n   * });\n   *\n   * const x = tf.tensor1d([-1, -2, 3]);\n   * const dx = tf.grad(x => customOp(x));\n   *\n   * console.log(`f(x):`);\n   * customOp(x).print();\n   * console.log(`f'(x):`);\n   * dx(x).print();\n   * ```\n   *\n   * @param f The function to evaluate in forward mode, which should return\n   *     `{value: Tensor, gradFunc: (dy) => Tensor[]}`, where `gradFunc` returns\n   *     the custom gradients of `f` with respect to its inputs.\n   */\n  @doc({heading: 'Training', subheading: 'Gradients'})\n  static customGrad<T extends Tensor>(f: CustomGradientFunc<T>):\n      (...args: Tensor[]) => T {\n    return ENV.engine.customGrad(f);\n  }\n}\n\nfunction checkGrads(grads: Tensor[]) {\n  const numNullGradients = grads.filter(g => g == null).length;\n  if (numNullGradients > 0) {\n    throw new Error(\n        `Cannot compute gradient of y=f(x) with respect to x. Make sure that\n    the f you passed encloses all operations that lead from x to y.`);\n  }\n}\n"]}},"hash":"484a2450cecba47b91a2bee086eb19c6","cacheData":{"env":{}}}