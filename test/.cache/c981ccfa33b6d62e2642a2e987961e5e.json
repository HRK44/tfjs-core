{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/package.json","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1524062920943},{"name":"../environment","loc":{"line":13,"column":28}},{"name":"../globals","loc":{"line":14,"column":24}},{"name":"../ops/ops","loc":{"line":15,"column":20}},{"name":"../serialization","loc":{"line":16,"column":30}},{"name":"./optimizer","loc":{"line":17,"column":26}}],"generated":{"js":"\"use strict\";\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = Object.setPrototypeOf ||\n        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n        function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar environment_1 = require(\"../environment\");\nvar globals_1 = require(\"../globals\");\nvar ops_1 = require(\"../ops/ops\");\nvar serialization_1 = require(\"../serialization\");\nvar optimizer_1 = require(\"./optimizer\");\nvar AdamOptimizer = (function (_super) {\n    __extends(AdamOptimizer, _super);\n    function AdamOptimizer(learningRate, beta1, beta2, epsilon) {\n        if (epsilon === void 0) { epsilon = 1e-8; }\n        var _this = _super.call(this) || this;\n        _this.learningRate = learningRate;\n        _this.beta1 = beta1;\n        _this.beta2 = beta2;\n        _this.epsilon = epsilon;\n        _this.accumulatedFirstMoment = {};\n        _this.accumulatedSecondMoment = {};\n        _this.c = globals_1.keep(ops_1.scalar(-learningRate));\n        _this.epsScalar = globals_1.keep(ops_1.scalar(epsilon));\n        _this.beta1Scalar = globals_1.keep(ops_1.scalar(beta1));\n        _this.beta2Scalar = globals_1.keep(ops_1.scalar(beta2));\n        globals_1.tidy(function () {\n            _this.accBeta1 = ops_1.scalar(beta1).variable();\n            _this.accBeta2 = ops_1.scalar(beta2).variable();\n        });\n        _this.oneMinusBeta1 = globals_1.keep(ops_1.scalar(1 - beta1));\n        _this.oneMinusBeta2 = globals_1.keep(ops_1.scalar(1 - beta2));\n        _this.one = globals_1.keep(ops_1.scalar(1));\n        return _this;\n    }\n    AdamOptimizer.prototype.applyGradients = function (variableGradients) {\n        var _this = this;\n        globals_1.tidy(function () {\n            var oneMinusAccBeta1 = _this.one.sub(_this.accBeta1);\n            var oneMinusAccBeta2 = _this.one.sub(_this.accBeta2);\n            for (var variableName in variableGradients) {\n                var value = environment_1.ENV.engine.registeredVariables[variableName];\n                if (_this.accumulatedFirstMoment[variableName] == null) {\n                    var trainable = false;\n                    _this.accumulatedFirstMoment[variableName] =\n                        ops_1.zerosLike(value).variable(trainable);\n                }\n                if (_this.accumulatedSecondMoment[variableName] == null) {\n                    var trainable = false;\n                    _this.accumulatedSecondMoment[variableName] =\n                        ops_1.zerosLike(value).variable(trainable);\n                }\n                var gradient = variableGradients[variableName];\n                var firstMoment = _this.accumulatedFirstMoment[variableName];\n                var secondMoment = _this.accumulatedSecondMoment[variableName];\n                var newFirstMoment = _this.beta1Scalar.mul(firstMoment)\n                    .add(_this.oneMinusBeta1.mul(gradient));\n                var newSecondMoment = _this.beta2Scalar.mul(secondMoment)\n                    .add(_this.oneMinusBeta2.mul(gradient.square()));\n                var biasCorrectedFirstMoment = newFirstMoment.div(oneMinusAccBeta1);\n                var biasCorrectedSecondMoment = newSecondMoment.div(oneMinusAccBeta2);\n                _this.accumulatedFirstMoment[variableName].assign(newFirstMoment);\n                _this.accumulatedSecondMoment[variableName].assign(newSecondMoment);\n                var newValue = _this.c\n                    .mul(biasCorrectedFirstMoment.div(_this.epsScalar.add(biasCorrectedSecondMoment.sqrt())))\n                    .add(value);\n                value.assign(newValue);\n            }\n            _this.accBeta1.assign(_this.accBeta1.mul(_this.beta1Scalar));\n            _this.accBeta2.assign(_this.accBeta2.mul(_this.beta2Scalar));\n        });\n    };\n    AdamOptimizer.prototype.dispose = function () {\n        var _this = this;\n        this.c.dispose();\n        this.epsScalar.dispose();\n        this.beta1Scalar.dispose();\n        this.beta2Scalar.dispose();\n        this.accBeta1.dispose();\n        this.accBeta2.dispose();\n        this.oneMinusBeta1.dispose();\n        this.oneMinusBeta2.dispose();\n        this.one.dispose();\n        if (this.accumulatedFirstMoment != null) {\n            Object.keys(this.accumulatedFirstMoment)\n                .forEach(function (name) { return _this.accumulatedFirstMoment[name].dispose(); });\n        }\n        if (this.accumulatedSecondMoment != null) {\n            Object.keys(this.accumulatedSecondMoment)\n                .forEach(function (name) { return _this.accumulatedSecondMoment[name].dispose(); });\n        }\n    };\n    AdamOptimizer.prototype.getConfig = function () {\n        return {\n            learningRate: this.learningRate,\n            beta1: this.beta1,\n            beta2: this.beta2,\n            epsilon: this.epsilon,\n        };\n    };\n    AdamOptimizer.fromConfig = function (cls, config) {\n        return new cls(config.learningRate, config.beta1, config.beta2, config.epsilon);\n    };\n    AdamOptimizer.className = 'AdamOptimizer';\n    return AdamOptimizer;\n}(optimizer_1.Optimizer));\nexports.AdamOptimizer = AdamOptimizer;\nserialization_1.SerializationMap.register(AdamOptimizer);\n","map":{"version":3,"file":"adam_optimizer.js","sourceRoot":"","sources":["../src/optimizers/adam_optimizer.ts"],"names":[],"mappings":";;;;;;;;;;;;AAiBA,8CAAmC;AACnC,sCAAsC;AACtC,kCAA6C;AAE7C,kDAAqG;AAIrG,yCAAsC;AAEtC;IAAmC,iCAAS;IAe1C,uBACc,YAAoB,EAAY,KAAa,EAC7C,KAAa,EAAY,OAAc;QAAd,wBAAA,EAAA,cAAc;QAFrD,YAGE,iBAAO,SAcR;QAhBa,kBAAY,GAAZ,YAAY,CAAQ;QAAY,WAAK,GAAL,KAAK,CAAQ;QAC7C,WAAK,GAAL,KAAK,CAAQ;QAAY,aAAO,GAAP,OAAO,CAAO;QAL7C,4BAAsB,GAAqB,EAAE,CAAC;QAC9C,6BAAuB,GAAqB,EAAE,CAAC;QAMrD,KAAI,CAAC,CAAC,GAAG,cAAI,CAAC,YAAM,CAAC,CAAC,YAAY,CAAC,CAAC,CAAC;QACrC,KAAI,CAAC,SAAS,GAAG,cAAI,CAAC,YAAM,CAAC,OAAO,CAAC,CAAC,CAAC;QAEvC,KAAI,CAAC,WAAW,GAAG,cAAI,CAAC,YAAM,CAAC,KAAK,CAAC,CAAC,CAAC;QACvC,KAAI,CAAC,WAAW,GAAG,cAAI,CAAC,YAAM,CAAC,KAAK,CAAC,CAAC,CAAC;QACvC,cAAI,CAAC;YAEH,KAAI,CAAC,QAAQ,GAAG,YAAM,CAAC,KAAK,CAAC,CAAC,QAAQ,EAAE,CAAC;YACzC,KAAI,CAAC,QAAQ,GAAG,YAAM,CAAC,KAAK,CAAC,CAAC,QAAQ,EAAE,CAAC;QAC3C,CAAC,CAAC,CAAC;QACH,KAAI,CAAC,aAAa,GAAG,cAAI,CAAC,YAAM,CAAC,CAAC,GAAG,KAAK,CAAC,CAAC,CAAC;QAC7C,KAAI,CAAC,aAAa,GAAG,cAAI,CAAC,YAAM,CAAC,CAAC,GAAG,KAAK,CAAC,CAAC,CAAC;QAC7C,KAAI,CAAC,GAAG,GAAG,cAAI,CAAC,YAAM,CAAC,CAAC,CAAC,CAAC,CAAC;;IAC7B,CAAC;IAED,sCAAc,GAAd,UAAe,iBAAmC;QAAlD,iBA6CC;QA5CC,cAAI,CAAC;YACH,IAAM,gBAAgB,GAAG,KAAI,CAAC,GAAG,CAAC,GAAG,CAAC,KAAI,CAAC,QAAQ,CAAC,CAAC;YACrD,IAAM,gBAAgB,GAAG,KAAI,CAAC,GAAG,CAAC,GAAG,CAAC,KAAI,CAAC,QAAQ,CAAC,CAAC;YAErD,GAAG,CAAC,CAAC,IAAM,YAAY,IAAI,iBAAiB,CAAC,CAAC,CAAC;gBAC7C,IAAM,KAAK,GAAG,iBAAG,CAAC,MAAM,CAAC,mBAAmB,CAAC,YAAY,CAAC,CAAC;gBAC3D,EAAE,CAAC,CAAC,KAAI,CAAC,sBAAsB,CAAC,YAAY,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC;oBACtD,IAAM,SAAS,GAAG,KAAK,CAAC;oBACxB,KAAI,CAAC,sBAAsB,CAAC,YAAY,CAAC;wBACrC,eAAS,CAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC,CAAC;gBAC3C,CAAC;gBACD,EAAE,CAAC,CAAC,KAAI,CAAC,uBAAuB,CAAC,YAAY,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC;oBACvD,IAAM,SAAS,GAAG,KAAK,CAAC;oBACxB,KAAI,CAAC,uBAAuB,CAAC,YAAY,CAAC;wBACtC,eAAS,CAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC,CAAC;gBAC3C,CAAC;gBAED,IAAM,QAAQ,GAAG,iBAAiB,CAAC,YAAY,CAAC,CAAC;gBACjD,IAAM,WAAW,GAAG,KAAI,CAAC,sBAAsB,CAAC,YAAY,CAAC,CAAC;gBAC9D,IAAM,YAAY,GAAG,KAAI,CAAC,uBAAuB,CAAC,YAAY,CAAC,CAAC;gBAEhE,IAAM,cAAc,GAAG,KAAI,CAAC,WAAW,CAAC,GAAG,CAAC,WAAW,CAAC;qBAC5B,GAAG,CAAC,KAAI,CAAC,aAAa,CAAC,GAAG,CAAC,QAAQ,CAAC,CAAC,CAAC;gBAClE,IAAM,eAAe,GACjB,KAAI,CAAC,WAAW,CAAC,GAAG,CAAC,YAAY,CAAC;qBAC7B,GAAG,CAAC,KAAI,CAAC,aAAa,CAAC,GAAG,CAAC,QAAQ,CAAC,MAAM,EAAE,CAAC,CAAC,CAAC;gBAExD,IAAM,wBAAwB,GAAG,cAAc,CAAC,GAAG,CAAC,gBAAgB,CAAC,CAAC;gBACtE,IAAM,yBAAyB,GAAG,eAAe,CAAC,GAAG,CAAC,gBAAgB,CAAC,CAAC;gBAExE,KAAI,CAAC,sBAAsB,CAAC,YAAY,CAAC,CAAC,MAAM,CAAC,cAAc,CAAC,CAAC;gBACjE,KAAI,CAAC,uBAAuB,CAAC,YAAY,CAAC,CAAC,MAAM,CAAC,eAAe,CAAC,CAAC;gBAEnE,IAAM,QAAQ,GACV,KAAI,CAAC,CAAC;qBACD,GAAG,CAAC,wBAAwB,CAAC,GAAG,CAC7B,KAAI,CAAC,SAAS,CAAC,GAAG,CAAC,yBAAyB,CAAC,IAAI,EAAE,CAAC,CAAC,CAAC;qBACzD,GAAG,CAAC,KAAK,CAAC,CAAC;gBACpB,KAAK,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;YACzB,CAAC;YAED,KAAI,CAAC,QAAQ,CAAC,MAAM,CAAC,KAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,KAAI,CAAC,WAAW,CAAC,CAAC,CAAC;YAC1D,KAAI,CAAC,QAAQ,CAAC,MAAM,CAAC,KAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,KAAI,CAAC,WAAW,CAAC,CAAC,CAAC;QAC5D,CAAC,CAAC,CAAC;IACL,CAAC;IAED,+BAAO,GAAP;QAAA,iBAoBC;QAnBC,IAAI,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC;QACjB,IAAI,CAAC,SAAS,CAAC,OAAO,EAAE,CAAC;QACzB,IAAI,CAAC,WAAW,CAAC,OAAO,EAAE,CAAC;QAC3B,IAAI,CAAC,WAAW,CAAC,OAAO,EAAE,CAAC;QAC3B,IAAI,CAAC,QAAQ,CAAC,OAAO,EAAE,CAAC;QACxB,IAAI,CAAC,QAAQ,CAAC,OAAO,EAAE,CAAC;QACxB,IAAI,CAAC,aAAa,CAAC,OAAO,EAAE,CAAC;QAC7B,IAAI,CAAC,aAAa,CAAC,OAAO,EAAE,CAAC;QAC7B,IAAI,CAAC,GAAG,CAAC,OAAO,EAAE,CAAC;QAEnB,EAAE,CAAC,CAAC,IAAI,CAAC,sBAAsB,IAAI,IAAI,CAAC,CAAC,CAAC;YACxC,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,sBAAsB,CAAC;iBACnC,OAAO,CAAC,UAAA,IAAI,IAAI,OAAA,KAAI,CAAC,sBAAsB,CAAC,IAAI,CAAC,CAAC,OAAO,EAAE,EAA3C,CAA2C,CAAC,CAAC;QACpE,CAAC;QAED,EAAE,CAAC,CAAC,IAAI,CAAC,uBAAuB,IAAI,IAAI,CAAC,CAAC,CAAC;YACzC,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,uBAAuB,CAAC;iBACpC,OAAO,CAAC,UAAA,IAAI,IAAI,OAAA,KAAI,CAAC,uBAAuB,CAAC,IAAI,CAAC,CAAC,OAAO,EAAE,EAA5C,CAA4C,CAAC,CAAC;QACrE,CAAC;IACH,CAAC;IACD,iCAAS,GAAT;QACE,MAAM,CAAC;YACL,YAAY,EAAE,IAAI,CAAC,YAAY;YAC/B,KAAK,EAAE,IAAI,CAAC,KAAK;YACjB,KAAK,EAAE,IAAI,CAAC,KAAK;YACjB,OAAO,EAAE,IAAI,CAAC,OAAO;SACtB,CAAC;IACJ,CAAC;IACM,wBAAU,GAAjB,UACI,GAA+B,EAAE,MAAkB;QACrD,MAAM,CAAC,IAAI,GAAG,CACV,MAAM,CAAC,YAAY,EAAE,MAAM,CAAC,KAAK,EAAE,MAAM,CAAC,KAAK,EAAE,MAAM,CAAC,OAAO,CAAC,CAAC;IACvE,CAAC;IAjHM,uBAAS,GAAG,eAAe,CAAC;IAkHrC,oBAAC;CAAA,AAnHD,CAAmC,qBAAS,GAmH3C;AAnHY,sCAAa;AAoH1B,gCAAgB,CAAC,QAAQ,CAAC,aAAa,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENV} from '../environment';\nimport {keep, tidy} from '../globals';\nimport {scalar, zerosLike} from '../ops/ops';\n// tslint:disable-next-line:max-line-length\nimport {ConfigDict, Serializable, SerializableConstructor, SerializationMap} from '../serialization';\nimport {Scalar, Variable} from '../tensor';\nimport {NamedVariableMap} from '../types';\n\nimport {Optimizer} from './optimizer';\n\nexport class AdamOptimizer extends Optimizer {\n  static className = 'AdamOptimizer';\n  private c: Scalar;\n  private epsScalar: Scalar;\n  private beta1Scalar: Scalar;\n  private beta2Scalar: Scalar;\n  private accBeta1: Variable;\n  private accBeta2: Variable;\n  private oneMinusBeta1: Scalar;\n  private oneMinusBeta2: Scalar;\n  private one: Scalar;\n\n  private accumulatedFirstMoment: NamedVariableMap = {};\n  private accumulatedSecondMoment: NamedVariableMap = {};\n\n  constructor(\n      protected learningRate: number, protected beta1: number,\n      protected beta2: number, protected epsilon = 1e-8) {\n    super();\n    this.c = keep(scalar(-learningRate));\n    this.epsScalar = keep(scalar(epsilon));\n    // b1, b2 keep initial value of beta* hyperparameters.\n    this.beta1Scalar = keep(scalar(beta1));\n    this.beta2Scalar = keep(scalar(beta2));\n    tidy(() => {\n      // accB* will be updated by batch.\n      this.accBeta1 = scalar(beta1).variable();\n      this.accBeta2 = scalar(beta2).variable();\n    });\n    this.oneMinusBeta1 = keep(scalar(1 - beta1));\n    this.oneMinusBeta2 = keep(scalar(1 - beta2));\n    this.one = keep(scalar(1));\n  }\n\n  applyGradients(variableGradients: NamedVariableMap) {\n    tidy(() => {\n      const oneMinusAccBeta1 = this.one.sub(this.accBeta1);\n      const oneMinusAccBeta2 = this.one.sub(this.accBeta2);\n\n      for (const variableName in variableGradients) {\n        const value = ENV.engine.registeredVariables[variableName];\n        if (this.accumulatedFirstMoment[variableName] == null) {\n          const trainable = false;\n          this.accumulatedFirstMoment[variableName] =\n              zerosLike(value).variable(trainable);\n        }\n        if (this.accumulatedSecondMoment[variableName] == null) {\n          const trainable = false;\n          this.accumulatedSecondMoment[variableName] =\n              zerosLike(value).variable(trainable);\n        }\n\n        const gradient = variableGradients[variableName];\n        const firstMoment = this.accumulatedFirstMoment[variableName];\n        const secondMoment = this.accumulatedSecondMoment[variableName];\n\n        const newFirstMoment = this.beta1Scalar.mul(firstMoment)\n                                   .add(this.oneMinusBeta1.mul(gradient));\n        const newSecondMoment =\n            this.beta2Scalar.mul(secondMoment)\n                .add(this.oneMinusBeta2.mul(gradient.square()));\n\n        const biasCorrectedFirstMoment = newFirstMoment.div(oneMinusAccBeta1);\n        const biasCorrectedSecondMoment = newSecondMoment.div(oneMinusAccBeta2);\n\n        this.accumulatedFirstMoment[variableName].assign(newFirstMoment);\n        this.accumulatedSecondMoment[variableName].assign(newSecondMoment);\n\n        const newValue =\n            this.c\n                .mul(biasCorrectedFirstMoment.div(\n                    this.epsScalar.add(biasCorrectedSecondMoment.sqrt())))\n                .add(value);\n        value.assign(newValue);\n      }\n\n      this.accBeta1.assign(this.accBeta1.mul(this.beta1Scalar));\n      this.accBeta2.assign(this.accBeta2.mul(this.beta2Scalar));\n    });\n  }\n\n  dispose() {\n    this.c.dispose();\n    this.epsScalar.dispose();\n    this.beta1Scalar.dispose();\n    this.beta2Scalar.dispose();\n    this.accBeta1.dispose();\n    this.accBeta2.dispose();\n    this.oneMinusBeta1.dispose();\n    this.oneMinusBeta2.dispose();\n    this.one.dispose();\n\n    if (this.accumulatedFirstMoment != null) {\n      Object.keys(this.accumulatedFirstMoment)\n          .forEach(name => this.accumulatedFirstMoment[name].dispose());\n    }\n\n    if (this.accumulatedSecondMoment != null) {\n      Object.keys(this.accumulatedSecondMoment)\n          .forEach(name => this.accumulatedSecondMoment[name].dispose());\n    }\n  }\n  getConfig(): ConfigDict {\n    return {\n      learningRate: this.learningRate,\n      beta1: this.beta1,\n      beta2: this.beta2,\n      epsilon: this.epsilon,\n    };\n  }\n  static fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(\n        config.learningRate, config.beta1, config.beta2, config.epsilon);\n  }\n}\nSerializationMap.register(AdamOptimizer);\n"]}},"hash":"8158c1cd56c043e0a7fce3032a10cfee","cacheData":{"env":{}}}