{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/package.json","includedInParent":true,"mtime":1528810356568},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1528810356568},{"name":"../doc","loc":{"line":9,"column":20}},{"name":"../util","loc":{"line":10,"column":19}},{"name":"./operation","loc":{"line":11,"column":26}},{"name":"./ops","loc":{"line":12,"column":18}}],"generated":{"js":"\"use strict\";\nvar __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar doc_1 = require(\"../doc\");\nvar util = require(\"../util\");\nvar operation_1 = require(\"./operation\");\nvar ops = require(\"./ops\");\nvar Reduction;\n(function (Reduction) {\n    Reduction[Reduction[\"NONE\"] = 0] = \"NONE\";\n    Reduction[Reduction[\"MEAN\"] = 1] = \"MEAN\";\n    Reduction[Reduction[\"SUM\"] = 2] = \"SUM\";\n    Reduction[Reduction[\"SUM_BY_NONZERO_WEIGHTS\"] = 3] = \"SUM_BY_NONZERO_WEIGHTS\";\n})(Reduction = exports.Reduction || (exports.Reduction = {}));\nvar LossOps = (function () {\n    function LossOps() {\n    }\n    LossOps.computeWeightedLoss = function (losses, weights, reduction) {\n        if (reduction === void 0) { reduction = Reduction.SUM_BY_NONZERO_WEIGHTS; }\n        util.assertArgumentsAreTensors({ losses: losses }, 'computeWeightedLoss');\n        if (weights != null) {\n            util.assertArgumentsAreTensors({ weights: weights }, 'computeWeightedLoss');\n        }\n        var weightedLoss = (weights == null) ? losses : losses.mul(weights);\n        if (reduction === Reduction.NONE) {\n            return weightedLoss;\n        }\n        if (reduction === Reduction.SUM) {\n            return weightedLoss.sum();\n        }\n        if (reduction === Reduction.MEAN) {\n            return (weights == null) ? weightedLoss.mean() :\n                weightedLoss.sum().div(weights.sum());\n        }\n        if (reduction === Reduction.SUM_BY_NONZERO_WEIGHTS) {\n            if (weights == null) {\n                return weightedLoss.sum().div(ops.scalar(losses.size));\n            }\n            else {\n                var numNonZeros = weights.notEqual(ops.scalar(0)).sum().toFloat();\n                return weightedLoss.sum().div(numNonZeros);\n            }\n        }\n        throw Error(\"Unknown reduction: \" + reduction);\n    };\n    LossOps.absoluteDifference = function (labels, predictions, weights, reduction) {\n        if (reduction === void 0) { reduction = Reduction.SUM_BY_NONZERO_WEIGHTS; }\n        util.assertArgumentsAreTensors({ labels: labels, predictions: predictions }, 'absoluteDifference');\n        if (weights != null) {\n            util.assertArgumentsAreTensors({ weights: weights }, 'absoluteDifference');\n        }\n        util.assertShapesMatch(labels.shape, predictions.shape, 'Error in absoluteDifference: ');\n        var losses = labels.sub(predictions).abs();\n        return LossOps.computeWeightedLoss(losses, weights, reduction);\n    };\n    LossOps.meanSquaredError = function (labels, predictions, weights, reduction) {\n        if (reduction === void 0) { reduction = Reduction.SUM_BY_NONZERO_WEIGHTS; }\n        util.assertArgumentsAreTensors({ labels: labels, predictions: predictions }, 'meanSquaredError');\n        if (weights != null) {\n            util.assertArgumentsAreTensors({ weights: weights }, 'meanSquaredError');\n        }\n        util.assertShapesMatch(labels.shape, predictions.shape, 'Error in meanSquaredError: ');\n        var losses = labels.squaredDifference(predictions);\n        return LossOps.computeWeightedLoss(losses, weights, reduction);\n    };\n    LossOps.cosineDistance = function (labels, predictions, axis, weights, reduction) {\n        if (reduction === void 0) { reduction = Reduction.SUM_BY_NONZERO_WEIGHTS; }\n        util.assertArgumentsAreTensors({ labels: labels, predictions: predictions }, 'cosineDistance');\n        if (weights != null) {\n            util.assertArgumentsAreTensors({ weights: weights }, 'cosineDistance');\n        }\n        util.assertShapesMatch(labels.shape, predictions.shape, 'Error in cosineDistance: ');\n        var one = ops.scalar(1);\n        var losses = one.sub(labels.mul(predictions).sum(axis, true));\n        return LossOps.computeWeightedLoss(losses, weights, reduction);\n    };\n    LossOps.hingeLoss = function (labels, predictions, weights, reduction) {\n        if (reduction === void 0) { reduction = Reduction.SUM_BY_NONZERO_WEIGHTS; }\n        util.assertArgumentsAreTensors({ labels: labels, predictions: predictions }, 'hingeLoss');\n        if (weights != null) {\n            util.assertArgumentsAreTensors({ weights: weights }, 'hingeLoss');\n        }\n        util.assertShapesMatch(labels.shape, predictions.shape, 'Error in hingeLoss: ');\n        var one = ops.scalar(1);\n        labels = ops.scalar(2).mul(labels).sub(one);\n        var losses = one.sub(labels.mul(predictions)).relu();\n        return LossOps.computeWeightedLoss(losses, weights, reduction);\n    };\n    LossOps.logLoss = function (labels, predictions, weights, epsilon, reduction) {\n        if (epsilon === void 0) { epsilon = 1e-7; }\n        if (reduction === void 0) { reduction = Reduction.SUM_BY_NONZERO_WEIGHTS; }\n        util.assertArgumentsAreTensors({ labels: labels, predictions: predictions }, 'logLoss');\n        if (weights != null) {\n            util.assertArgumentsAreTensors({ weights: weights }, 'logLoss');\n        }\n        util.assertShapesMatch(labels.shape, predictions.shape, 'Error in logLoss: ');\n        var one = ops.scalar(1);\n        var epsilonScalar = ops.scalar(epsilon);\n        var losses = labels.mul(predictions.add(epsilonScalar).log())\n            .neg()\n            .sub(one.sub(labels).mul(one.sub(predictions).add(epsilonScalar).log()));\n        return LossOps.computeWeightedLoss(losses, weights, reduction);\n    };\n    LossOps.huberLoss = function (labels, predictions, weights, delta, reduction) {\n        if (delta === void 0) { delta = 1.0; }\n        if (reduction === void 0) { reduction = Reduction.SUM_BY_NONZERO_WEIGHTS; }\n        util.assertArgumentsAreTensors({ labels: labels, predictions: predictions }, 'huberLoss');\n        if (weights != null) {\n            util.assertArgumentsAreTensors({ weights: weights }, 'huberLoss');\n        }\n        util.assertShapesMatch(labels.shape, predictions.shape, 'Error in huberLoss: ');\n        var deltaScalar = ops.scalar(delta);\n        var error = predictions.sub(labels).abs();\n        var quadratic = ops.minimum(error, deltaScalar);\n        var linear = error.sub(quadratic);\n        var losses = ops.scalar(0.5).mul(quadratic.square()).add(deltaScalar.mul(linear));\n        return LossOps.computeWeightedLoss(losses, weights, reduction);\n    };\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Losses', namespace: 'losses' }),\n        operation_1.operation\n    ], LossOps, \"computeWeightedLoss\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Losses', namespace: 'losses' }),\n        operation_1.operation\n    ], LossOps, \"absoluteDifference\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Losses', namespace: 'losses' }),\n        operation_1.operation\n    ], LossOps, \"meanSquaredError\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Losses', namespace: 'losses' }),\n        operation_1.operation\n    ], LossOps, \"cosineDistance\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Losses', namespace: 'losses' }),\n        operation_1.operation\n    ], LossOps, \"hingeLoss\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Losses', namespace: 'losses' }),\n        operation_1.operation\n    ], LossOps, \"logLoss\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Losses', namespace: 'losses' }),\n        operation_1.operation\n    ], LossOps, \"huberLoss\", null);\n    return LossOps;\n}());\nexports.LossOps = LossOps;\n","map":{"version":3,"file":"loss_ops.js","sourceRoot":"","sources":["../src/ops/loss_ops.ts"],"names":[],"mappings":";;;;;;;;AAiBA,8BAA2B;AAE3B,8BAAgC;AAEhC,yCAAsC;AACtC,2BAA6B;AAE7B,IAAY,SAKX;AALD,WAAY,SAAS;IACnB,yCAAI,CAAA;IACJ,yCAAI,CAAA;IACJ,uCAAG,CAAA;IACH,6EAAsB,CAAA;AACxB,CAAC,EALW,SAAS,GAAT,iBAAS,KAAT,iBAAS,QAKpB;AAED;IAAA;IA2OA,CAAC;IA/NQ,2BAAmB,GAA1B,UACI,MAAS,EAAE,OAAgB,EAC3B,SAA4C;QAA5C,0BAAA,EAAA,YAAY,SAAS,CAAC,sBAAsB;QAC9C,IAAI,CAAC,yBAAyB,CAAC,EAAC,MAAM,QAAA,EAAC,EAAE,qBAAqB,CAAC,CAAC;QAChE,EAAE,CAAC,CAAC,OAAO,IAAI,IAAI,CAAC,CAAC,CAAC;YACpB,IAAI,CAAC,yBAAyB,CAAC,EAAC,OAAO,SAAA,EAAC,EAAE,qBAAqB,CAAC,CAAC;QACnE,CAAC;QAED,IAAM,YAAY,GAAG,CAAC,OAAO,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,CAAC,MAAM,CAAC,GAAG,CAAC,OAAO,CAAC,CAAC;QAEtE,EAAE,CAAC,CAAC,SAAS,KAAK,SAAS,CAAC,IAAI,CAAC,CAAC,CAAC;YACjC,MAAM,CAAC,YAAiB,CAAC;QAC3B,CAAC;QACD,EAAE,CAAC,CAAC,SAAS,KAAK,SAAS,CAAC,GAAG,CAAC,CAAC,CAAC;YAChC,MAAM,CAAC,YAAY,CAAC,GAAG,EAAE,CAAC;QAC5B,CAAC;QACD,EAAE,CAAC,CAAC,SAAS,KAAK,SAAS,CAAC,IAAI,CAAC,CAAC,CAAC;YACjC,MAAM,CAAC,CAAC,OAAO,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,YAAY,CAAC,IAAI,EAAE,CAAC,CAAC;gBACrB,YAAY,CAAC,GAAG,EAAE,CAAC,GAAG,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,CAAC;QACnE,CAAC;QACD,EAAE,CAAC,CAAC,SAAS,KAAK,SAAS,CAAC,sBAAsB,CAAC,CAAC,CAAC;YACnD,EAAE,CAAC,CAAC,OAAO,IAAI,IAAI,CAAC,CAAC,CAAC;gBACpB,MAAM,CAAC,YAAY,CAAC,GAAG,EAAE,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC;YACzD,CAAC;YAAC,IAAI,CAAC,CAAC;gBACN,IAAM,WAAW,GAAG,OAAO,CAAC,QAAQ,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,EAAE,CAAC,OAAO,EAAE,CAAC;gBACpE,MAAM,CAAC,YAAY,CAAC,GAAG,EAAE,CAAC,GAAG,CAAC,WAAW,CAAC,CAAC;YAC7C,CAAC;QACH,CAAC;QAED,MAAM,KAAK,CAAC,wBAAsB,SAAW,CAAC,CAAC;IACjD,CAAC;IAiBM,0BAAkB,GAAzB,UACI,MAAS,EAAE,WAAc,EAAE,OAAgB,EAC3C,SAA4C;QAA5C,0BAAA,EAAA,YAAY,SAAS,CAAC,sBAAsB;QAC9C,IAAI,CAAC,yBAAyB,CAAC,EAAC,MAAM,QAAA,EAAE,WAAW,aAAA,EAAC,EAAE,oBAAoB,CAAC,CAAC;QAC5E,EAAE,CAAC,CAAC,OAAO,IAAI,IAAI,CAAC,CAAC,CAAC;YACpB,IAAI,CAAC,yBAAyB,CAAC,EAAC,OAAO,SAAA,EAAC,EAAE,oBAAoB,CAAC,CAAC;QAClE,CAAC;QACD,IAAI,CAAC,iBAAiB,CAClB,MAAM,CAAC,KAAK,EAAE,WAAW,CAAC,KAAK,EAAE,+BAA+B,CAAC,CAAC;QAEtE,IAAM,MAAM,GAAG,MAAM,CAAC,GAAG,CAAC,WAAW,CAAC,CAAC,GAAG,EAAE,CAAC;QAC7C,MAAM,CAAC,OAAO,CAAC,mBAAmB,CAAC,MAAM,EAAE,OAAO,EAAE,SAAS,CAAC,CAAC;IACjE,CAAC;IAiBM,wBAAgB,GAAvB,UACI,MAAS,EAAE,WAAc,EAAE,OAAgB,EAC3C,SAA4C;QAA5C,0BAAA,EAAA,YAAY,SAAS,CAAC,sBAAsB;QAC9C,IAAI,CAAC,yBAAyB,CAAC,EAAC,MAAM,QAAA,EAAE,WAAW,aAAA,EAAC,EAAE,kBAAkB,CAAC,CAAC;QAC1E,EAAE,CAAC,CAAC,OAAO,IAAI,IAAI,CAAC,CAAC,CAAC;YACpB,IAAI,CAAC,yBAAyB,CAAC,EAAC,OAAO,SAAA,EAAC,EAAE,kBAAkB,CAAC,CAAC;QAChE,CAAC;QACD,IAAI,CAAC,iBAAiB,CAClB,MAAM,CAAC,KAAK,EAAE,WAAW,CAAC,KAAK,EAAE,6BAA6B,CAAC,CAAC;QAEpE,IAAM,MAAM,GAAG,MAAM,CAAC,iBAAiB,CAAC,WAAW,CAAC,CAAC;QACrD,MAAM,CAAC,OAAO,CAAC,mBAAmB,CAAC,MAAM,EAAE,OAAO,EAAE,SAAS,CAAC,CAAC;IACjE,CAAC;IAkBM,sBAAc,GAArB,UACI,MAAS,EAAE,WAAc,EAAE,IAAY,EAAE,OAAgB,EACzD,SAA4C;QAA5C,0BAAA,EAAA,YAAY,SAAS,CAAC,sBAAsB;QAC9C,IAAI,CAAC,yBAAyB,CAAC,EAAC,MAAM,QAAA,EAAE,WAAW,aAAA,EAAC,EAAE,gBAAgB,CAAC,CAAC;QACxE,EAAE,CAAC,CAAC,OAAO,IAAI,IAAI,CAAC,CAAC,CAAC;YACpB,IAAI,CAAC,yBAAyB,CAAC,EAAC,OAAO,SAAA,EAAC,EAAE,gBAAgB,CAAC,CAAC;QAC9D,CAAC;QACD,IAAI,CAAC,iBAAiB,CAClB,MAAM,CAAC,KAAK,EAAE,WAAW,CAAC,KAAK,EAAE,2BAA2B,CAAC,CAAC;QAElE,IAAM,GAAG,GAAG,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC;QAC1B,IAAM,MAAM,GAAG,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,GAAG,CAAC,WAAW,CAAC,CAAC,GAAG,CAAC,IAAI,EAAE,IAAI,CAAC,CAAC,CAAC;QAChE,MAAM,CAAC,OAAO,CAAC,mBAAmB,CAAC,MAAM,EAAE,OAAO,EAAE,SAAS,CAAC,CAAC;IACjE,CAAC;IAiBM,iBAAS,GAAhB,UACI,MAAS,EAAE,WAAc,EAAE,OAAgB,EAC3C,SAA4C;QAA5C,0BAAA,EAAA,YAAY,SAAS,CAAC,sBAAsB;QAC9C,IAAI,CAAC,yBAAyB,CAAC,EAAC,MAAM,QAAA,EAAE,WAAW,aAAA,EAAC,EAAE,WAAW,CAAC,CAAC;QACnE,EAAE,CAAC,CAAC,OAAO,IAAI,IAAI,CAAC,CAAC,CAAC;YACpB,IAAI,CAAC,yBAAyB,CAAC,EAAC,OAAO,SAAA,EAAC,EAAE,WAAW,CAAC,CAAC;QACzD,CAAC;QACD,IAAI,CAAC,iBAAiB,CAClB,MAAM,CAAC,KAAK,EAAE,WAAW,CAAC,KAAK,EAAE,sBAAsB,CAAC,CAAC;QAE7D,IAAM,GAAG,GAAG,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC;QAE1B,MAAM,GAAG,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC;QAC5C,IAAM,MAAM,GAAG,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,GAAG,CAAC,WAAW,CAAC,CAAC,CAAC,IAAI,EAAE,CAAC;QACvD,MAAM,CAAC,OAAO,CAAC,mBAAmB,CAAC,MAAM,EAAE,OAAO,EAAE,SAAS,CAAC,CAAC;IACjE,CAAC;IAkBM,eAAO,GAAd,UACI,MAAS,EAAE,WAAc,EAAE,OAAgB,EAAE,OAAc,EAC3D,SAA4C;QADC,wBAAA,EAAA,cAAc;QAC3D,0BAAA,EAAA,YAAY,SAAS,CAAC,sBAAsB;QAC9C,IAAI,CAAC,yBAAyB,CAAC,EAAC,MAAM,QAAA,EAAE,WAAW,aAAA,EAAC,EAAE,SAAS,CAAC,CAAC;QACjE,EAAE,CAAC,CAAC,OAAO,IAAI,IAAI,CAAC,CAAC,CAAC;YACpB,IAAI,CAAC,yBAAyB,CAAC,EAAC,OAAO,SAAA,EAAC,EAAE,SAAS,CAAC,CAAC;QACvD,CAAC;QACD,IAAI,CAAC,iBAAiB,CAClB,MAAM,CAAC,KAAK,EAAE,WAAW,CAAC,KAAK,EAAE,oBAAoB,CAAC,CAAC;QAE3D,IAAM,GAAG,GAAG,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC;QAC1B,IAAM,aAAa,GAAG,GAAG,CAAC,MAAM,CAAC,OAAO,CAAC,CAAC;QAC1C,IAAM,MAAM,GAAG,MAAM,CAAC,GAAG,CAAC,WAAW,CAAC,GAAG,CAAC,aAAa,CAAC,CAAC,GAAG,EAAE,CAAC;aAC3C,GAAG,EAAE;aACL,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,GAAG,CACpB,GAAG,CAAC,GAAG,CAAC,WAAW,CAAC,CAAC,GAAG,CAAC,aAAa,CAAC,CAAC,GAAG,EAAE,CAAC,CAAC,CAAC;QACvE,MAAM,CAAC,OAAO,CAAC,mBAAmB,CAAC,MAAM,EAAE,OAAO,EAAE,SAAS,CAAC,CAAC;IACjE,CAAC;IAkBM,iBAAS,GAAhB,UACI,MAAS,EAAE,WAAc,EAAE,OAAgB,EAAE,KAAW,EACxD,SAA4C;QADC,sBAAA,EAAA,WAAW;QACxD,0BAAA,EAAA,YAAY,SAAS,CAAC,sBAAsB;QAC9C,IAAI,CAAC,yBAAyB,CAAC,EAAC,MAAM,QAAA,EAAE,WAAW,aAAA,EAAC,EAAE,WAAW,CAAC,CAAC;QACnE,EAAE,CAAC,CAAC,OAAO,IAAI,IAAI,CAAC,CAAC,CAAC;YACpB,IAAI,CAAC,yBAAyB,CAAC,EAAC,OAAO,SAAA,EAAC,EAAE,WAAW,CAAC,CAAC;QACzD,CAAC;QACD,IAAI,CAAC,iBAAiB,CAClB,MAAM,CAAC,KAAK,EAAE,WAAW,CAAC,KAAK,EAAE,sBAAsB,CAAC,CAAC;QAE7D,IAAM,WAAW,GAAG,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC;QACtC,IAAM,KAAK,GAAG,WAAW,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,GAAG,EAAE,CAAC;QAC5C,IAAM,SAAS,GAAG,GAAG,CAAC,OAAO,CAAC,KAAK,EAAE,WAAW,CAAC,CAAC;QAClD,IAAM,MAAM,GAAG,KAAK,CAAC,GAAG,CAAC,SAAS,CAAC,CAAC;QAEpC,IAAM,MAAM,GACR,GAAG,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,SAAS,CAAC,MAAM,EAAE,CAAC,CAAC,GAAG,CAAC,WAAW,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC;QACzE,MAAM,CAAC,OAAO,CAAC,mBAAmB,CAAC,MAAM,EAAE,OAAO,EAAE,SAAS,CAAC,CAAC;IACjE,CAAC;IA9ND;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,EAAC,CAAC;QACrE,qBAAS;4CA+BT;IAiBD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,EAAC,CAAC;QACrE,qBAAS;2CAaT;IAiBD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,EAAC,CAAC;QACrE,qBAAS;yCAaT;IAkBD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,EAAC,CAAC;QACrE,qBAAS;uCAcT;IAiBD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,EAAC,CAAC;QACrE,qBAAS;kCAgBT;IAkBD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,EAAC,CAAC;QACrE,qBAAS;gCAkBT;IAkBD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,EAAC,CAAC;QACrE,qBAAS;kCAmBT;IACH,cAAC;CAAA,AA3OD,IA2OC;AA3OY,0BAAO","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {doc} from '../doc';\nimport {Tensor} from '../tensor';\nimport * as util from '../util';\n\nimport {operation} from './operation';\nimport * as ops from './ops';\n\nexport enum Reduction {\n  NONE,\n  MEAN,\n  SUM,\n  SUM_BY_NONZERO_WEIGHTS\n}\n\nexport class LossOps {\n  /**\n   * Computes the weighted loss between two tensors.\n   *\n   * @param losses Tensor of shape `[batch_size, d1, ... dN]`.\n   * @param weights Tensor whose rank is either 0, or the same rank as\n   *    `losses`, and must be broadcastable to `losses` (i.e., all\n   *    dimensions must be either `1`, or the same as the corresponding\n   *    `losses` dimension).\n   */\n  @doc({heading: 'Training', subheading: 'Losses', namespace: 'losses'})\n  @operation\n  static computeWeightedLoss<T extends Tensor, O extends Tensor>(\n      losses: T, weights?: Tensor,\n      reduction = Reduction.SUM_BY_NONZERO_WEIGHTS): O {\n    util.assertArgumentsAreTensors({losses}, 'computeWeightedLoss');\n    if (weights != null) {\n      util.assertArgumentsAreTensors({weights}, 'computeWeightedLoss');\n    }\n\n    const weightedLoss = (weights == null) ? losses : losses.mul(weights);\n\n    if (reduction === Reduction.NONE) {\n      return weightedLoss as O;\n    }\n    if (reduction === Reduction.SUM) {\n      return weightedLoss.sum();\n    }\n    if (reduction === Reduction.MEAN) {\n      return (weights == null) ? weightedLoss.mean() :\n                                 weightedLoss.sum().div(weights.sum());\n    }\n    if (reduction === Reduction.SUM_BY_NONZERO_WEIGHTS) {\n      if (weights == null) {\n        return weightedLoss.sum().div(ops.scalar(losses.size));\n      } else {\n        const numNonZeros = weights.notEqual(ops.scalar(0)).sum().toFloat();\n        return weightedLoss.sum().div(numNonZeros);\n      }\n    }\n\n    throw Error(`Unknown reduction: ${reduction}`);\n  }\n\n  /**\n   * Computes the absolute difference loss between two tensors.\n   *\n   * @param labels The ground truth output tensor, same dimensions as\n   *    'predictions'.\n   * @param predictions The predicted outputs.\n   * @param weights Tensor whose rank is either 0, or the same rank as\n   *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n   *    must be either `1`, or the same as the corresponding `losses`\n   *    dimension).\n   * @param reduction Type of reduction to apply to loss. Should be of type\n   *    `Reduction`\n   */\n  @doc({heading: 'Training', subheading: 'Losses', namespace: 'losses'})\n  @operation\n  static absoluteDifference<T extends Tensor, O extends Tensor>(\n      labels: T, predictions: T, weights?: Tensor,\n      reduction = Reduction.SUM_BY_NONZERO_WEIGHTS): O {\n    util.assertArgumentsAreTensors({labels, predictions}, 'absoluteDifference');\n    if (weights != null) {\n      util.assertArgumentsAreTensors({weights}, 'absoluteDifference');\n    }\n    util.assertShapesMatch(\n        labels.shape, predictions.shape, 'Error in absoluteDifference: ');\n\n    const losses = labels.sub(predictions).abs();\n    return LossOps.computeWeightedLoss(losses, weights, reduction);\n  }\n\n  /**\n   * Computes the mean squared error between two tensors.\n   *\n   * @param labels The ground truth output tensor, same dimensions as\n   *    'predictions'.\n   * @param predictions The predicted outputs.\n   * @param weights Tensor whose rank is either 0, or the same rank as\n   *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n   *    must be either `1`, or the same as the corresponding `losses`\n   *    dimension).\n   * @param reduction Type of reduction to apply to loss. Should be of type\n   *    `Reduction`\n   */\n  @doc({heading: 'Training', subheading: 'Losses', namespace: 'losses'})\n  @operation\n  static meanSquaredError<T extends Tensor, O extends Tensor>(\n      labels: T, predictions: T, weights?: Tensor,\n      reduction = Reduction.SUM_BY_NONZERO_WEIGHTS): O {\n    util.assertArgumentsAreTensors({labels, predictions}, 'meanSquaredError');\n    if (weights != null) {\n      util.assertArgumentsAreTensors({weights}, 'meanSquaredError');\n    }\n    util.assertShapesMatch(\n        labels.shape, predictions.shape, 'Error in meanSquaredError: ');\n\n    const losses = labels.squaredDifference(predictions);\n    return LossOps.computeWeightedLoss(losses, weights, reduction);\n  }\n\n  /**\n   * Computes the cosine distance loss between two tensors.\n   *\n   * @param labels The ground truth output tensor, same dimensions as\n   *    'predictions'.\n   * @param predictions The predicted outputs.\n   * @param axis The dimension along which the cosine distance is computed.\n   * @param weights Tensor whose rank is either 0, or the same rank as\n   *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n   *    must be either `1`, or the same as the corresponding `losses`\n   *    dimension).\n   * @param reduction Type of reduction to apply to loss. Should be of type\n   *    `Reduction`\n   */\n  @doc({heading: 'Training', subheading: 'Losses', namespace: 'losses'})\n  @operation\n  static cosineDistance<T extends Tensor, O extends Tensor>(\n      labels: T, predictions: T, axis: number, weights?: Tensor,\n      reduction = Reduction.SUM_BY_NONZERO_WEIGHTS): O {\n    util.assertArgumentsAreTensors({labels, predictions}, 'cosineDistance');\n    if (weights != null) {\n      util.assertArgumentsAreTensors({weights}, 'cosineDistance');\n    }\n    util.assertShapesMatch(\n        labels.shape, predictions.shape, 'Error in cosineDistance: ');\n\n    const one = ops.scalar(1);\n    const losses = one.sub(labels.mul(predictions).sum(axis, true));\n    return LossOps.computeWeightedLoss(losses, weights, reduction);\n  }\n\n  /**\n   * Computes the Hinge loss between two tensors.\n   *\n   * @param labels The ground truth output tensor, same dimensions as\n   *    'predictions'.\n   * @param predictions The predicted outputs.\n   * @param weights Tensor whose rank is either 0, or the same rank as\n   *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n   *    must be either `1`, or the same as the corresponding `losses`\n   *    dimension).\n   * @param reduction Type of reduction to apply to loss. Should be of type\n   *    `Reduction`\n   */\n  @doc({heading: 'Training', subheading: 'Losses', namespace: 'losses'})\n  @operation\n  static hingeLoss<T extends Tensor, O extends Tensor>(\n      labels: T, predictions: T, weights?: Tensor,\n      reduction = Reduction.SUM_BY_NONZERO_WEIGHTS): O {\n    util.assertArgumentsAreTensors({labels, predictions}, 'hingeLoss');\n    if (weights != null) {\n      util.assertArgumentsAreTensors({weights}, 'hingeLoss');\n    }\n    util.assertShapesMatch(\n        labels.shape, predictions.shape, 'Error in hingeLoss: ');\n\n    const one = ops.scalar(1);\n    // Convert binary labels to (-1, 1)\n    labels = ops.scalar(2).mul(labels).sub(one);\n    const losses = one.sub(labels.mul(predictions)).relu();\n    return LossOps.computeWeightedLoss(losses, weights, reduction);\n  }\n\n  /**\n   * Computes the log loss between two tensors.\n   *\n   * @param labels The ground truth output tensor, same dimensions as\n   *    'predictions'.\n   * @param predictions The predicted outputs.\n   * @param weights Tensor whose rank is either 0, or the same rank as\n   *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n   *    must be either `1`, or the same as the corresponding `losses`\n   *    dimension).\n   * @param epsilon A small increment to avoid taking log of zero\n   * @param reduction Type of reduction to apply to loss. Should be of type\n   *    `Reduction`\n   */\n  @doc({heading: 'Training', subheading: 'Losses', namespace: 'losses'})\n  @operation\n  static logLoss<T extends Tensor, O extends Tensor>(\n      labels: T, predictions: T, weights?: Tensor, epsilon = 1e-7,\n      reduction = Reduction.SUM_BY_NONZERO_WEIGHTS): O {\n    util.assertArgumentsAreTensors({labels, predictions}, 'logLoss');\n    if (weights != null) {\n      util.assertArgumentsAreTensors({weights}, 'logLoss');\n    }\n    util.assertShapesMatch(\n        labels.shape, predictions.shape, 'Error in logLoss: ');\n\n    const one = ops.scalar(1);\n    const epsilonScalar = ops.scalar(epsilon);\n    const losses = labels.mul(predictions.add(epsilonScalar).log())\n                       .neg()\n                       .sub(one.sub(labels).mul(\n                           one.sub(predictions).add(epsilonScalar).log()));\n    return LossOps.computeWeightedLoss(losses, weights, reduction);\n  }\n\n  /**\n   * Computes the huber loss between two tensors.\n   *\n   * @param labels The ground truth output tensor, same dimensions as\n   *    'predictions'.\n   * @param predictions The predicted outputs.\n   * @param weights Tensor whose rank is either 0, or the same rank as\n   *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n   *    must be either `1`, or the same as the corresponding `losses`\n   *    dimension).\n   * @param delta Point where huber loss changes from quadratic to linear.\n   * @param reduction Type of reduction to apply to loss. Should be of type\n   *    `Reduction`.\n   */\n  @doc({heading: 'Training', subheading: 'Losses', namespace: 'losses'})\n  @operation\n  static huberLoss<T extends Tensor, O extends Tensor>(\n      labels: T, predictions: T, weights?: Tensor, delta = 1.0,\n      reduction = Reduction.SUM_BY_NONZERO_WEIGHTS): O {\n    util.assertArgumentsAreTensors({labels, predictions}, 'huberLoss');\n    if (weights != null) {\n      util.assertArgumentsAreTensors({weights}, 'huberLoss');\n    }\n    util.assertShapesMatch(\n        labels.shape, predictions.shape, 'Error in huberLoss: ');\n\n    const deltaScalar = ops.scalar(delta);\n    const error = predictions.sub(labels).abs();\n    const quadratic = ops.minimum(error, deltaScalar);\n    const linear = error.sub(quadratic);\n\n    const losses =\n        ops.scalar(0.5).mul(quadratic.square()).add(deltaScalar.mul(linear));\n    return LossOps.computeWeightedLoss(losses, weights, reduction);\n  }\n}\n"]}},"hash":"ca0671a442d34e3eba4a2309627e60eb","cacheData":{"env":{}}}