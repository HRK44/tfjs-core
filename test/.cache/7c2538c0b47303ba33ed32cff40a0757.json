{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/package.json","includedInParent":true,"mtime":1528810356568},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1528810356568},{"name":"../doc","loc":{"line":9,"column":20}},{"name":"../environment","loc":{"line":10,"column":28}},{"name":"../util","loc":{"line":11,"column":19}},{"name":"./operation","loc":{"line":12,"column":26}}],"generated":{"js":"\"use strict\";\nvar __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar doc_1 = require(\"../doc\");\nvar environment_1 = require(\"../environment\");\nvar util = require(\"../util\");\nvar operation_1 = require(\"./operation\");\nvar MatmulOps = (function () {\n    function MatmulOps() {\n    }\n    MatmulOps.matMul = function (a, b, transposeA, transposeB) {\n        if (transposeA === void 0) { transposeA = false; }\n        if (transposeB === void 0) { transposeB = false; }\n        util.assertArgumentsAreTensors({ a: a, b: b }, 'matMul');\n        var innerShapeA = transposeA ? a.shape[0] : a.shape[1];\n        var innerShapeB = transposeB ? b.shape[1] : b.shape[0];\n        util.assert(a.rank === 2 && b.rank === 2, \"Error in matMul: inputs must be rank 2, got ranks \" + a.rank +\n            (\" and \" + b.rank + \".\"));\n        util.assert(innerShapeA === innerShapeB, \"Error in matMul: inner shapes (\" + innerShapeA + \") and (\" +\n            (innerShapeB + \") of Tensors with shapes \" + a.shape + \" and \") +\n            (b.shape + \" and transposeA=\" + transposeA) +\n            (\" and transposeB=\" + transposeB + \" must match.\"));\n        var grad = function (dy) {\n            if (!transposeA && !transposeB) {\n                return {\n                    a: function () { return dy.matMul(b.toFloat(), false, true); },\n                    b: function () { return a.toFloat().matMul(dy, true, false); }\n                };\n            }\n            else if (!transposeA && transposeB) {\n                return {\n                    a: function () { return dy.matMul(b.toFloat(), false, false); },\n                    b: function () { return dy.matMul(a.toFloat(), true, false); }\n                };\n            }\n            else if (transposeA && !transposeB) {\n                return {\n                    a: function () { return b.toFloat().matMul(dy, false, true); },\n                    b: function () { return a.toFloat().matMul(dy, false, false); }\n                };\n            }\n            else {\n                return {\n                    a: function () { return b.toFloat().matMul(dy, true, true); },\n                    b: function () { return dy.matMul(a.toFloat(), true, true); }\n                };\n            }\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.matMul(a, b, transposeA, transposeB); }, { a: a, b: b }, grad);\n    };\n    MatmulOps.vectorTimesMatrix = function (v, matrix) {\n        util.assert(v.rank === 1, \"Error in vectorTimesMatrix: first input must be rank 1, but got \" +\n            (\"rank \" + v.rank + \".\"));\n        util.assert(matrix.rank === 2, \"Error in vectorTimesMatrix: second input must be rank 2, but got \" +\n            (\"rank \" + matrix.rank + \".\"));\n        util.assert(v.size === matrix.shape[0], \"Error in vectorTimesMatrix: size of vector (\" + v.size + \") \" +\n            (\"must match first dimension of matrix (\" + matrix.shape[0] + \")\"));\n        return v.as2D(1, -1).matMul(matrix).as1D();\n    };\n    MatmulOps.matrixTimesVector = function (matrix, v) {\n        util.assert(v.rank === 1, \"Error in matrixTimesVector: second input must rank 1, but got \" +\n            (\"rank \" + v.rank + \".\"));\n        util.assert(matrix.rank === 2, \"Error in matrixTimesVector: first input must be a rank 2, but got \" +\n            (\"rank \" + matrix.rank + \".\"));\n        util.assert(v.size === matrix.shape[1], \"Error in matrixTimesVector: size of first rank 1 input \" + v.size + \" \" +\n            \"must match inner dimension of second rank 2 input, but got \" +\n            (\"shape \" + matrix.shape + \".\"));\n        return matrix.matMul(v.as2D(-1, 1)).as1D();\n    };\n    MatmulOps.dotProduct = function (v1, v2) {\n        util.assert(v1.rank === 1 && v2.rank === 1, \"Error in dotProduct: inputs must be rank 1, but got ranks \" +\n            (v1.rank + \" and \" + v2.rank + \".\"));\n        util.assert(v1.size === v2.size, \"Error in dotProduct: size of inputs (\" + v1.size + \") and (\" +\n            (v2.size + \") must match.\"));\n        return v1.as2D(1, -1).matMul(v2.as2D(-1, 1)).asScalar();\n    };\n    MatmulOps.outerProduct = function (v1, v2) {\n        util.assert(v1.rank === 1 && v2.rank === 1, \"Error in outerProduct: inputs must be rank 1, but got ranks \" +\n            (v1.rank + \" and \" + v2.rank + \".\"));\n        return v1.as2D(-1, 1).matMul(v2.as2D(1, -1));\n    };\n    MatmulOps.dot = function (t1, t2) {\n        util.assert((t1.rank === 1 || t1.rank === 2) && (t2.rank === 1 || t2.rank === 2), \"Error in dot: inputs must all be rank 1 or 2, but got ranks \" +\n            (t1.rank + \" and \" + t2.rank + \".\"));\n        var t1Inner = (t1.rank === 1 ? t1.size : t1.shape[1]);\n        var t2Inner = (t2.rank === 1 ? t2.size : t2.shape[0]);\n        util.assert(t1Inner === t2Inner, \"Error in dot: inner dimensions of inputs must match, but got \" +\n            (t1Inner + \" and \" + t2Inner + \".\"));\n        if (t1.rank === 1 && t2.rank === 1) {\n            return t1.as2D(1, -1).matMul(t2.as2D(-1, 1)).asScalar();\n        }\n        else if (t1.rank === 1 && t2.rank === 2) {\n            return t1.as2D(1, -1).matMul(t2.as2D(t2.shape[0], t2.shape[1])).as1D();\n        }\n        else if (t1.rank === 2 && t2.rank === 1) {\n            return t1.matMul(t2.as2D(-1, 1)).as1D();\n        }\n        else {\n            return t1.matMul(t2.as2D(t2.shape[0], t2.shape[1]));\n        }\n    };\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Matrices' }),\n        operation_1.operation\n    ], MatmulOps, \"matMul\", null);\n    __decorate([\n        operation_1.operation\n    ], MatmulOps, \"vectorTimesMatrix\", null);\n    __decorate([\n        operation_1.operation\n    ], MatmulOps, \"matrixTimesVector\", null);\n    __decorate([\n        operation_1.operation\n    ], MatmulOps, \"dotProduct\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Matrices' }),\n        operation_1.operation\n    ], MatmulOps, \"outerProduct\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Matrices' }),\n        operation_1.operation\n    ], MatmulOps, \"dot\", null);\n    return MatmulOps;\n}());\nexports.MatmulOps = MatmulOps;\n","map":{"version":3,"file":"matmul.js","sourceRoot":"","sources":["../src/ops/matmul.ts"],"names":[],"mappings":";;;;;;;;AAiBA,8BAA2B;AAC3B,8CAAmC;AAEnC,8BAAgC;AAChC,yCAAsC;AAEtC;IAAA;IAmMA,CAAC;IAlLQ,gBAAM,GAAb,UACI,CAAW,EAAE,CAAW,EAAE,UAAkB,EAAE,UAAkB;QAAtC,2BAAA,EAAA,kBAAkB;QAAE,2BAAA,EAAA,kBAAkB;QAElE,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,QAAQ,CAAC,CAAC;QAEjD,IAAM,WAAW,GAAG,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;QACzD,IAAM,WAAW,GAAG,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;QAEzD,IAAI,CAAC,MAAM,CACP,CAAC,CAAC,IAAI,KAAK,CAAC,IAAI,CAAC,CAAC,IAAI,KAAK,CAAC,EAC5B,uDAAqD,CAAC,CAAC,IAAM;aACzD,UAAQ,CAAC,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAE3B,IAAI,CAAC,MAAM,CACP,WAAW,KAAK,WAAW,EAC3B,oCAAkC,WAAW,YAAS;aAC/C,WAAW,iCAA4B,CAAC,CAAC,KAAK,UAAO,CAAA;aACrD,CAAC,CAAC,KAAK,wBAAmB,UAAY,CAAA;aACzC,qBAAmB,UAAU,iBAAc,CAAA,CAAC,CAAC;QAErD,IAAM,IAAI,GAAG,UAAC,EAAY;YACxB,EAAE,CAAC,CAAC,CAAC,UAAU,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC;gBAC/B,MAAM,CAAC;oBACL,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,OAAO,EAAE,EAAE,KAAK,EAAE,IAAI,CAAC,EAAnC,CAAmC;oBAC5C,CAAC,EAAE,cAAM,OAAA,CAAC,CAAC,OAAO,EAAE,CAAC,MAAM,CAAC,EAAE,EAAE,IAAI,EAAE,KAAK,CAAC,EAAnC,CAAmC;iBAC7C,CAAC;YACJ,CAAC;YAAC,IAAI,CAAC,EAAE,CAAC,CAAC,CAAC,UAAU,IAAI,UAAU,CAAC,CAAC,CAAC;gBACrC,MAAM,CAAC;oBACL,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,OAAO,EAAE,EAAE,KAAK,EAAE,KAAK,CAAC,EAApC,CAAoC;oBAC7C,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,OAAO,EAAE,EAAE,IAAI,EAAE,KAAK,CAAC,EAAnC,CAAmC;iBAC7C,CAAC;YACJ,CAAC;YAAC,IAAI,CAAC,EAAE,CAAC,CAAC,UAAU,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC;gBACrC,MAAM,CAAC;oBACL,CAAC,EAAE,cAAM,OAAA,CAAC,CAAC,OAAO,EAAE,CAAC,MAAM,CAAC,EAAE,EAAE,KAAK,EAAE,IAAI,CAAC,EAAnC,CAAmC;oBAC5C,CAAC,EAAE,cAAM,OAAA,CAAC,CAAC,OAAO,EAAE,CAAC,MAAM,CAAC,EAAE,EAAE,KAAK,EAAE,KAAK,CAAC,EAApC,CAAoC;iBAC9C,CAAC;YACJ,CAAC;YAAC,IAAI,CAAC,CAAC;gBACN,MAAM,CAAC;oBACL,CAAC,EAAE,cAAM,OAAA,CAAC,CAAC,OAAO,EAAE,CAAC,MAAM,CAAC,EAAE,EAAE,IAAI,EAAE,IAAI,CAAC,EAAlC,CAAkC;oBAC3C,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,OAAO,EAAE,EAAE,IAAI,EAAE,IAAI,CAAC,EAAlC,CAAkC;iBAC5C,CAAC;YACJ,CAAC;QACH,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CACvB,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,MAAM,CAAC,CAAC,EAAE,CAAC,EAAE,UAAU,EAAE,UAAU,CAAC,EAA5C,CAA4C,EAAE,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IAC7E,CAAC;IASM,2BAAiB,GAAxB,UAAyB,CAAW,EAAE,MAAgB;QACpD,IAAI,CAAC,MAAM,CACP,CAAC,CAAC,IAAI,KAAK,CAAC,EACZ,kEAAkE;aAC9D,UAAQ,CAAC,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAC3B,IAAI,CAAC,MAAM,CACP,MAAM,CAAC,IAAI,KAAK,CAAC,EACjB,mEAAmE;aAC/D,UAAQ,MAAM,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAChC,IAAI,CAAC,MAAM,CACP,CAAC,CAAC,IAAI,KAAK,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,EAC1B,iDAA+C,CAAC,CAAC,IAAI,OAAI;aACrD,2CAAyC,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,MAAG,CAAA,CAAC,CAAC;QACrE,MAAM,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,CAAC;IAC7C,CAAC;IASM,2BAAiB,GAAxB,UAAyB,MAAgB,EAAE,CAAW;QACpD,IAAI,CAAC,MAAM,CACP,CAAC,CAAC,IAAI,KAAK,CAAC,EACZ,gEAAgE;aAC5D,UAAQ,CAAC,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAC3B,IAAI,CAAC,MAAM,CACP,MAAM,CAAC,IAAI,KAAK,CAAC,EACjB,oEAAoE;aAChE,UAAQ,MAAM,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAChC,IAAI,CAAC,MAAM,CACP,CAAC,CAAC,IAAI,KAAK,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,EAC1B,4DAA0D,CAAC,CAAC,IAAI,MAAG;YAC/D,6DAA6D;aAC7D,WAAS,MAAM,CAAC,KAAK,MAAG,CAAA,CAAC,CAAC;QAElC,MAAM,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,EAAE,CAAC;IAC7C,CAAC;IASM,oBAAU,GAAjB,UAAkB,EAAY,EAAE,EAAY;QAC1C,IAAI,CAAC,MAAM,CACP,EAAE,CAAC,IAAI,KAAK,CAAC,IAAI,EAAE,CAAC,IAAI,KAAK,CAAC,EAC9B,4DAA4D;aACrD,EAAE,CAAC,IAAI,aAAQ,EAAE,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QACtC,IAAI,CAAC,MAAM,CACP,EAAE,CAAC,IAAI,KAAK,EAAE,CAAC,IAAI,EACnB,0CAAwC,EAAE,CAAC,IAAI,YAAS;aACjD,EAAE,CAAC,IAAI,kBAAe,CAAA,CAAC,CAAC;QACnC,MAAM,CAAC,EAAE,CAAC,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,EAAE,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;IAC1D,CAAC;IAgBM,sBAAY,GAAnB,UAAoB,EAAY,EAAE,EAAY;QAC5C,IAAI,CAAC,MAAM,CACP,EAAE,CAAC,IAAI,KAAK,CAAC,IAAI,EAAE,CAAC,IAAI,KAAK,CAAC,EAC9B,8DAA8D;aACvD,EAAE,CAAC,IAAI,aAAQ,EAAE,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAEtC,MAAM,CAAC,EAAE,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,MAAM,CAAC,EAAE,CAAC,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IAC/C,CAAC;IAmBM,aAAG,GAAV,UAAW,EAAU,EAAE,EAAU;QAC/B,IAAI,CAAC,MAAM,CACP,CAAC,EAAE,CAAC,IAAI,KAAK,CAAC,IAAI,EAAE,CAAC,IAAI,KAAK,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,IAAI,KAAK,CAAC,IAAI,EAAE,CAAC,IAAI,KAAK,CAAC,CAAC,EACpE,8DAA8D;aACvD,EAAE,CAAC,IAAI,aAAQ,EAAE,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAEtC,IAAM,OAAO,GAAG,CAAC,EAAE,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;QACxD,IAAM,OAAO,GAAG,CAAC,EAAE,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;QAExD,IAAI,CAAC,MAAM,CACP,OAAO,KAAK,OAAO,EACnB,+DAA+D;aACxD,OAAO,aAAQ,OAAO,MAAG,CAAA,CAAC,CAAC;QAEtC,EAAE,CAAC,CAAC,EAAE,CAAC,IAAI,KAAK,CAAC,IAAI,EAAE,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;YACnC,MAAM,CAAC,EAAE,CAAC,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,EAAE,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAC1D,CAAC;QAAC,IAAI,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,IAAI,KAAK,CAAC,IAAI,EAAE,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;YAC1C,MAAM,CAAC,EAAE,CAAC,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,EAAE,CAAC,IAAI,CAAC,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,EAAE,CAAC;QACzE,CAAC;QAAC,IAAI,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,IAAI,KAAK,CAAC,IAAI,EAAE,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;YAC1C,MAAM,CAAC,EAAE,CAAC,MAAM,CAAC,EAAE,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,EAAE,CAAC;QAC1C,CAAC;QAAC,IAAI,CAAC,CAAC;YACN,MAAM,CAAC,EAAE,CAAC,MAAM,CAAC,EAAE,CAAC,IAAI,CAAC,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QACtD,CAAC;IACH,CAAC;IAjLD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,UAAU,EAAC,CAAC;QACpD,qBAAS;iCA8CT;IASD;QADC,qBAAS;4CAeT;IASD;QADC,qBAAS;4CAiBT;IASD;QADC,qBAAS;qCAWT;IAgBD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,UAAU,EAAC,CAAC;QACpD,qBAAS;uCAQT;IAmBD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,UAAU,EAAC,CAAC;QACpD,qBAAS;8BAwBT;IACH,gBAAC;CAAA,AAnMD,IAmMC;AAnMY,8BAAS","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {doc} from '../doc';\nimport {ENV} from '../environment';\nimport {Scalar, Tensor, Tensor1D, Tensor2D} from '../tensor';\nimport * as util from '../util';\nimport {operation} from './operation';\n\nexport class MatmulOps {\n  /**\n   * Computes the dot product of two matrices, A * B. These must be matrices.\n   *\n   * ```js\n   * const a = tf.tensor2d([1, 2], [1, 2]);\n   * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n   *\n   * a.matMul(b).print();  // or tf.matMul(a, b)\n   * ```\n   * @param a First matrix in dot product operation.\n   * @param b Second matrix in dot product operation.\n   * @param transposeA If true, `a` is transposed before multiplication.\n   * @param transposeB If true, `b` is transposed before multiplication.\n   */\n  @doc({heading: 'Operations', subheading: 'Matrices'})\n  @operation\n  static matMul(\n      a: Tensor2D, b: Tensor2D, transposeA = false, transposeB = false):\n      Tensor2D {\n    util.assertArgumentsAreTensors({a, b}, 'matMul');\n\n    const innerShapeA = transposeA ? a.shape[0] : a.shape[1];\n    const innerShapeB = transposeB ? b.shape[1] : b.shape[0];\n\n    util.assert(\n        a.rank === 2 && b.rank === 2,\n        `Error in matMul: inputs must be rank 2, got ranks ${a.rank}` +\n            ` and ${b.rank}.`);\n\n    util.assert(\n        innerShapeA === innerShapeB,\n        `Error in matMul: inner shapes (${innerShapeA}) and (` +\n            `${innerShapeB}) of Tensors with shapes ${a.shape} and ` +\n            `${b.shape} and transposeA=${transposeA}` +\n            ` and transposeB=${transposeB} must match.`);\n\n    const grad = (dy: Tensor2D) => {\n      if (!transposeA && !transposeB) {\n        return {\n          a: () => dy.matMul(b.toFloat(), false, true),\n          b: () => a.toFloat().matMul(dy, true, false)\n        };\n      } else if (!transposeA && transposeB) {\n        return {\n          a: () => dy.matMul(b.toFloat(), false, false),\n          b: () => dy.matMul(a.toFloat(), true, false)\n        };\n      } else if (transposeA && !transposeB) {\n        return {\n          a: () => b.toFloat().matMul(dy, false, true),\n          b: () => a.toFloat().matMul(dy, false, false)\n        };\n      } else {\n        return {\n          a: () => b.toFloat().matMul(dy, true, true),\n          b: () => dy.matMul(a.toFloat(), true, true)\n        };\n      }\n    };\n    return ENV.engine.runKernel(\n        backend => backend.matMul(a, b, transposeA, transposeB), {a, b}, grad);\n  }\n\n  /**\n   * Computes the dot product of a vector and a matrix, v * B.\n   *\n   * @param v The vector in dot product operation.\n   * @param matrix The matrix in dot product operation.\n   */\n  @operation\n  static vectorTimesMatrix(v: Tensor1D, matrix: Tensor2D): Tensor1D {\n    util.assert(\n        v.rank === 1,\n        `Error in vectorTimesMatrix: first input must be rank 1, but got ` +\n            `rank ${v.rank}.`);\n    util.assert(\n        matrix.rank === 2,\n        `Error in vectorTimesMatrix: second input must be rank 2, but got ` +\n            `rank ${matrix.rank}.`);\n    util.assert(\n        v.size === matrix.shape[0],\n        `Error in vectorTimesMatrix: size of vector (${v.size}) ` +\n            `must match first dimension of matrix (${matrix.shape[0]})`);\n    return v.as2D(1, -1).matMul(matrix).as1D();\n  }\n\n  /**\n   * Computes the dot product of a matrix and vector, A * v.\n   *\n   * @param matrix The matrix in dot product operation.\n   * @param v The vector in dot product operation.\n   */\n  @operation\n  static matrixTimesVector(matrix: Tensor2D, v: Tensor1D): Tensor1D {\n    util.assert(\n        v.rank === 1,\n        `Error in matrixTimesVector: second input must rank 1, but got ` +\n            `rank ${v.rank}.`);\n    util.assert(\n        matrix.rank === 2,\n        `Error in matrixTimesVector: first input must be a rank 2, but got ` +\n            `rank ${matrix.rank}.`);\n    util.assert(\n        v.size === matrix.shape[1],\n        `Error in matrixTimesVector: size of first rank 1 input ${v.size} ` +\n            `must match inner dimension of second rank 2 input, but got ` +\n            `shape ${matrix.shape}.`);\n\n    return matrix.matMul(v.as2D(-1, 1)).as1D();\n  }\n\n  /**\n   * Computes the dot product of two vectors, v1 * v2.\n   *\n   * @param v1 The first vector in the dot product operation.\n   * @param v2 The second vector in the dot product operation.\n   */\n  @operation\n  static dotProduct(v1: Tensor1D, v2: Tensor1D): Scalar {\n    util.assert(\n        v1.rank === 1 && v2.rank === 1,\n        `Error in dotProduct: inputs must be rank 1, but got ranks ` +\n            `${v1.rank} and ${v2.rank}.`);\n    util.assert(\n        v1.size === v2.size,\n        `Error in dotProduct: size of inputs (${v1.size}) and (` +\n            `${v2.size}) must match.`);\n    return v1.as2D(1, -1).matMul(v2.as2D(-1, 1)).asScalar();\n  }\n\n  /**\n   * Computes the outer product of two vectors, v1 and v2.\n   *\n   * ```js\n   * const a = tf.tensor1d([1, 2, 3]);\n   * const b = tf.tensor1d([3, 4, 5]);\n   *\n   * tf.outerProduct(a, b).print();\n   * ```\n   * @param v1 The first vector in the outer product operation.\n   * @param v2 The second vector in the dot product operation.\n   */\n  @doc({heading: 'Operations', subheading: 'Matrices'})\n  @operation\n  static outerProduct(v1: Tensor1D, v2: Tensor1D): Tensor2D {\n    util.assert(\n        v1.rank === 1 && v2.rank === 1,\n        `Error in outerProduct: inputs must be rank 1, but got ranks ` +\n            `${v1.rank} and ${v2.rank}.`);\n\n    return v1.as2D(-1, 1).matMul(v2.as2D(1, -1));\n  }\n\n  /**\n   * Computes the dot product of two matrices and/or vectors, t1 and t2.\n   *\n   * ```js\n   * const a = tf.tensor1d([1, 2]);\n   * const b = tf.tensor2d([[1, 2], [3, 4]]);\n   * const c = tf.tensor2d([[1, 2, 3], [4, 5, 6]]);\n   *\n   * a.dot(b).print();  // or tf.dot(a, b)\n   * b.dot(a).print();\n   * b.dot(c).print();\n   * ```\n   * @param t1 The first tensor in the dot operation.\n   * @param t2 The second tensor in the dot operation.\n   */\n  @doc({heading: 'Operations', subheading: 'Matrices'})\n  @operation\n  static dot(t1: Tensor, t2: Tensor): Tensor {\n    util.assert(\n        (t1.rank === 1 || t1.rank === 2) && (t2.rank === 1 || t2.rank === 2),\n        `Error in dot: inputs must all be rank 1 or 2, but got ranks ` +\n            `${t1.rank} and ${t2.rank}.`);\n\n    const t1Inner = (t1.rank === 1 ? t1.size : t1.shape[1]);\n    const t2Inner = (t2.rank === 1 ? t2.size : t2.shape[0]);\n\n    util.assert(\n        t1Inner === t2Inner,\n        `Error in dot: inner dimensions of inputs must match, but got ` +\n            `${t1Inner} and ${t2Inner}.`);\n\n    if (t1.rank === 1 && t2.rank === 1) {\n      return t1.as2D(1, -1).matMul(t2.as2D(-1, 1)).asScalar();\n    } else if (t1.rank === 1 && t2.rank === 2) {\n      return t1.as2D(1, -1).matMul(t2.as2D(t2.shape[0], t2.shape[1])).as1D();\n    } else if (t1.rank === 2 && t2.rank === 1) {\n      return t1.matMul(t2.as2D(-1, 1)).as1D();\n    } else {\n      return t1.matMul(t2.as2D(t2.shape[0], t2.shape[1]));\n    }\n  }\n}\n"]}},"hash":"96094b23b23b3ae5816adfd143e28ae9","cacheData":{"env":{}}}