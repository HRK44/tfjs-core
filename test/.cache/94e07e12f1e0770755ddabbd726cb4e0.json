{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/package.json","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1524062920943},{"name":"../doc","loc":{"line":9,"column":20}},{"name":"../environment","loc":{"line":10,"column":28}},{"name":"../globals","loc":{"line":11,"column":24}},{"name":"../util","loc":{"line":12,"column":19}},{"name":"./axis_util","loc":{"line":13,"column":24}},{"name":"./operation","loc":{"line":14,"column":26}},{"name":"./ops","loc":{"line":15,"column":18}}],"generated":{"js":"\"use strict\";\nvar __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar doc_1 = require(\"../doc\");\nvar environment_1 = require(\"../environment\");\nvar globals_1 = require(\"../globals\");\nvar util = require(\"../util\");\nvar axis_util = require(\"./axis_util\");\nvar operation_1 = require(\"./operation\");\nvar ops = require(\"./ops\");\nvar ReductionOps = (function () {\n    function ReductionOps() {\n    }\n    ReductionOps.logSumExp = function (x, axis, keepDims) {\n        if (axis === void 0) { axis = null; }\n        if (keepDims === void 0) { keepDims = false; }\n        util.assertArgumentsAreTensors({ x: x }, 'logSumExp');\n        var axes = axis_util.parseAxisParam(axis, x.shape);\n        var xMax = x.max(axes, true);\n        var a = x.sub(xMax);\n        var b = a.exp();\n        var c = b.sum(axes);\n        var d = c.log();\n        var res = xMax.reshape(d.shape).add(d);\n        if (keepDims) {\n            var newShape = axis_util.expandShapeToKeepDim(res.shape, axes);\n            return res.reshape(newShape);\n        }\n        return res;\n    };\n    ReductionOps.sum = function (x, axis, keepDims) {\n        if (axis === void 0) { axis = null; }\n        if (keepDims === void 0) { keepDims = false; }\n        util.assertArgumentsAreTensors({ x: x }, 'sum');\n        if (x.dtype === 'bool') {\n            x = x.toInt();\n        }\n        var axes = axis_util.parseAxisParam(axis, x.shape);\n        var customOp = globals_1.customGrad(function (x) {\n            var permutation = axis_util.getAxesPermutation(axes, x.rank);\n            var reductionAxes = axes;\n            var permutedX = x;\n            if (permutation != null) {\n                permutedX = x.transpose(permutation);\n                reductionAxes =\n                    axis_util.getInnerMostAxes(reductionAxes.length, x.rank);\n            }\n            var value = environment_1.ENV.engine.runKernel(function (backend) { return backend.sum(permutedX, reductionAxes); }, { permutedX: permutedX });\n            if (keepDims) {\n                var newShape = axis_util.expandShapeToKeepDim(value.shape, axes);\n                value = value.reshape(newShape);\n            }\n            var gradFunc = function (dy) {\n                var expandedDyShape = x.shape.slice();\n                axes.forEach(function (axis) {\n                    expandedDyShape[axis] = 1;\n                });\n                var expandedDy = dy.reshape(expandedDyShape);\n                var derX = expandedDy.mul(ops.ones(x.shape, 'float32'));\n                return derX;\n            };\n            return { value: value, gradFunc: gradFunc };\n        });\n        return customOp(x);\n    };\n    ReductionOps.mean = function (x, axis, keepDims) {\n        if (axis === void 0) { axis = null; }\n        if (keepDims === void 0) { keepDims = false; }\n        util.assertArgumentsAreTensors({ x: x }, 'mean');\n        var axes = axis_util.parseAxisParam(axis, x.shape);\n        var shapes = axis_util.computeOutAndReduceShapes(x.shape, axes);\n        var reduceShape = shapes[1];\n        var reduceSize = util.sizeFromShape(reduceShape);\n        var customOp = globals_1.customGrad(function (x) {\n            var reduceSizeScalar = ops.scalar(reduceSize);\n            var xReduce = reduceSizeScalar.dtype === x.dtype ?\n                x :\n                x.cast(reduceSizeScalar.dtype);\n            var res = xReduce.div(reduceSizeScalar);\n            var value = res.sum(axis, keepDims);\n            var gradFunc = function (dy) {\n                var expandedDyShape = x.shape.slice();\n                axes.forEach(function (axis) {\n                    expandedDyShape[axis] = 1;\n                });\n                var expandedDy = dy.reshape(expandedDyShape);\n                var derX = expandedDy.mul(ops.ones(x.shape, 'float32')).div(reduceSizeScalar);\n                return derX;\n            };\n            return { value: value, gradFunc: gradFunc };\n        });\n        return customOp(x);\n    };\n    ReductionOps.min = function (x, axis, keepDims) {\n        if (axis === void 0) { axis = null; }\n        if (keepDims === void 0) { keepDims = false; }\n        util.assertArgumentsAreTensors({ x: x }, 'min');\n        var origAxes = axis_util.parseAxisParam(axis, x.shape);\n        var axes = origAxes;\n        var permutedAxes = axis_util.getAxesPermutation(axes, x.rank);\n        if (permutedAxes != null) {\n            x = x.transpose(permutedAxes);\n            axes = axis_util.getInnerMostAxes(axes.length, x.rank);\n        }\n        var res = environment_1.ENV.engine.runKernel(function (backend) { return backend.min(x, axes); }, { x: x });\n        if (keepDims) {\n            var newShape = axis_util.expandShapeToKeepDim(res.shape, origAxes);\n            return res.reshape(newShape);\n        }\n        return res;\n    };\n    ReductionOps.max = function (x, axis, keepDims) {\n        if (axis === void 0) { axis = null; }\n        if (keepDims === void 0) { keepDims = false; }\n        util.assertArgumentsAreTensors({ x: x }, 'max');\n        var origAxes = axis_util.parseAxisParam(axis, x.shape);\n        var axes = origAxes;\n        var permutedAxes = axis_util.getAxesPermutation(axes, x.rank);\n        if (permutedAxes != null) {\n            x = x.transpose(permutedAxes);\n            axes = axis_util.getInnerMostAxes(axes.length, x.rank);\n        }\n        var res = environment_1.ENV.engine.runKernel(function (backend) { return backend.max(x, axes); }, { x: x });\n        if (keepDims) {\n            var newShape = axis_util.expandShapeToKeepDim(res.shape, origAxes);\n            return res.reshape(newShape);\n        }\n        return res;\n    };\n    ReductionOps.argMin = function (x, axis) {\n        if (axis === void 0) { axis = 0; }\n        util.assertArgumentsAreTensors({ x: x }, 'argMin');\n        if (axis == null) {\n            axis = 0;\n        }\n        var axes = axis_util.parseAxisParam(axis, x.shape);\n        var permutedAxes = axis_util.getAxesPermutation(axes, x.rank);\n        if (permutedAxes != null) {\n            x = x.transpose(permutedAxes);\n            axes = axis_util.getInnerMostAxes(axes.length, x.rank);\n        }\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.argMin(x, axes[0]); }, { x: x });\n    };\n    ReductionOps.argMax = function (x, axis) {\n        if (axis === void 0) { axis = 0; }\n        util.assertArgumentsAreTensors({ x: x }, 'argMax');\n        if (axis == null) {\n            axis = 0;\n        }\n        var axes = axis_util.parseAxisParam(axis, x.shape);\n        var permutedAxes = axis_util.getAxesPermutation(axes, x.rank);\n        if (permutedAxes != null) {\n            x = x.transpose(permutedAxes);\n            axes = axis_util.getInnerMostAxes(axes.length, x.rank);\n        }\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.argMax(x, axes[0]); }, { x: x });\n    };\n    ReductionOps.moments = function (x, axis, keepDims) {\n        if (axis === void 0) { axis = null; }\n        if (keepDims === void 0) { keepDims = false; }\n        util.assertArgumentsAreTensors({ x: x }, 'moments');\n        var axes = axis_util.parseAxisParam(axis, x.shape);\n        var mean = x.mean(axes, keepDims);\n        var keepDimsShape = mean.shape;\n        if (!keepDims) {\n            keepDimsShape = axis_util.expandShapeToKeepDim(mean.shape, axes);\n        }\n        var devSquared = x.toFloat().sub(mean.reshape(keepDimsShape)).square();\n        var variance = devSquared.mean(axes, keepDims);\n        return { mean: mean, variance: variance };\n    };\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Reduction' }),\n        operation_1.operation\n    ], ReductionOps, \"logSumExp\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Reduction' }),\n        operation_1.operation\n    ], ReductionOps, \"sum\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Reduction' }),\n        operation_1.operation\n    ], ReductionOps, \"mean\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Reduction' }),\n        operation_1.operation\n    ], ReductionOps, \"min\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Reduction' }),\n        operation_1.operation\n    ], ReductionOps, \"max\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Reduction' }),\n        operation_1.operation\n    ], ReductionOps, \"argMin\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Reduction' }),\n        operation_1.operation\n    ], ReductionOps, \"argMax\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Normalization' }),\n        operation_1.operation\n    ], ReductionOps, \"moments\", null);\n    return ReductionOps;\n}());\nexports.ReductionOps = ReductionOps;\n","map":{"version":3,"file":"reduction_ops.js","sourceRoot":"","sources":["../src/ops/reduction_ops.ts"],"names":[],"mappings":";;;;;;;;AAiBA,8BAA2B;AAC3B,8CAAmC;AACnC,sCAAsC;AAEtC,8BAAgC;AAChC,uCAAyC;AACzC,yCAAsC;AACtC,2BAA6B;AAE7B;IAAA;IAwYA,CAAC;IA1WQ,sBAAS,GAAhB,UACI,CAAS,EAAE,IAA4B,EAAE,QAAgB;QAA9C,qBAAA,EAAA,WAA4B;QAAE,yBAAA,EAAA,gBAAgB;QAC3D,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,WAAW,CAAC,CAAC;QAEjD,IAAM,IAAI,GAAG,SAAS,CAAC,cAAc,CAAC,IAAI,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QACrD,IAAM,IAAI,GAAG,CAAC,CAAC,GAAG,CAAC,IAAI,EAAE,IAAI,CAAgB,CAAC;QAC9C,IAAM,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC;QACtB,IAAM,CAAC,GAAG,CAAC,CAAC,GAAG,EAAE,CAAC;QAClB,IAAM,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC;QACtB,IAAM,CAAC,GAAG,CAAC,CAAC,GAAG,EAAE,CAAC;QAClB,IAAM,GAAG,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;QAEzC,EAAE,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;YACb,IAAM,QAAQ,GAAG,SAAS,CAAC,oBAAoB,CAAC,GAAG,CAAC,KAAK,EAAE,IAAI,CAAC,CAAC;YACjE,MAAM,CAAC,GAAG,CAAC,OAAO,CAAC,QAAQ,CAAM,CAAC;QACpC,CAAC;QACD,MAAM,CAAC,GAAQ,CAAC;IAClB,CAAC;IAgCM,gBAAG,GAAV,UACI,CAAS,EAAE,IAA4B,EAAE,QAAgB;QAA9C,qBAAA,EAAA,WAA4B;QAAE,yBAAA,EAAA,gBAAgB;QAC3D,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAE3C,EAAE,CAAC,CAAC,CAAC,CAAC,KAAK,KAAK,MAAM,CAAC,CAAC,CAAC;YACvB,CAAC,GAAG,CAAC,CAAC,KAAK,EAAE,CAAC;QAChB,CAAC;QACD,IAAM,IAAI,GAAG,SAAS,CAAC,cAAc,CAAC,IAAI,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QAIrD,IAAM,QAAQ,GAAG,oBAAU,CAAC,UAAA,CAAC;YAC3B,IAAM,WAAW,GAAG,SAAS,CAAC,kBAAkB,CAAC,IAAI,EAAE,CAAC,CAAC,IAAI,CAAC,CAAC;YAC/D,IAAI,aAAa,GAAG,IAAI,CAAC;YACzB,IAAI,SAAS,GAAG,CAAC,CAAC;YAClB,EAAE,CAAC,CAAC,WAAW,IAAI,IAAI,CAAC,CAAC,CAAC;gBACxB,SAAS,GAAG,CAAC,CAAC,SAAS,CAAC,WAAW,CAAC,CAAC;gBACrC,aAAa;oBACT,SAAS,CAAC,gBAAgB,CAAC,aAAa,CAAC,MAAM,EAAE,CAAC,CAAC,IAAI,CAAC,CAAC;YAC/D,CAAC;YACD,IAAI,KAAK,GAAG,iBAAG,CAAC,MAAM,CAAC,SAAS,CAC5B,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,GAAG,CAAC,SAAS,EAAE,aAAa,CAAC,EAArC,CAAqC,EAAE,EAAC,SAAS,WAAA,EAAC,CAAC,CAAC;YACnE,EAAE,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;gBACb,IAAM,QAAQ,GAAG,SAAS,CAAC,oBAAoB,CAAC,KAAK,CAAC,KAAK,EAAE,IAAI,CAAC,CAAC;gBACnE,KAAK,GAAG,KAAK,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC;YAClC,CAAC;YAED,IAAM,QAAQ,GAAG,UAAC,EAAU;gBAC1B,IAAM,eAAe,GAAG,CAAC,CAAC,KAAK,CAAC,KAAK,EAAE,CAAC;gBACxC,IAAI,CAAC,OAAO,CAAC,UAAA,IAAI;oBACf,eAAe,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;gBAC5B,CAAC,CAAC,CAAC;gBACH,IAAM,UAAU,GAAG,EAAE,CAAC,OAAO,CAAC,eAAe,CAAC,CAAC;gBAC/C,IAAM,IAAI,GAAG,UAAU,CAAC,GAAG,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,KAAK,EAAE,SAAS,CAAC,CAAC,CAAC;gBAC1D,MAAM,CAAC,IAAI,CAAC;YACd,CAAC,CAAC;YACF,MAAM,CAAC,EAAC,KAAK,OAAA,EAAE,QAAQ,UAAA,EAAC,CAAC;QAC3B,CAAC,CAAC,CAAC;QAEH,MAAM,CAAC,QAAQ,CAAC,CAAC,CAAM,CAAC;IAC1B,CAAC;IA+BM,iBAAI,GAAX,UACI,CAAS,EAAE,IAA4B,EAAE,QAAgB;QAA9C,qBAAA,EAAA,WAA4B;QAAE,yBAAA,EAAA,gBAAgB;QAC3D,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,MAAM,CAAC,CAAC;QAE5C,IAAM,IAAI,GAAG,SAAS,CAAC,cAAc,CAAC,IAAI,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QACrD,IAAM,MAAM,GAAG,SAAS,CAAC,yBAAyB,CAAC,CAAC,CAAC,KAAK,EAAE,IAAI,CAAC,CAAC;QAClE,IAAM,WAAW,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC;QAC9B,IAAM,UAAU,GAAG,IAAI,CAAC,aAAa,CAAC,WAAW,CAAC,CAAC;QAInD,IAAM,QAAQ,GAAG,oBAAU,CAAC,UAAA,CAAC;YAC3B,IAAM,gBAAgB,GAAG,GAAG,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC;YAEhD,IAAM,OAAO,GAAG,gBAAgB,CAAC,KAAK,KAAK,CAAC,CAAC,KAAK,CAAC,CAAC;gBAChD,CAAC,CAAC,CAAC;gBACH,CAAC,CAAC,IAAI,CAAC,gBAAgB,CAAC,KAAK,CAAC,CAAC;YACnC,IAAM,GAAG,GAAG,OAAO,CAAC,GAAG,CAAC,gBAAgB,CAAC,CAAC;YAC1C,IAAM,KAAK,GAAG,GAAG,CAAC,GAAG,CAAC,IAAI,EAAE,QAAQ,CAAC,CAAC;YAEtC,IAAM,QAAQ,GAAG,UAAC,EAAU;gBAC1B,IAAM,eAAe,GAAG,CAAC,CAAC,KAAK,CAAC,KAAK,EAAE,CAAC;gBACxC,IAAI,CAAC,OAAO,CAAC,UAAA,IAAI;oBACf,eAAe,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;gBAC5B,CAAC,CAAC,CAAC;gBACH,IAAM,UAAU,GAAG,EAAE,CAAC,OAAO,CAAC,eAAe,CAAC,CAAC;gBAC/C,IAAM,IAAI,GACN,UAAU,CAAC,GAAG,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,KAAK,EAAE,SAAS,CAAC,CAAC,CAAC,GAAG,CAAC,gBAAgB,CAAC,CAAC;gBACvE,MAAM,CAAC,IAAI,CAAC;YACd,CAAC,CAAC;YACF,MAAM,CAAC,EAAC,KAAK,OAAA,EAAE,QAAQ,UAAA,EAAC,CAAC;QAC3B,CAAC,CAAC,CAAC;QAEH,MAAM,CAAC,QAAQ,CAAC,CAAC,CAAM,CAAC;IAC1B,CAAC;IA+BM,gBAAG,GAAV,UACI,CAAS,EAAE,IAA4B,EAAE,QAAgB;QAA9C,qBAAA,EAAA,WAA4B;QAAE,yBAAA,EAAA,gBAAgB;QAC3D,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAE3C,IAAM,QAAQ,GAAG,SAAS,CAAC,cAAc,CAAC,IAAI,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QACzD,IAAI,IAAI,GAAG,QAAQ,CAAC;QACpB,IAAM,YAAY,GAAG,SAAS,CAAC,kBAAkB,CAAC,IAAI,EAAE,CAAC,CAAC,IAAI,CAAC,CAAC;QAChE,EAAE,CAAC,CAAC,YAAY,IAAI,IAAI,CAAC,CAAC,CAAC;YACzB,CAAC,GAAG,CAAC,CAAC,SAAS,CAAC,YAAY,CAAC,CAAC;YAC9B,IAAI,GAAG,SAAS,CAAC,gBAAgB,CAAC,IAAI,CAAC,MAAM,EAAE,CAAC,CAAC,IAAI,CAAC,CAAC;QACzD,CAAC;QACD,IAAM,GAAG,GAAG,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC,EAApB,CAAoB,EAAE,EAAC,CAAC,GAAA,EAAC,CAAC,CAAC;QACvE,EAAE,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;YACb,IAAM,QAAQ,GAAG,SAAS,CAAC,oBAAoB,CAAC,GAAG,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;YACrE,MAAM,CAAC,GAAG,CAAC,OAAO,CAAC,QAAQ,CAAM,CAAC;QACpC,CAAC;QACD,MAAM,CAAC,GAAQ,CAAC;IAClB,CAAC;IA+BM,gBAAG,GAAV,UACI,CAAS,EAAE,IAA4B,EAAE,QAAgB;QAA9C,qBAAA,EAAA,WAA4B;QAAE,yBAAA,EAAA,gBAAgB;QAC3D,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAE3C,IAAM,QAAQ,GAAG,SAAS,CAAC,cAAc,CAAC,IAAI,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QACzD,IAAI,IAAI,GAAG,QAAQ,CAAC;QACpB,IAAM,YAAY,GAAG,SAAS,CAAC,kBAAkB,CAAC,IAAI,EAAE,CAAC,CAAC,IAAI,CAAC,CAAC;QAChE,EAAE,CAAC,CAAC,YAAY,IAAI,IAAI,CAAC,CAAC,CAAC;YACzB,CAAC,GAAG,CAAC,CAAC,SAAS,CAAC,YAAY,CAAC,CAAC;YAC9B,IAAI,GAAG,SAAS,CAAC,gBAAgB,CAAC,IAAI,CAAC,MAAM,EAAE,CAAC,CAAC,IAAI,CAAC,CAAC;QACzD,CAAC;QACD,IAAM,GAAG,GAAG,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC,EAApB,CAAoB,EAAE,EAAC,CAAC,GAAA,EAAC,CAAC,CAAC;QACvE,EAAE,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;YACb,IAAM,QAAQ,GAAG,SAAS,CAAC,oBAAoB,CAAC,GAAG,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;YACrE,MAAM,CAAC,GAAG,CAAC,OAAO,CAAC,QAAQ,CAAM,CAAC;QACpC,CAAC;QACD,MAAM,CAAC,GAAQ,CAAC;IAClB,CAAC;IA2BM,mBAAM,GAAb,UAAgC,CAAS,EAAE,IAAQ;QAAR,qBAAA,EAAA,QAAQ;QACjD,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,QAAQ,CAAC,CAAC;QAE9C,EAAE,CAAC,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC;YACjB,IAAI,GAAG,CAAC,CAAC;QACX,CAAC;QACD,IAAI,IAAI,GAAG,SAAS,CAAC,cAAc,CAAC,IAAI,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QACnD,IAAM,YAAY,GAAG,SAAS,CAAC,kBAAkB,CAAC,IAAI,EAAE,CAAC,CAAC,IAAI,CAAC,CAAC;QAChE,EAAE,CAAC,CAAC,YAAY,IAAI,IAAI,CAAC,CAAC,CAAC;YACzB,CAAC,GAAG,CAAC,CAAC,SAAS,CAAC,YAAY,CAAC,CAAC;YAC9B,IAAI,GAAG,SAAS,CAAC,gBAAgB,CAAC,IAAI,CAAC,MAAM,EAAE,CAAC,CAAC,IAAI,CAAC,CAAC;QACzD,CAAC;QACD,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,MAAM,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,CAAC,CAAC,EAA1B,CAA0B,EAAE,EAAC,CAAC,GAAA,EAAC,CACjE,CAAC;IACR,CAAC;IA0BM,mBAAM,GAAb,UAAgC,CAAS,EAAE,IAAQ;QAAR,qBAAA,EAAA,QAAQ;QACjD,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,QAAQ,CAAC,CAAC;QAE9C,EAAE,CAAC,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC;YACjB,IAAI,GAAG,CAAC,CAAC;QACX,CAAC;QACD,IAAI,IAAI,GAAG,SAAS,CAAC,cAAc,CAAC,IAAI,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QACnD,IAAM,YAAY,GAAG,SAAS,CAAC,kBAAkB,CAAC,IAAI,EAAE,CAAC,CAAC,IAAI,CAAC,CAAC;QAChE,EAAE,CAAC,CAAC,YAAY,IAAI,IAAI,CAAC,CAAC,CAAC;YACzB,CAAC,GAAG,CAAC,CAAC,SAAS,CAAC,YAAY,CAAC,CAAC;YAC9B,IAAI,GAAG,SAAS,CAAC,gBAAgB,CAAC,IAAI,CAAC,MAAM,EAAE,CAAC,CAAC,IAAI,CAAC,CAAC;QACzD,CAAC;QAED,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,MAAM,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,CAAC,CAAC,EAA1B,CAA0B,EAAE,EAAC,CAAC,GAAA,EAAC,CACjE,CAAC;IACR,CAAC;IAgBM,oBAAO,GAAd,UAAe,CAAS,EAAE,IAA4B,EAAE,QAAgB;QAA9C,qBAAA,EAAA,WAA4B;QAAE,yBAAA,EAAA,gBAAgB;QAEtE,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,SAAS,CAAC,CAAC;QAE/C,IAAM,IAAI,GAAG,SAAS,CAAC,cAAc,CAAC,IAAI,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QACrD,IAAM,IAAI,GAAG,CAAC,CAAC,IAAI,CAAC,IAAI,EAAE,QAAQ,CAAC,CAAC;QACpC,IAAI,aAAa,GAAG,IAAI,CAAC,KAAK,CAAC;QAC/B,EAAE,CAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC;YACd,aAAa,GAAG,SAAS,CAAC,oBAAoB,CAAC,IAAI,CAAC,KAAK,EAAE,IAAI,CAAC,CAAC;QACnE,CAAC;QACD,IAAM,UAAU,GAAG,CAAC,CAAC,OAAO,EAAE,CAAC,GAAG,CAAC,IAAI,CAAC,OAAO,CAAC,aAAa,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC;QACzE,IAAM,QAAQ,GAAG,UAAU,CAAC,IAAI,CAAC,IAAI,EAAE,QAAQ,CAAC,CAAC;QACjD,MAAM,CAAC,EAAC,IAAI,MAAA,EAAE,QAAQ,UAAA,EAAC,CAAC;IAC1B,CAAC;IAzWD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,WAAW,EAAC,CAAC;QACrD,qBAAS;uCAkBT;IAgCD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,WAAW,EAAC,CAAC;QACrD,qBAAS;iCAyCT;IA+BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,WAAW,EAAC,CAAC;QACrD,qBAAS;kCAmCT;IA+BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,WAAW,EAAC,CAAC;QACrD,qBAAS;iCAkBT;IA+BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,WAAW,EAAC,CAAC;QACrD,qBAAS;iCAkBT;IA2BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,WAAW,EAAC,CAAC;QACrD,qBAAS;oCAeT;IA0BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,WAAW,EAAC,CAAC;QACrD,qBAAS;oCAgBT;IAgBD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,eAAe,EAAC,CAAC;QACzD,qBAAS;qCAcT;IACH,mBAAC;CAAA,AAxYD,IAwYC;AAxYY,oCAAY","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {doc} from '../doc';\nimport {ENV} from '../environment';\nimport {customGrad} from '../globals';\nimport {Tensor} from '../tensor';\nimport * as util from '../util';\nimport * as axis_util from './axis_util';\nimport {operation} from './operation';\nimport * as ops from './ops';\n\nexport class ReductionOps {\n  /**\n   * Computes the log(sum(exp(elements across the reduction dimensions)).\n   *\n   * Reduces the input along the dimensions given in `axis`. Unless `keepDims`\n   * is true, the rank of the array is reduced by 1 for each entry in `axis`.\n   * If `keepDims` is true, the reduced dimensions are retained with length 1.\n   * If `axis` has no entries, all dimensions are reduced, and an array with a\n   * single element is returned.\n   *\n   * ```js\n   * const x = tf.tensor1d([1, 2, 3]);\n   *\n   * x.logSumExp().print();  // or tf.logSumExp(x)\n   * ```\n   *\n   * ```js\n   * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n   *\n   * const axis = 1;\n   * x.logSumExp(axis).print();  // or tf.logSumExp(a, axis)\n   * ```\n   * @param x The input tensor.\n   * @param axis The dimension(s) to reduce. If null (the default),\n   *     reduces all dimensions.\n   * @param keepDims If true, retains reduced dimensions with length\n   *     of 1. Defaults to false.\n   */\n  @doc({heading: 'Operations', subheading: 'Reduction'})\n  @operation\n  static logSumExp<T extends Tensor>(\n      x: Tensor, axis: number|number[] = null, keepDims = false): T {\n    util.assertArgumentsAreTensors({x}, 'logSumExp');\n\n    const axes = axis_util.parseAxisParam(axis, x.shape);\n    const xMax = x.max(axes, true /* keepDims */);\n    const a = x.sub(xMax);\n    const b = a.exp();\n    const c = b.sum(axes);\n    const d = c.log();\n    const res = xMax.reshape(d.shape).add(d);\n\n    if (keepDims) {\n      const newShape = axis_util.expandShapeToKeepDim(res.shape, axes);\n      return res.reshape(newShape) as T;\n    }\n    return res as T;\n  }\n\n  /**\n   * Computes the sum of elements across dimensions of a `Tensor`.\n   *\n   * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n   * is true, the rank of the `Tensor` is reduced by 1 for each entry in `axes`.\n   * If `keepDims` is true, the reduced dimensions are retained with length 1.\n   * If axes has no entries, all dimensions are reduced, and a `Tensor` with a\n   * single element is returned.\n   *\n   * ```js\n   * const x = tf.tensor1d([1, 2, 3]);\n   *\n   * x.sum().print();  // or tf.sum(x)\n   * ```\n   *\n   * ```js\n   * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n   *\n   * const axis = 1;\n   * x.sum(axis).print();  // or tf.sum(x, axis)\n   * ```\n   *\n   * @param x The input tensor to compute the sum over. If the dtype is `bool`\n   *   it will be converted to `int32` and the output dtype will be `int32`.\n   * @param axis The dimension(s) to reduce. By default it reduces\n   *     all dimensions.\n   * @param keepDims If true, retains reduced dimensions with size 1.\n   */\n  @doc({heading: 'Operations', subheading: 'Reduction'})\n  @operation\n  static sum<T extends Tensor>(\n      x: Tensor, axis: number|number[] = null, keepDims = false): T {\n    util.assertArgumentsAreTensors({x}, 'sum');\n\n    if (x.dtype === 'bool') {\n      x = x.toInt();\n    }\n    const axes = axis_util.parseAxisParam(axis, x.shape);\n\n    // Use a custom gradient to bypass 2 gradient backprops since sum is used\n    // extremely often.\n    const customOp = customGrad(x => {\n      const permutation = axis_util.getAxesPermutation(axes, x.rank);\n      let reductionAxes = axes;\n      let permutedX = x;\n      if (permutation != null) {\n        permutedX = x.transpose(permutation);\n        reductionAxes =\n            axis_util.getInnerMostAxes(reductionAxes.length, x.rank);\n      }\n      let value = ENV.engine.runKernel(\n          backend => backend.sum(permutedX, reductionAxes), {permutedX});\n      if (keepDims) {\n        const newShape = axis_util.expandShapeToKeepDim(value.shape, axes);\n        value = value.reshape(newShape);\n      }\n\n      const gradFunc = (dy: Tensor) => {\n        const expandedDyShape = x.shape.slice();\n        axes.forEach(axis => {\n          expandedDyShape[axis] = 1;\n        });\n        const expandedDy = dy.reshape(expandedDyShape);\n        const derX = expandedDy.mul(ops.ones(x.shape, 'float32'));\n        return derX;\n      };\n      return {value, gradFunc};\n    });\n\n    return customOp(x) as T;\n  }\n\n  /**\n   * Computes the mean of elements across dimensions of a `Tensor`.\n   *\n   * Reduces `x` along the dimensions given in `axis`. Unless `keepDims` is\n   * true, the rank of the `Tensor` is reduced by 1 for each entry in `axis`.\n   * If `keepDims` is true, the reduced dimensions are retained with length 1.\n   * If `axis` has no entries, all dimensions are reduced, and a `Tensor` with\n   * a single element is returned.\n   *\n   * ```js\n   * const x = tf.tensor1d([1, 2, 3]);\n   *\n   * x.mean().print();  // or tf.mean(a)\n   * ```\n   *\n   * ```js\n   * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n   *\n   * const axis = 1;\n   * x.mean(axis).print();  // or tf.mean(x, axis)\n   * ```\n   *\n   * @param x The input tensor.\n   * @param axis The dimension(s) to reduce. By default it reduces\n   *     all dimensions.\n   * @param keepDims If true, retains reduced dimensions with size 1.\n   */\n  @doc({heading: 'Operations', subheading: 'Reduction'})\n  @operation\n  static mean<T extends Tensor>(\n      x: Tensor, axis: number|number[] = null, keepDims = false): T {\n    util.assertArgumentsAreTensors({x}, 'mean');\n\n    const axes = axis_util.parseAxisParam(axis, x.shape);\n    const shapes = axis_util.computeOutAndReduceShapes(x.shape, axes);\n    const reduceShape = shapes[1];\n    const reduceSize = util.sizeFromShape(reduceShape);\n\n    // Use a custom gradient to bypass 2 gradient backprops since mean is used\n    // extremely often.\n    const customOp = customGrad(x => {\n      const reduceSizeScalar = ops.scalar(reduceSize);\n      // Cast if needed.\n      const xReduce = reduceSizeScalar.dtype === x.dtype ?\n          x :\n          x.cast(reduceSizeScalar.dtype);\n      const res = xReduce.div(reduceSizeScalar);\n      const value = res.sum(axis, keepDims);\n\n      const gradFunc = (dy: Tensor) => {\n        const expandedDyShape = x.shape.slice();\n        axes.forEach(axis => {\n          expandedDyShape[axis] = 1;\n        });\n        const expandedDy = dy.reshape(expandedDyShape);\n        const derX =\n            expandedDy.mul(ops.ones(x.shape, 'float32')).div(reduceSizeScalar);\n        return derX;\n      };\n      return {value, gradFunc};\n    });\n\n    return customOp(x) as T;\n  }\n\n  /**\n   * Computes the minimum value from the input.\n   *\n   * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n   * is true, the rank of the array is reduced by 1 for each entry in `axes`.\n   * If `keepDims` is true, the reduced dimensions are retained with length 1.\n   * If `axes` has no entries, all dimensions are reduced, and an array with a\n   * single element is returned.\n   *\n   * ```js\n   * const x = tf.tensor1d([1, 2, 3]);\n   *\n   * x.min().print();  // or tf.min(x)\n   * ```\n   *\n   * ```js\n   * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n   *\n   * const axis = 1;\n   * x.min(axis).print();  // or tf.min(x, axis)\n   * ```\n   *\n   * @param x The input Tensor.\n   * @param axis The dimension(s) to reduce. By default it reduces\n   *     all dimensions.\n   * @param keepDims If true, retains reduced dimensions with size 1.\n   */\n  @doc({heading: 'Operations', subheading: 'Reduction'})\n  @operation\n  static min<T extends Tensor>(\n      x: Tensor, axis: number|number[] = null, keepDims = false): T {\n    util.assertArgumentsAreTensors({x}, 'min');\n\n    const origAxes = axis_util.parseAxisParam(axis, x.shape);\n    let axes = origAxes;\n    const permutedAxes = axis_util.getAxesPermutation(axes, x.rank);\n    if (permutedAxes != null) {\n      x = x.transpose(permutedAxes);\n      axes = axis_util.getInnerMostAxes(axes.length, x.rank);\n    }\n    const res = ENV.engine.runKernel(backend => backend.min(x, axes), {x});\n    if (keepDims) {\n      const newShape = axis_util.expandShapeToKeepDim(res.shape, origAxes);\n      return res.reshape(newShape) as T;\n    }\n    return res as T;\n  }\n\n  /**\n   * Computes the maximum of elements across dimensions of a `Tensor`.\n   *\n   * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n   * is true, the rank of the `Tensor` is reduced by 1 for each entry in `axes`.\n   * If `keepDims` is true, the reduced dimensions are retained with length 1.\n   * If `axes` has no entries, all dimensions are reduced, and an `Tensor` with\n   * a single element is returned.\n   *\n   * ```js\n   * const x = tf.tensor1d([1, 2, 3]);\n   *\n   * x.max().print();  // or tf.max(x)\n   * ```\n   *\n   * ```js\n   * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n   *\n   * const axis = 1;\n   * x.max(axis).print();  // or tf.max(x, axis)\n   * ```\n   *\n   * @param x The input tensor.\n   * @param axis The dimension(s) to reduce. By default it reduces\n   *     all dimensions.\n   * @param keepDims If true, retains reduced dimensions with size 1.\n   */\n  @doc({heading: 'Operations', subheading: 'Reduction'})\n  @operation\n  static max<T extends Tensor>(\n      x: Tensor, axis: number|number[] = null, keepDims = false): T {\n    util.assertArgumentsAreTensors({x}, 'max');\n\n    const origAxes = axis_util.parseAxisParam(axis, x.shape);\n    let axes = origAxes;\n    const permutedAxes = axis_util.getAxesPermutation(axes, x.rank);\n    if (permutedAxes != null) {\n      x = x.transpose(permutedAxes);\n      axes = axis_util.getInnerMostAxes(axes.length, x.rank);\n    }\n    const res = ENV.engine.runKernel(backend => backend.max(x, axes), {x});\n    if (keepDims) {\n      const newShape = axis_util.expandShapeToKeepDim(res.shape, origAxes);\n      return res.reshape(newShape) as T;\n    }\n    return res as T;\n  }\n\n  /**\n   * Returns the indices of the minimum values along an `axis`.\n   *\n   * The result has the same shape as `input` with the dimension along `axis`\n   * removed.\n   *\n   * ```js\n   * const x = tf.tensor1d([1, 2, 3]);\n   *\n   * x.argMin().print();  // or tf.argMin(x)\n   * ```\n   *\n   * ```js\n   * const x = tf.tensor2d([1, 2, 4, 3], [2, 2]);\n   *\n   * const axis = 1;\n   * x.argMin(axis).print();  // or tf.argMin(x, axis)\n   * ```\n   *\n   * @param x The input tensor.\n   * @param axis The dimension to reduce. Defaults to 0 (outer-most dimension).\n   *\n   */\n  @doc({heading: 'Operations', subheading: 'Reduction'})\n  @operation\n  static argMin<T extends Tensor>(x: Tensor, axis = 0): T {\n    util.assertArgumentsAreTensors({x}, 'argMin');\n\n    if (axis == null) {\n      axis = 0;\n    }\n    let axes = axis_util.parseAxisParam(axis, x.shape);\n    const permutedAxes = axis_util.getAxesPermutation(axes, x.rank);\n    if (permutedAxes != null) {\n      x = x.transpose(permutedAxes);\n      axes = axis_util.getInnerMostAxes(axes.length, x.rank);\n    }\n    return ENV.engine.runKernel(backend => backend.argMin(x, axes[0]), {x}) as\n        T;\n  }\n\n  /**\n   * Returns the indices of the maximum values along an `axis`.\n   *\n   * The result has the same shape as `input` with the dimension along `axis`\n   * removed.\n   *\n   * ```js\n   * const x = tf.tensor1d([1, 2, 3]);\n   *\n   * x.argMax().print();  // or tf.argMax(x)\n   * ```\n   *\n   * ```js\n   * const x = tf.tensor2d([1, 2, 4, 3], [2, 2]);\n   *\n   * const axis = 1;\n   * x.argMax(axis).print();  // or tf.argMax(x, axis)\n   * ```\n   *\n   * @param x The input tensor.\n   * @param axis The dimension to reduce. Defaults to 0 (outer-most dimension).\n   */\n  @doc({heading: 'Operations', subheading: 'Reduction'})\n  @operation\n  static argMax<T extends Tensor>(x: Tensor, axis = 0): T {\n    util.assertArgumentsAreTensors({x}, 'argMax');\n\n    if (axis == null) {\n      axis = 0;\n    }\n    let axes = axis_util.parseAxisParam(axis, x.shape);\n    const permutedAxes = axis_util.getAxesPermutation(axes, x.rank);\n    if (permutedAxes != null) {\n      x = x.transpose(permutedAxes);\n      axes = axis_util.getInnerMostAxes(axes.length, x.rank);\n    }\n\n    return ENV.engine.runKernel(backend => backend.argMax(x, axes[0]), {x}) as\n        T;\n  }\n\n  /**\n   * Calculates the mean and variance of `x`. The mean and variance are\n   * calculated by aggregating the contents of `x` across `axes`. If `x` is\n   * 1-D and `axes = [0]` this is just the mean and variance of a vector.\n   *\n   * @param x The input tensor.\n   * @param axis The dimension(s) along with to compute mean and\n   *     variance. By default it reduces all dimensions.\n   * @param keepDims If true, the moments have the same dimensionality as the\n   *     input.\n   * @return An object with two keys: `mean` and `variance`.\n   */\n  @doc({heading: 'Operations', subheading: 'Normalization'})\n  @operation\n  static moments(x: Tensor, axis: number|number[] = null, keepDims = false):\n      {mean: Tensor, variance: Tensor} {\n    util.assertArgumentsAreTensors({x}, 'moments');\n\n    const axes = axis_util.parseAxisParam(axis, x.shape);\n    const mean = x.mean(axes, keepDims);\n    let keepDimsShape = mean.shape;\n    if (!keepDims) {\n      keepDimsShape = axis_util.expandShapeToKeepDim(mean.shape, axes);\n    }\n    const devSquared = x.toFloat().sub(mean.reshape(keepDimsShape)).square();\n    const variance = devSquared.mean(axes, keepDims);\n    return {mean, variance};\n  }\n}\n"]}},"hash":"2dd5ea8000561afd212c29f7300c9fa1","cacheData":{"env":{}}}