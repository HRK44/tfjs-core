{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/package.json","includedInParent":true,"mtime":1528810356568},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1528810356568},{"name":"../doc","loc":{"line":9,"column":20}},{"name":"../tracking","loc":{"line":10,"column":25}},{"name":"../util","loc":{"line":11,"column":21}},{"name":"./operation","loc":{"line":12,"column":26}},{"name":"./ops","loc":{"line":13,"column":20}}],"generated":{"js":"\"use strict\";\nvar __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar doc_1 = require(\"../doc\");\nvar tracking_1 = require(\"../tracking\");\nvar util_1 = require(\"../util\");\nvar operation_1 = require(\"./operation\");\nvar ops_1 = require(\"./ops\");\nvar LinalgOps = (function () {\n    function LinalgOps() {\n    }\n    LinalgOps.gramSchmidt = function (xs) {\n        var inputIsTensor2D;\n        if (Array.isArray(xs)) {\n            inputIsTensor2D = false;\n            util_1.assert(xs != null && xs.length > 0, 'Gram-Schmidt process: input must not be null, undefined, or empty');\n            var dim = xs[0].shape[0];\n            for (var i = 1; i < xs.length; ++i) {\n                util_1.assert(xs[i].shape[0] === dim, 'Gram-Schmidt: Non-unique lengths found in the input vectors: ' +\n                    (\"(\" + xs[i].shape[0] + \" vs. \" + dim + \")\"));\n            }\n        }\n        else {\n            inputIsTensor2D = true;\n            xs = ops_1.split(xs, xs.shape[0], 0).map(function (x) { return ops_1.squeeze(x, [0]); });\n        }\n        util_1.assert(xs.length <= xs[0].shape[0], \"Gram-Schmidt: Number of vectors (\" + xs.length + \") exceeds \" +\n            (\"number of dimensions (\" + xs[0].shape[0] + \").\"));\n        var ys = [];\n        var xs1d = xs;\n        var _loop_1 = function (i) {\n            ys.push(tracking_1.Tracking.tidy(function () {\n                var x = xs1d[i];\n                if (i > 0) {\n                    for (var j = 0; j < i; ++j) {\n                        var proj = ops_1.sum(ys[j].mulStrict(x)).mul(ys[j]);\n                        x = x.sub(proj);\n                    }\n                }\n                return x.div(ops_1.norm(x, 'euclidean'));\n            }));\n        };\n        for (var i = 0; i < xs.length; ++i) {\n            _loop_1(i);\n        }\n        if (inputIsTensor2D) {\n            return ops_1.stack(ys, 0);\n        }\n        else {\n            return ys;\n        }\n    };\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Linear Algebra' }),\n        operation_1.operation\n    ], LinalgOps, \"gramSchmidt\", null);\n    return LinalgOps;\n}());\nexports.LinalgOps = LinalgOps;\n","map":{"version":3,"file":"linalg_ops.js","sourceRoot":"","sources":["../src/ops/linalg_ops.ts"],"names":[],"mappings":";;;;;;;;AAqBA,8BAA2B;AAE3B,wCAAqC;AACrC,gCAA+B;AAE/B,yCAAsC;AACtC,6BAAuD;AAEvD;IAAA;IA+DA,CAAC;IA7CQ,qBAAW,GAAlB,UAAmB,EAAuB;QACxC,IAAI,eAAwB,CAAC;QAC7B,EAAE,CAAC,CAAC,KAAK,CAAC,OAAO,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;YACtB,eAAe,GAAG,KAAK,CAAC;YACxB,aAAM,CACF,EAAE,IAAI,IAAI,IAAI,EAAE,CAAC,MAAM,GAAG,CAAC,EAC3B,mEAAmE,CAAC,CAAC;YACzE,IAAM,GAAG,GAAG,EAAE,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;YAC3B,GAAG,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE,CAAC;gBACnC,aAAM,CACF,EAAE,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,GAAG,EACtB,+DAA+D;qBAC3D,MAAI,EAAE,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,aAAQ,GAAG,MAAG,CAAA,CAAC,CAAC;YAC5C,CAAC;QACH,CAAC;QAAC,IAAI,CAAC,CAAC;YACN,eAAe,GAAG,IAAI,CAAC;YACvB,EAAE,GAAG,WAAK,CAAC,EAAE,EAAE,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,aAAO,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAf,CAAe,CAAC,CAAC;QAC3D,CAAC;QAED,aAAM,CACF,EAAE,CAAC,MAAM,IAAI,EAAE,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,EAC3B,sCAAoC,EAAE,CAAC,MAAM,eAAY;aACrD,2BAAyB,EAAE,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,OAAI,CAAA,CAAC,CAAC;QAErD,IAAM,EAAE,GAAe,EAAE,CAAC;QAC1B,IAAM,IAAI,GAAG,EAAgB,CAAC;gCACrB,CAAC;YACR,EAAE,CAAC,IAAI,CAAC,mBAAQ,CAAC,IAAI,CAAC;gBACpB,IAAI,CAAC,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;gBAChB,EAAE,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;oBACV,GAAG,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC;wBAC3B,IAAM,IAAI,GAAG,SAAG,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;wBAChD,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC;oBAClB,CAAC;gBACH,CAAC;gBACD,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,UAAI,CAAC,CAAC,EAAE,WAAW,CAAC,CAAC,CAAC;YACrC,CAAC,CAAC,CAAC,CAAC;QACN,CAAC;QAXD,GAAG,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,CAAC,MAAM,EAAE,EAAE,CAAC;oBAAzB,CAAC;SAWT;QAED,EAAE,CAAC,CAAC,eAAe,CAAC,CAAC,CAAC;YACpB,MAAM,CAAC,WAAK,CAAC,EAAE,EAAE,CAAC,CAAa,CAAC;QAClC,CAAC;QAAC,IAAI,CAAC,CAAC;YACN,MAAM,CAAC,EAAE,CAAC;QACZ,CAAC;IACH,CAAC;IA5CD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,gBAAgB,EAAC,CAAC;QAC1D,qBAAS;sCA6CT;IACH,gBAAC;CAAA,AA/DD,IA+DC;AA/DY,8BAAS","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n/**\n * Linear algebra ops.\n */\n\nimport {doc} from '../doc';\nimport {Tensor1D, Tensor2D} from '../tensor';\nimport {Tracking} from '../tracking';\nimport {assert} from '../util';\n\nimport {operation} from './operation';\nimport {norm, split, squeeze, stack, sum} from './ops';\n\nexport class LinalgOps {\n  /**\n   * Gram-Schmidt orthogonalization.\n   *\n   * @param xs The vectors to be orthogonalized, in one of the two following\n   *   formats:\n   *   - An Array of `Tensor1D`.\n   *   - A `Tensor2D`, i.e., a matrix, in which case the vectors are the rows\n   *     of `xs`.\n   *   In each case, all the vectors must have the same length and the length\n   *   must be greater than or equal to the number of vectors.\n   * @returns The orthogonalized and normalized vectors or matrix.\n   *   Orthogonalization means that the vectors or the rows of the matrix\n   *   are orthogonal (zero inner products). Normalization means that each\n   *   vector or each row of the matrix has an L2 norm that equals `1`.\n   */\n  @doc({heading: 'Operations', subheading: 'Linear Algebra'})\n  @operation\n  static gramSchmidt(xs: Tensor1D[]|Tensor2D): Tensor1D[]|Tensor2D {\n    let inputIsTensor2D: boolean;\n    if (Array.isArray(xs)) {\n      inputIsTensor2D = false;\n      assert(\n          xs != null && xs.length > 0,\n          'Gram-Schmidt process: input must not be null, undefined, or empty');\n      const dim = xs[0].shape[0];\n      for (let i = 1; i < xs.length; ++i) {\n        assert(\n            xs[i].shape[0] === dim,\n            'Gram-Schmidt: Non-unique lengths found in the input vectors: ' +\n                `(${xs[i].shape[0]} vs. ${dim})`);\n      }\n    } else {\n      inputIsTensor2D = true;\n      xs = split(xs, xs.shape[0], 0).map(x => squeeze(x, [0]));\n    }\n\n    assert(\n        xs.length <= xs[0].shape[0],\n        `Gram-Schmidt: Number of vectors (${xs.length}) exceeds ` +\n            `number of dimensions (${xs[0].shape[0]}).`);\n\n    const ys: Tensor1D[] = [];\n    const xs1d = xs as Tensor1D[];\n    for (let i = 0; i < xs.length; ++i) {\n      ys.push(Tracking.tidy(() => {\n        let x = xs1d[i];\n        if (i > 0) {\n          for (let j = 0; j < i; ++j) {\n            const proj = sum(ys[j].mulStrict(x)).mul(ys[j]);\n            x = x.sub(proj);\n          }\n        }\n        return x.div(norm(x, 'euclidean'));\n      }));\n    }\n\n    if (inputIsTensor2D) {\n      return stack(ys, 0) as Tensor2D;\n    } else {\n      return ys;\n    }\n  }\n}\n"]}},"hash":"c6f2fbc89e9ddd099f767b6c846c38af","cacheData":{"env":{}}}