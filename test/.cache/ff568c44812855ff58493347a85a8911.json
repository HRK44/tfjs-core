{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/package.json","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1524062920943},{"name":"../doc","loc":{"line":19,"column":20}},{"name":"../globals","loc":{"line":20,"column":24}},{"name":"../serialization","loc":{"line":21,"column":30}}],"generated":{"js":"\"use strict\";\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = Object.setPrototypeOf ||\n        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n        function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nvar __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar doc_1 = require(\"../doc\");\nvar globals_1 = require(\"../globals\");\nvar serialization_1 = require(\"../serialization\");\nvar Optimizer = (function (_super) {\n    __extends(Optimizer, _super);\n    function Optimizer() {\n        return _super !== null && _super.apply(this, arguments) || this;\n    }\n    Optimizer.prototype.minimize = function (f, returnCost, varList) {\n        if (returnCost === void 0) { returnCost = false; }\n        var _a = this.computeGradients(f, varList), value = _a.value, grads = _a.grads;\n        this.applyGradients(grads);\n        var varNames = Object.keys(grads);\n        varNames.forEach(function (varName) { return grads[varName].dispose(); });\n        if (returnCost) {\n            return value;\n        }\n        else {\n            value.dispose();\n            return null;\n        }\n    };\n    Optimizer.prototype.computeGradients = function (f, varList) {\n        return globals_1.variableGrads(f, varList);\n    };\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Optimizers' })\n    ], Optimizer.prototype, \"minimize\", null);\n    Optimizer = __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Classes', namespace: 'train' })\n    ], Optimizer);\n    return Optimizer;\n}(serialization_1.Serializable));\nexports.Optimizer = Optimizer;\n","map":{"version":3,"file":"optimizer.js","sourceRoot":"","sources":["../src/optimizers/optimizer.ts"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;AAiBA,8BAA2B;AAC3B,sCAAyC;AACzC,kDAA8C;AAK9C;IAAwC,6BAAY;IAApD;;IAmDA,CAAC;IAtCC,4BAAQ,GAAR,UAAS,CAAe,EAAE,UAAkB,EAAE,OAAoB;QAAxC,2BAAA,EAAA,kBAAkB;QAEpC,IAAA,sCAAkD,EAAjD,gBAAK,EAAE,gBAAK,CAAsC;QAEzD,IAAI,CAAC,cAAc,CAAC,KAAK,CAAC,CAAC;QAG3B,IAAM,QAAQ,GAAG,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;QACpC,QAAQ,CAAC,OAAO,CAAC,UAAA,OAAO,IAAI,OAAA,KAAK,CAAC,OAAO,CAAC,CAAC,OAAO,EAAE,EAAxB,CAAwB,CAAC,CAAC;QAEtD,EAAE,CAAC,CAAC,UAAU,CAAC,CAAC,CAAC;YACf,MAAM,CAAC,KAAe,CAAC;QACzB,CAAC;QAAC,IAAI,CAAC,CAAC;YACN,KAAK,CAAC,OAAO,EAAE,CAAC;YAChB,MAAM,CAAC,IAAI,CAAC;QACd,CAAC;IACH,CAAC;IAYD,oCAAgB,GAAhB,UAAiB,CAAe,EAAE,OAAoB;QAEpD,MAAM,CAAC,uBAAa,CAAC,CAAC,EAAE,OAAO,CAAC,CAAC;IACnC,CAAC;IA/BD;QADC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;6CAiBpD;IA7BmB,SAAS;QAD9B,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,SAAS,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC;OAChD,SAAS,CAmD9B;IAAD,gBAAC;CAAA,AAnDD,CAAwC,4BAAY,GAmDnD;AAnDqB,8BAAS","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {doc} from '../doc';\nimport {variableGrads} from '../globals';\nimport {Serializable} from '../serialization';\nimport {Scalar, Variable} from '../tensor';\nimport {NamedTensorMap} from '../types';\n\n@doc({heading: 'Training', subheading: 'Classes', namespace: 'train'})\nexport abstract class Optimizer extends Serializable {\n  /**\n   * Executes `f()` and minimizes the scalar output of `f()` by computing\n   * gradients of y with respect to the list of trainable variables provided by\n   * `varList`. If no list is provided, it defaults to all trainable variables.\n   * @param f The function to execute and whose output to minimize.\n   * @param returnCost Whether to return the scalar cost value produced by\n   * executing `f()`.\n   * @param varList An optional list of variables to update. If specified, only\n   * the trainable variables in varList will be updated by minimize. Defaults to\n   * all trainable variables.\n   */\n  @doc({heading: 'Training', subheading: 'Optimizers'})\n  minimize(f: () => Scalar, returnCost = false, varList?: Variable[]): Scalar\n      |null {\n    const {value, grads} = this.computeGradients(f, varList);\n\n    this.applyGradients(grads);\n\n    // Dispose gradients.\n    const varNames = Object.keys(grads);\n    varNames.forEach(varName => grads[varName].dispose());\n\n    if (returnCost) {\n      return value as Scalar;\n    } else {\n      value.dispose();\n      return null;\n    }\n  }\n\n  /**\n   * Executes f() and computes the gradient of the scalar output of f() with\n   * respect to the list of trainable variables provided by `varList`. If no\n   * list is provided, it defaults to all trainable variables.\n   * @param f The function to execute and whose output to use for computing\n   * gradients with respect to variables.\n   * @param varList An optional list of variables to compute gradients with\n   * respect to. If specified, only the trainable variables in varList will have\n   * gradients computed with respect to. Defaults to all trainable variables.\n   */\n  computeGradients(f: () => Scalar, varList?: Variable[]):\n      {value: Scalar, grads: NamedTensorMap} {\n    return variableGrads(f, varList);\n  }\n\n  /**\n   * Updates variables by using the computed gradients.\n   * @param variableGradients A mapping of variable name to its gradient value.\n   */\n  abstract applyGradients(variableGradients: NamedTensorMap): void;\n}\n"]}},"hash":"46812d1aaf8d5966804a12ec8f6fc436","cacheData":{"env":{}}}