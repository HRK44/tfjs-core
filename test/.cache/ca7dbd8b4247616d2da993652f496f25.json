{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/package.json","includedInParent":true,"mtime":1528810356568},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1528810356568},{"name":"../doc","loc":{"line":9,"column":20}},{"name":"../environment","loc":{"line":10,"column":28}},{"name":"../util","loc":{"line":11,"column":19}},{"name":"./array_ops","loc":{"line":12,"column":26}},{"name":"./broadcast_util","loc":{"line":13,"column":31}},{"name":"./operation","loc":{"line":14,"column":26}},{"name":"./ops","loc":{"line":15,"column":20}}],"generated":{"js":"\"use strict\";\nvar __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar doc_1 = require(\"../doc\");\nvar environment_1 = require(\"../environment\");\nvar util = require(\"../util\");\nvar array_ops_1 = require(\"./array_ops\");\nvar broadcast_util_1 = require(\"./broadcast_util\");\nvar operation_1 = require(\"./operation\");\nvar ops_1 = require(\"./ops\");\nvar BatchNormOps = (function () {\n    function BatchNormOps() {\n    }\n    BatchNormOps.batchNormalization2d = function (x, mean, variance, varianceEpsilon, scale, offset) {\n        if (varianceEpsilon === void 0) { varianceEpsilon = .001; }\n        util.assert(x.rank === 2, \"Error in batchNormalization3D: x must be rank 3 but got rank \" +\n            (x.rank + \".\"));\n        util.assert(mean.rank === 2 || mean.rank === 1, \"Error in batchNormalization2D: mean must be rank 2 or rank 1 but \" +\n            (\"got rank \" + mean.rank + \".\"));\n        util.assert(variance.rank === 2 || variance.rank === 1, \"Error in batchNormalization2D: variance must be rank 2 or rank 1 \" +\n            (\"but got rank \" + variance.rank + \".\"));\n        if (scale != null) {\n            util.assert(scale.rank === 2 || scale.rank === 1, \"Error in batchNormalization2D: scale must be rank 2 or rank 1 \" +\n                (\"but got rank \" + scale.rank + \".\"));\n        }\n        if (offset != null) {\n            util.assert(offset.rank === 2 || offset.rank === 1, \"Error in batchNormalization2D: offset must be rank 2 or rank 1 \" +\n                (\"but got rank \" + offset.rank + \".\"));\n        }\n        return BatchNormOps.batchNormalization(x, mean, variance, varianceEpsilon, scale, offset);\n    };\n    BatchNormOps.batchNormalization3d = function (x, mean, variance, varianceEpsilon, scale, offset) {\n        if (varianceEpsilon === void 0) { varianceEpsilon = .001; }\n        util.assert(x.rank === 3, \"Error in batchNormalization3D: x must be rank 3 but got rank \" +\n            (x.rank + \".\"));\n        util.assert(mean.rank === 3 || mean.rank === 1, \"Error in batchNormalization3D: mean must be rank 3 or rank 1 but \" +\n            (\"got rank \" + mean.rank + \".\"));\n        util.assert(variance.rank === 3 || variance.rank === 1, \"Error in batchNormalization3D: variance must be rank 3 or rank 1 \" +\n            (\"but got rank \" + variance.rank + \".\"));\n        if (scale != null) {\n            util.assert(scale.rank === 3 || scale.rank === 1, \"Error in batchNormalization3D: scale must be rank 3 or rank 1 \" +\n                (\"but got rank \" + scale.rank + \".\"));\n        }\n        if (offset != null) {\n            util.assert(offset.rank === 3 || offset.rank === 1, \"Error in batchNormalization3D: offset must be rank 3 or rank 1 \" +\n                (\"but got rank \" + offset.rank + \".\"));\n        }\n        return BatchNormOps.batchNormalization(x, mean, variance, varianceEpsilon, scale, offset);\n    };\n    BatchNormOps.batchNormalization4d = function (x, mean, variance, varianceEpsilon, scale, offset) {\n        if (varianceEpsilon === void 0) { varianceEpsilon = .001; }\n        util.assert(x.rank === 4, \"Error in batchNormalization4D: x must be rank 4 but got rank \" +\n            (x.rank + \".\"));\n        util.assert(mean.rank === 4 || mean.rank === 1, \"Error in batchNormalization4D: mean must be rank 4 or rank 1 but \" +\n            (\"got rank \" + mean.rank + \".\"));\n        util.assert(variance.rank === 4 || variance.rank === 1, \"Error in batchNormalization4D: variance must be rank 4 or rank 1 \" +\n            (\"but got rank \" + variance.rank + \".\"));\n        if (scale != null) {\n            util.assert(scale.rank === 4 || scale.rank === 1, \"Error in batchNormalization4D: scale must be rank 4 or rank 1 \" +\n                (\"but got rank \" + scale.rank + \".\"));\n        }\n        if (offset != null) {\n            util.assert(offset.rank === 4 || offset.rank === 1, \"Error in batchNormalization4D: offset must be rank 4 or rank 1 \" +\n                (\"but got rank \" + offset.rank + \".\"));\n        }\n        return BatchNormOps.batchNormalization(x, mean, variance, varianceEpsilon, scale, offset);\n    };\n    BatchNormOps.batchNormalization = function (x, mean, variance, varianceEpsilon, scale, offset) {\n        if (varianceEpsilon === void 0) { varianceEpsilon = .001; }\n        util.assertArgumentsAreTensors({ x: x, mean: mean, variance: variance }, 'batchNormalization');\n        if (scale != null) {\n            util.assertArgumentsAreTensors({ scale: scale }, 'batchNormalization');\n        }\n        if (offset != null) {\n            util.assertArgumentsAreTensors({ offset: offset }, 'batchNormalization');\n        }\n        util.assert(mean.rank === variance.rank, 'Batch normalization gradient requires mean and variance to have ' +\n            'equal ranks.');\n        util.assert(offset == null || mean.rank === offset.rank, 'Batch normalization gradient requires mean and offset to have ' +\n            'equal ranks.');\n        util.assert(scale == null || mean.rank === scale.rank, 'Batch normalization gradient requires mean and scale to have ' +\n            'equal ranks.');\n        var x4D;\n        if (x.rank === 0 || x.rank === 1) {\n            x4D = x.as4D(1, 1, 1, x.size);\n        }\n        else if (x.rank === 2) {\n            x4D = x.as4D(1, 1, x.shape[0], x.shape[1]);\n        }\n        else if (x.rank === 3) {\n            x4D = x.as4D(1, x.shape[0], x.shape[1], x.shape[2]);\n        }\n        else {\n            x4D = x;\n        }\n        var der = function (dy) {\n            var scaleValue = scale == null ? array_ops_1.ArrayOps.scalar(1) : scale;\n            var reductionAxes = broadcast_util_1.getReductionAxes(mean.shape, x4D.shape);\n            var tileShape = [];\n            if (mean.rank === 1) {\n                for (var i = 0; i < x4D.shape.length - 1; ++i) {\n                    tileShape.push(x4D.shape[i]);\n                }\n                tileShape.push(1);\n            }\n            var xMinusMean = x.sub(mean);\n            var dyTimesScaleValue = dy.mul(scaleValue);\n            var oneOverSqrtVariance = ops_1.rsqrt(variance.add(array_ops_1.ArrayOps.scalar(varianceEpsilon)));\n            var minusHalfRCube = oneOverSqrtVariance.mul(oneOverSqrtVariance)\n                .mul(oneOverSqrtVariance)\n                .mul(array_ops_1.ArrayOps.scalar(-0.5));\n            var derX = function () {\n                if (mean.rank === 1) {\n                    return dy\n                        .mul(array_ops_1.ArrayOps.tile(oneOverSqrtVariance.as4D(1, 1, 1, mean.shape[0]), tileShape))\n                        .mul(scaleValue)\n                        .reshape(x.shape);\n                }\n                else {\n                    return dy.mul(oneOverSqrtVariance).mul(scaleValue).reshape(x.shape);\n                }\n            };\n            var derMean = function () {\n                var meanDer = oneOverSqrtVariance.mul(array_ops_1.ArrayOps.scalar(-1)).mul(dyTimesScaleValue);\n                if (mean.rank === 1) {\n                    meanDer = meanDer.sum(reductionAxes);\n                }\n                return meanDer.reshape(mean.shape);\n            };\n            var derVariance = function () {\n                var varianceDer = minusHalfRCube.mul(xMinusMean).mul(dyTimesScaleValue);\n                if (mean.rank === 1) {\n                    varianceDer = varianceDer.sum(reductionAxes);\n                }\n                return varianceDer.reshape(mean.shape);\n            };\n            var derScale = function () {\n                var xMinusMean2TimesRsqrt = xMinusMean.mul(oneOverSqrtVariance);\n                var scaleDer = dy.mul(xMinusMean2TimesRsqrt);\n                if (mean.rank === 1) {\n                    scaleDer = scaleDer.sum(reductionAxes);\n                }\n                return scaleDer.reshape(mean.shape);\n            };\n            var derOffset = function () {\n                var offsetDer = dy;\n                if (mean.rank === 1) {\n                    offsetDer = offsetDer.sum(reductionAxes);\n                }\n                return offsetDer.reshape(mean.shape);\n            };\n            return {\n                x: derX,\n                mean: derMean,\n                variance: derVariance,\n                scale: derScale,\n                offset: derOffset\n            };\n        };\n        var res = environment_1.ENV.engine.runKernel(function (backend) { return backend.batchNormalization(x4D, batchnormReshape4D(mean), batchnormReshape4D(variance), varianceEpsilon, batchnormReshape4D(scale), batchnormReshape4D(offset)); }, { x: x, mean: mean, variance: variance, scale: scale, offset: offset }, der);\n        return res.reshape(x.shape);\n    };\n    __decorate([\n        operation_1.operation\n    ], BatchNormOps, \"batchNormalization2d\", null);\n    __decorate([\n        operation_1.operation\n    ], BatchNormOps, \"batchNormalization3d\", null);\n    __decorate([\n        operation_1.operation\n    ], BatchNormOps, \"batchNormalization4d\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Normalization' })\n    ], BatchNormOps, \"batchNormalization\", null);\n    return BatchNormOps;\n}());\nexports.BatchNormOps = BatchNormOps;\nfunction batchnormReshape4D(x) {\n    if (x == null) {\n        return null;\n    }\n    if (x.rank === 0) {\n        return x.as1D();\n    }\n    else if (x.rank === 1) {\n        return x;\n    }\n    else if (x.rank === 2) {\n        return x.as4D(1, 1, x.shape[0], x.shape[1]);\n    }\n    else if (x.rank === 3) {\n        return x.as4D(1, x.shape[0], x.shape[1], x.shape[2]);\n    }\n    return x;\n}\n","map":{"version":3,"file":"batchnorm.js","sourceRoot":"","sources":["../src/ops/batchnorm.ts"],"names":[],"mappings":";;;;;;;;AAiBA,8BAA2B;AAC3B,8CAAmC;AAGnC,8BAAgC;AAEhC,yCAAqC;AACrC,mDAAkD;AAClD,yCAAsC;AACtC,6BAA4B;AAE5B;IAAA;IA2QA,CAAC;IA9PQ,iCAAoB,GAA3B,UACI,CAAW,EAAE,IAAuB,EAAE,QAA2B,EACjE,eAAsB,EAAE,KAAyB,EACjD,MAA0B;QAD1B,gCAAA,EAAA,sBAAsB;QAExB,IAAI,CAAC,MAAM,CACP,CAAC,CAAC,IAAI,KAAK,CAAC,EACZ,+DAA+D;aACxD,CAAC,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QACtB,IAAI,CAAC,MAAM,CACP,IAAI,CAAC,IAAI,KAAK,CAAC,IAAI,IAAI,CAAC,IAAI,KAAK,CAAC,EAClC,mEAAmE;aAC/D,cAAY,IAAI,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAClC,IAAI,CAAC,MAAM,CACP,QAAQ,CAAC,IAAI,KAAK,CAAC,IAAI,QAAQ,CAAC,IAAI,KAAK,CAAC,EAC1C,mEAAmE;aAC/D,kBAAgB,QAAQ,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAC1C,EAAE,CAAC,CAAC,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC;YAClB,IAAI,CAAC,MAAM,CACP,KAAK,CAAC,IAAI,KAAK,CAAC,IAAI,KAAK,CAAC,IAAI,KAAK,CAAC,EACpC,gEAAgE;iBAC5D,kBAAgB,KAAK,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QACzC,CAAC;QACD,EAAE,CAAC,CAAC,MAAM,IAAI,IAAI,CAAC,CAAC,CAAC;YACnB,IAAI,CAAC,MAAM,CACP,MAAM,CAAC,IAAI,KAAK,CAAC,IAAI,MAAM,CAAC,IAAI,KAAK,CAAC,EACtC,iEAAiE;iBAC7D,kBAAgB,MAAM,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAC1C,CAAC;QAED,MAAM,CAAC,YAAY,CAAC,kBAAkB,CAClC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,eAAe,EAAE,KAAK,EAAE,MAAM,CAAC,CAAC;IACzD,CAAC;IAcM,iCAAoB,GAA3B,UACI,CAAW,EAAE,IAAuB,EAAE,QAA2B,EACjE,eAAsB,EAAE,KAAyB,EACjD,MAA0B;QAD1B,gCAAA,EAAA,sBAAsB;QAExB,IAAI,CAAC,MAAM,CACP,CAAC,CAAC,IAAI,KAAK,CAAC,EACZ,+DAA+D;aACxD,CAAC,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QACtB,IAAI,CAAC,MAAM,CACP,IAAI,CAAC,IAAI,KAAK,CAAC,IAAI,IAAI,CAAC,IAAI,KAAK,CAAC,EAClC,mEAAmE;aAC/D,cAAY,IAAI,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAClC,IAAI,CAAC,MAAM,CACP,QAAQ,CAAC,IAAI,KAAK,CAAC,IAAI,QAAQ,CAAC,IAAI,KAAK,CAAC,EAC1C,mEAAmE;aAC/D,kBAAgB,QAAQ,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAC1C,EAAE,CAAC,CAAC,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC;YAClB,IAAI,CAAC,MAAM,CACP,KAAK,CAAC,IAAI,KAAK,CAAC,IAAI,KAAK,CAAC,IAAI,KAAK,CAAC,EACpC,gEAAgE;iBAC5D,kBAAgB,KAAK,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QACzC,CAAC;QACD,EAAE,CAAC,CAAC,MAAM,IAAI,IAAI,CAAC,CAAC,CAAC;YACnB,IAAI,CAAC,MAAM,CACP,MAAM,CAAC,IAAI,KAAK,CAAC,IAAI,MAAM,CAAC,IAAI,KAAK,CAAC,EACtC,iEAAiE;iBAC7D,kBAAgB,MAAM,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAC1C,CAAC;QAED,MAAM,CAAC,YAAY,CAAC,kBAAkB,CAClC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,eAAe,EAAE,KAAK,EAAE,MAAM,CAAC,CAAC;IACzD,CAAC;IAcM,iCAAoB,GAA3B,UACI,CAAW,EAAE,IAAuB,EAAE,QAA2B,EACjE,eAAsB,EAAE,KAAyB,EACjD,MAA0B;QAD1B,gCAAA,EAAA,sBAAsB;QAExB,IAAI,CAAC,MAAM,CACP,CAAC,CAAC,IAAI,KAAK,CAAC,EACZ,+DAA+D;aACxD,CAAC,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QACtB,IAAI,CAAC,MAAM,CACP,IAAI,CAAC,IAAI,KAAK,CAAC,IAAI,IAAI,CAAC,IAAI,KAAK,CAAC,EAClC,mEAAmE;aAC/D,cAAY,IAAI,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAClC,IAAI,CAAC,MAAM,CACP,QAAQ,CAAC,IAAI,KAAK,CAAC,IAAI,QAAQ,CAAC,IAAI,KAAK,CAAC,EAC1C,mEAAmE;aAC/D,kBAAgB,QAAQ,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAC1C,EAAE,CAAC,CAAC,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC;YAClB,IAAI,CAAC,MAAM,CACP,KAAK,CAAC,IAAI,KAAK,CAAC,IAAI,KAAK,CAAC,IAAI,KAAK,CAAC,EACpC,gEAAgE;iBAC5D,kBAAgB,KAAK,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QACzC,CAAC;QACD,EAAE,CAAC,CAAC,MAAM,IAAI,IAAI,CAAC,CAAC,CAAC;YACnB,IAAI,CAAC,MAAM,CACP,MAAM,CAAC,IAAI,KAAK,CAAC,IAAI,MAAM,CAAC,IAAI,KAAK,CAAC,EACtC,iEAAiE;iBAC7D,kBAAgB,MAAM,CAAC,IAAI,MAAG,CAAA,CAAC,CAAC;QAC1C,CAAC;QACD,MAAM,CAAC,YAAY,CAAC,kBAAkB,CAClC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,eAAe,EAAE,KAAK,EAAE,MAAM,CAAC,CAAC;IACzD,CAAC;IAsBM,+BAAkB,GAAzB,UACI,CAAY,EAAE,IAAwB,EAAE,QAA4B,EACpE,eAAsB,EAAE,KAA0B,EAClD,MAA2B;QAD3B,gCAAA,EAAA,sBAAsB;QAExB,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAE,IAAI,MAAA,EAAE,QAAQ,UAAA,EAAC,EAAE,oBAAoB,CAAC,CAAC;QAC1E,EAAE,CAAC,CAAC,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC;YAClB,IAAI,CAAC,yBAAyB,CAAC,EAAC,KAAK,OAAA,EAAC,EAAE,oBAAoB,CAAC,CAAC;QAChE,CAAC;QACD,EAAE,CAAC,CAAC,MAAM,IAAI,IAAI,CAAC,CAAC,CAAC;YACnB,IAAI,CAAC,yBAAyB,CAAC,EAAC,MAAM,QAAA,EAAC,EAAE,oBAAoB,CAAC,CAAC;QACjE,CAAC;QAED,IAAI,CAAC,MAAM,CACP,IAAI,CAAC,IAAI,KAAK,QAAQ,CAAC,IAAI,EAC3B,kEAAkE;YAC9D,cAAc,CAAC,CAAC;QACxB,IAAI,CAAC,MAAM,CACP,MAAM,IAAI,IAAI,IAAI,IAAI,CAAC,IAAI,KAAK,MAAM,CAAC,IAAI,EAC3C,gEAAgE;YAC5D,cAAc,CAAC,CAAC;QACxB,IAAI,CAAC,MAAM,CACP,KAAK,IAAI,IAAI,IAAI,IAAI,CAAC,IAAI,KAAK,KAAK,CAAC,IAAI,EACzC,+DAA+D;YAC3D,cAAc,CAAC,CAAC;QAExB,IAAI,GAAa,CAAC;QAClB,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,KAAK,CAAC,IAAI,CAAC,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;YACjC,GAAG,GAAG,CAAC,CAAC,IAAI,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,IAAI,CAAC,CAAC;QAChC,CAAC;QAAC,IAAI,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;YACxB,GAAG,GAAG,CAAC,CAAC,IAAI,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;QAC7C,CAAC;QAAC,IAAI,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;YACxB,GAAG,GAAG,CAAC,CAAC,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAa,CAAC;QAClE,CAAC;QAAC,IAAI,CAAC,CAAC;YACN,GAAG,GAAG,CAAa,CAAC;QACtB,CAAC;QAED,IAAM,GAAG,GAAG,UAAC,EAAU;YACrB,IAAM,UAAU,GAAG,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC,oBAAQ,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC;YAC9D,IAAM,aAAa,GAAG,iCAAgB,CAAC,IAAI,CAAC,KAAK,EAAE,GAAG,CAAC,KAAK,CAAC,CAAC;YAC9D,IAAM,SAAS,GAAa,EAAE,CAAC;YAC/B,EAAE,CAAC,CAAC,IAAI,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;gBACpB,GAAG,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,GAAG,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC;oBAC9C,SAAS,CAAC,IAAI,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;gBAC/B,CAAC;gBACD,SAAS,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;YACpB,CAAC;YAED,IAAM,UAAU,GAAG,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC;YAC/B,IAAM,iBAAiB,GAAG,EAAE,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC;YAC7C,IAAM,mBAAmB,GACrB,WAAK,CAAC,QAAQ,CAAC,GAAG,CAAC,oBAAQ,CAAC,MAAM,CAAC,eAAe,CAAC,CAAC,CAAC,CAAC;YAC1D,IAAM,cAAc,GAAG,mBAAmB,CAAC,GAAG,CAAC,mBAAmB,CAAC;iBACvC,GAAG,CAAC,mBAAmB,CAAC;iBACxB,GAAG,CAAC,oBAAQ,CAAC,MAAM,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;YACvD,IAAM,IAAI,GAAG;gBACX,EAAE,CAAC,CAAC,IAAI,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;oBACpB,MAAM,CAAC,EAAE;yBACJ,GAAG,CAAC,oBAAQ,CAAC,IAAI,CACd,mBAAmB,CAAC,IAAI,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,EAAE,SAAS,CAAC,CAAC;yBAChE,GAAG,CAAC,UAAU,CAAC;yBACf,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;gBACxB,CAAC;gBAAC,IAAI,CAAC,CAAC;oBACN,MAAM,CAAC,EAAE,CAAC,GAAG,CAAC,mBAAmB,CAAC,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;gBACtE,CAAC;YACH,CAAC,CAAC;YACF,IAAM,OAAO,GAAG;gBACd,IAAI,OAAO,GACP,mBAAmB,CAAC,GAAG,CAAC,oBAAQ,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,iBAAiB,CAAC,CAAC;gBACxE,EAAE,CAAC,CAAC,IAAI,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;oBACpB,OAAO,GAAG,OAAO,CAAC,GAAG,CAAC,aAAa,CAAC,CAAC;gBACvC,CAAC;gBACD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;YACrC,CAAC,CAAC;YACF,IAAM,WAAW,GAAG;gBAClB,IAAI,WAAW,GAAG,cAAc,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC,GAAG,CAAC,iBAAiB,CAAC,CAAC;gBACxE,EAAE,CAAC,CAAC,IAAI,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;oBACpB,WAAW,GAAG,WAAW,CAAC,GAAG,CAAC,aAAa,CAAC,CAAC;gBAC/C,CAAC;gBACD,MAAM,CAAC,WAAW,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;YACzC,CAAC,CAAC;YACF,IAAM,QAAQ,GAAG;gBACf,IAAM,qBAAqB,GAAG,UAAU,CAAC,GAAG,CAAC,mBAAmB,CAAC,CAAC;gBAClE,IAAI,QAAQ,GAAG,EAAE,CAAC,GAAG,CAAC,qBAAqB,CAAC,CAAC;gBAC7C,EAAE,CAAC,CAAC,IAAI,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;oBACpB,QAAQ,GAAG,QAAQ,CAAC,GAAG,CAAC,aAAa,CAAC,CAAC;gBACzC,CAAC;gBACD,MAAM,CAAC,QAAQ,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;YACtC,CAAC,CAAC;YACF,IAAM,SAAS,GAAG;gBAChB,IAAI,SAAS,GAAG,EAAE,CAAC;gBACnB,EAAE,CAAC,CAAC,IAAI,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;oBACpB,SAAS,GAAG,SAAS,CAAC,GAAG,CAAC,aAAa,CAAC,CAAC;gBAC3C,CAAC;gBACD,MAAM,CAAC,SAAS,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;YACvC,CAAC,CAAC;YACF,MAAM,CAAC;gBACL,CAAC,EAAE,IAAI;gBACP,IAAI,EAAE,OAAO;gBACb,QAAQ,EAAE,WAAW;gBACrB,KAAK,EAAE,QAAQ;gBACf,MAAM,EAAE,SAAS;aAClB,CAAC;QACJ,CAAC,CAAC;QAEF,IAAM,GAAG,GAAG,iBAAG,CAAC,MAAM,CAAC,SAAS,CAC5B,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,kBAAkB,CACjC,GAAG,EAAE,kBAAkB,CAAC,IAAI,CAAC,EAAE,kBAAkB,CAAC,QAAQ,CAAC,EAC3D,eAAe,EAAE,kBAAkB,CAAC,KAAK,CAAC,EAC1C,kBAAkB,CAAC,MAAM,CAAC,CAAC,EAHpB,CAGoB,EAC/B,EAAC,CAAC,GAAA,EAAE,IAAI,MAAA,EAAE,QAAQ,UAAA,EAAE,KAAK,OAAA,EAAE,MAAM,QAAA,EAAC,EAAE,GAAG,CAAC,CAAC;QAC7C,MAAM,CAAC,GAAG,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;IAC9B,CAAC;IA7PD;QADC,qBAAS;kDAgCT;IAcD;QADC,qBAAS;kDAgCT;IAcD;QADC,qBAAS;kDA+BT;IAsBD;QADC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,eAAe,EAAC,CAAC;gDAgHzD;IACH,mBAAC;CAAA,AA3QD,IA2QC;AA3QY,oCAAY;AA6QzB,4BAA4B,CAAS;IACnC,EAAE,CAAC,CAAC,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC;QACd,MAAM,CAAC,IAAI,CAAC;IACd,CAAC;IACD,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;QACjB,MAAM,CAAC,CAAC,CAAC,IAAI,EAAE,CAAC;IAClB,CAAC;IAAC,IAAI,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;QACxB,MAAM,CAAC,CAAa,CAAC;IACvB,CAAC;IAAC,IAAI,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;QACxB,MAAM,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;IAC9C,CAAC;IAAC,IAAI,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;QACxB,MAAM,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;IACvD,CAAC;IACD,MAAM,CAAC,CAAa,CAAC;AACvB,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {doc} from '../doc';\nimport {ENV} from '../environment';\nimport {Tensor, Tensor1D, Tensor2D, Tensor3D, Tensor4D} from '../tensor';\nimport {Rank} from '../types';\nimport * as util from '../util';\n\nimport {ArrayOps} from './array_ops';\nimport {getReductionAxes} from './broadcast_util';\nimport {operation} from './operation';\nimport {rsqrt} from './ops';\n\nexport class BatchNormOps {\n  /**\n   * Batch normalization, strictly for 2D. For the more relaxed version, see\n   * `batchNormalization`.\n   *\n   * @param x The input Tensor.\n   * @param mean A mean Tensor.\n   * @param variance A variance Tensor.\n   * @param varianceEpsilon A small float number to avoid dividing by 0.\n   * @param scale A scale Tensor.\n   * @param offset An offset Tensor.\n   */\n  @operation\n  static batchNormalization2d(\n      x: Tensor2D, mean: Tensor2D|Tensor1D, variance: Tensor2D|Tensor1D,\n      varianceEpsilon = .001, scale?: Tensor2D|Tensor1D,\n      offset?: Tensor2D|Tensor1D): Tensor2D {\n    util.assert(\n        x.rank === 2,\n        `Error in batchNormalization3D: x must be rank 3 but got rank ` +\n            `${x.rank}.`);\n    util.assert(\n        mean.rank === 2 || mean.rank === 1,\n        `Error in batchNormalization2D: mean must be rank 2 or rank 1 but ` +\n            `got rank ${mean.rank}.`);\n    util.assert(\n        variance.rank === 2 || variance.rank === 1,\n        `Error in batchNormalization2D: variance must be rank 2 or rank 1 ` +\n            `but got rank ${variance.rank}.`);\n    if (scale != null) {\n      util.assert(\n          scale.rank === 2 || scale.rank === 1,\n          `Error in batchNormalization2D: scale must be rank 2 or rank 1 ` +\n              `but got rank ${scale.rank}.`);\n    }\n    if (offset != null) {\n      util.assert(\n          offset.rank === 2 || offset.rank === 1,\n          `Error in batchNormalization2D: offset must be rank 2 or rank 1 ` +\n              `but got rank ${offset.rank}.`);\n    }\n\n    return BatchNormOps.batchNormalization(\n        x, mean, variance, varianceEpsilon, scale, offset);\n  }\n\n  /**\n   * Batch normalization, strictly for 3D. For the more relaxed version, see\n   * `batchNormalization`.\n   *\n   * @param x The input Tensor.\n   * @param mean A mean Tensor.\n   * @param variance A variance Tensor.\n   * @param varianceEpsilon A small float number to avoid dividing by 0.\n   * @param scale A scale Tensor.\n   * @param offset An offset Tensor.\n   */\n  @operation\n  static batchNormalization3d(\n      x: Tensor3D, mean: Tensor3D|Tensor1D, variance: Tensor3D|Tensor1D,\n      varianceEpsilon = .001, scale?: Tensor3D|Tensor1D,\n      offset?: Tensor3D|Tensor1D): Tensor3D {\n    util.assert(\n        x.rank === 3,\n        `Error in batchNormalization3D: x must be rank 3 but got rank ` +\n            `${x.rank}.`);\n    util.assert(\n        mean.rank === 3 || mean.rank === 1,\n        `Error in batchNormalization3D: mean must be rank 3 or rank 1 but ` +\n            `got rank ${mean.rank}.`);\n    util.assert(\n        variance.rank === 3 || variance.rank === 1,\n        `Error in batchNormalization3D: variance must be rank 3 or rank 1 ` +\n            `but got rank ${variance.rank}.`);\n    if (scale != null) {\n      util.assert(\n          scale.rank === 3 || scale.rank === 1,\n          `Error in batchNormalization3D: scale must be rank 3 or rank 1 ` +\n              `but got rank ${scale.rank}.`);\n    }\n    if (offset != null) {\n      util.assert(\n          offset.rank === 3 || offset.rank === 1,\n          `Error in batchNormalization3D: offset must be rank 3 or rank 1 ` +\n              `but got rank ${offset.rank}.`);\n    }\n\n    return BatchNormOps.batchNormalization(\n        x, mean, variance, varianceEpsilon, scale, offset);\n  }\n\n  /**\n   * Batch normalization, strictly for 4D. For the more relaxed version, see\n   * `batchNormalization`.\n   *\n   * @param x The input Tensor.\n   * @param mean A mean Tensor.\n   * @param variance A variance Tensor.\n   * @param varianceEpsilon A small float number to avoid dividing by 0.\n   * @param scale A scale Tensor.\n   * @param offset An offset Tensor.\n   */\n  @operation\n  static batchNormalization4d(\n      x: Tensor4D, mean: Tensor4D|Tensor1D, variance: Tensor4D|Tensor1D,\n      varianceEpsilon = .001, scale?: Tensor4D|Tensor1D,\n      offset?: Tensor4D|Tensor1D): Tensor4D {\n    util.assert(\n        x.rank === 4,\n        `Error in batchNormalization4D: x must be rank 4 but got rank ` +\n            `${x.rank}.`);\n    util.assert(\n        mean.rank === 4 || mean.rank === 1,\n        `Error in batchNormalization4D: mean must be rank 4 or rank 1 but ` +\n            `got rank ${mean.rank}.`);\n    util.assert(\n        variance.rank === 4 || variance.rank === 1,\n        `Error in batchNormalization4D: variance must be rank 4 or rank 1 ` +\n            `but got rank ${variance.rank}.`);\n    if (scale != null) {\n      util.assert(\n          scale.rank === 4 || scale.rank === 1,\n          `Error in batchNormalization4D: scale must be rank 4 or rank 1 ` +\n              `but got rank ${scale.rank}.`);\n    }\n    if (offset != null) {\n      util.assert(\n          offset.rank === 4 || offset.rank === 1,\n          `Error in batchNormalization4D: offset must be rank 4 or rank 1 ` +\n              `but got rank ${offset.rank}.`);\n    }\n    return BatchNormOps.batchNormalization(\n        x, mean, variance, varianceEpsilon, scale, offset);\n  }\n\n  /**\n   * Batch normalization.\n   *\n   * As described in\n   * [http://arxiv.org/abs/1502.03167](http://arxiv.org/abs/1502.03167).\n   *\n   * Mean, variance, scale, and offset can be of two\n   * shapes:\n   *   - The same shape as the input.\n   *   - In the common case, the depth dimension is the last dimension of x, so\n   *     the values would be an `Tensor1D` of shape [depth].\n   *\n   * @param x The input Tensor.\n   * @param mean A mean Tensor.\n   * @param variance A variance Tensor.\n   * @param varianceEpsilon A small float number to avoid dividing by 0.\n   * @param scale A scale Tensor.\n   * @param offset An offset Tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Normalization'})\n  static batchNormalization<R extends Rank>(\n      x: Tensor<R>, mean: Tensor<R>|Tensor1D, variance: Tensor<R>|Tensor1D,\n      varianceEpsilon = .001, scale?: Tensor<R>|Tensor1D,\n      offset?: Tensor<R>|Tensor1D): Tensor<R> {\n    util.assertArgumentsAreTensors({x, mean, variance}, 'batchNormalization');\n    if (scale != null) {\n      util.assertArgumentsAreTensors({scale}, 'batchNormalization');\n    }\n    if (offset != null) {\n      util.assertArgumentsAreTensors({offset}, 'batchNormalization');\n    }\n\n    util.assert(\n        mean.rank === variance.rank,\n        'Batch normalization gradient requires mean and variance to have ' +\n            'equal ranks.');\n    util.assert(\n        offset == null || mean.rank === offset.rank,\n        'Batch normalization gradient requires mean and offset to have ' +\n            'equal ranks.');\n    util.assert(\n        scale == null || mean.rank === scale.rank,\n        'Batch normalization gradient requires mean and scale to have ' +\n            'equal ranks.');\n\n    let x4D: Tensor4D;\n    if (x.rank === 0 || x.rank === 1) {\n      x4D = x.as4D(1, 1, 1, x.size);\n    } else if (x.rank === 2) {\n      x4D = x.as4D(1, 1, x.shape[0], x.shape[1]);\n    } else if (x.rank === 3) {\n      x4D = x.as4D(1, x.shape[0], x.shape[1], x.shape[2]) as Tensor4D;\n    } else {\n      x4D = x as Tensor4D;\n    }\n\n    const der = (dy: Tensor) => {\n      const scaleValue = scale == null ? ArrayOps.scalar(1) : scale;\n      const reductionAxes = getReductionAxes(mean.shape, x4D.shape);\n      const tileShape: number[] = [];\n      if (mean.rank === 1) {\n        for (let i = 0; i < x4D.shape.length - 1; ++i) {\n          tileShape.push(x4D.shape[i]);\n        }\n        tileShape.push(1);\n      }\n\n      const xMinusMean = x.sub(mean);\n      const dyTimesScaleValue = dy.mul(scaleValue);\n      const oneOverSqrtVariance =\n          rsqrt(variance.add(ArrayOps.scalar(varianceEpsilon)));\n      const minusHalfRCube = oneOverSqrtVariance.mul(oneOverSqrtVariance)\n                                 .mul(oneOverSqrtVariance)\n                                 .mul(ArrayOps.scalar(-0.5));\n      const derX = () => {\n        if (mean.rank === 1) {\n          return dy\n              .mul(ArrayOps.tile(\n                  oneOverSqrtVariance.as4D(1, 1, 1, mean.shape[0]), tileShape))\n              .mul(scaleValue)\n              .reshape(x.shape);\n        } else {\n          return dy.mul(oneOverSqrtVariance).mul(scaleValue).reshape(x.shape);\n        }\n      };\n      const derMean = () => {\n        let meanDer =\n            oneOverSqrtVariance.mul(ArrayOps.scalar(-1)).mul(dyTimesScaleValue);\n        if (mean.rank === 1) {\n          meanDer = meanDer.sum(reductionAxes);\n        }\n        return meanDer.reshape(mean.shape);\n      };\n      const derVariance = () => {\n        let varianceDer = minusHalfRCube.mul(xMinusMean).mul(dyTimesScaleValue);\n        if (mean.rank === 1) {\n          varianceDer = varianceDer.sum(reductionAxes);\n        }\n        return varianceDer.reshape(mean.shape);\n      };\n      const derScale = () => {\n        const xMinusMean2TimesRsqrt = xMinusMean.mul(oneOverSqrtVariance);\n        let scaleDer = dy.mul(xMinusMean2TimesRsqrt);\n        if (mean.rank === 1) {\n          scaleDer = scaleDer.sum(reductionAxes);\n        }\n        return scaleDer.reshape(mean.shape);\n      };\n      const derOffset = () => {\n        let offsetDer = dy;\n        if (mean.rank === 1) {\n          offsetDer = offsetDer.sum(reductionAxes);\n        }\n        return offsetDer.reshape(mean.shape);\n      };\n      return {\n        x: derX,\n        mean: derMean,\n        variance: derVariance,\n        scale: derScale,\n        offset: derOffset\n      };\n    };\n\n    const res = ENV.engine.runKernel(\n        backend => backend.batchNormalization(\n            x4D, batchnormReshape4D(mean), batchnormReshape4D(variance),\n            varianceEpsilon, batchnormReshape4D(scale),\n            batchnormReshape4D(offset)),\n        {x, mean, variance, scale, offset}, der);\n    return res.reshape(x.shape);\n  }\n}\n\nfunction batchnormReshape4D(x: Tensor): Tensor4D|Tensor1D {\n  if (x == null) {\n    return null;\n  }\n  if (x.rank === 0) {\n    return x.as1D();\n  } else if (x.rank === 1) {\n    return x as Tensor1D;\n  } else if (x.rank === 2) {\n    return x.as4D(1, 1, x.shape[0], x.shape[1]);\n  } else if (x.rank === 3) {\n    return x.as4D(1, x.shape[0], x.shape[1], x.shape[2]);\n  }\n  return x as Tensor4D;\n}\n"]}},"hash":"bb6e3c1a1286e92f0d8a79a62eaebb4f","cacheData":{"env":{}}}