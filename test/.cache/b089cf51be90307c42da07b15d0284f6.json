{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/package.json","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1524062920943},{"name":"../doc","loc":{"line":9,"column":20}},{"name":"../environment","loc":{"line":10,"column":28}},{"name":"../util","loc":{"line":11,"column":19}},{"name":"./operation","loc":{"line":12,"column":26}},{"name":"./ops","loc":{"line":14,"column":20}},{"name":"./selu_util","loc":{"line":15,"column":24}}],"generated":{"js":"\"use strict\";\nvar __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar doc_1 = require(\"../doc\");\nvar environment_1 = require(\"../environment\");\nvar util = require(\"../util\");\nvar operation_1 = require(\"./operation\");\nvar ops = require(\"./ops\");\nvar ops_1 = require(\"./ops\");\nvar selu_util = require(\"./selu_util\");\nvar UnaryOps = (function () {\n    function UnaryOps() {\n    }\n    UnaryOps.neg = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'neg');\n        var grad = function (dy) {\n            return { x: function () { return dy.neg(); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.neg(x); }, { x: x }, grad);\n    };\n    UnaryOps.ceil = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'ceil');\n        var grad = function (dy) {\n            return { x: function () { return ops.zerosLike(dy); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.ceil(x); }, { x: x }, grad);\n    };\n    UnaryOps.floor = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'floor');\n        var grad = function (dy) {\n            return { x: function () { return ops.zerosLike(dy); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.floor(x); }, { x: x }, grad);\n    };\n    UnaryOps.sign = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'sign');\n        var grad = function (dy) {\n            return { x: function () { return ops.zerosLike(dy); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.sign(x); }, { x: x }, grad);\n    };\n    UnaryOps.round = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'round');\n        var grad = function (dy) {\n            return { x: function () { return ops.zerosLike(dy); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.round(x); }, { x: x }, grad);\n    };\n    UnaryOps.exp = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'exp');\n        var bck = function (dy, saved) {\n            var y = saved[0];\n            return { x: function () { return dy.mulStrict(y); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend, save) { return save(backend.exp(x)); }, { x: x }, bck);\n    };\n    UnaryOps.expm1 = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'expm1');\n        var grad = function (dy) {\n            return { x: function () { return dy.mulStrict(x.exp()); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.expm1(x); }, { x: x }, grad);\n    };\n    UnaryOps.log = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'log');\n        var grad = function (dy) {\n            return { x: function () { return dy.divStrict(x.toFloat()); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.log(x); }, { x: x }, grad);\n    };\n    UnaryOps.log1p = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'log1p');\n        var grad = function (dy) {\n            return { x: function () { return dy.divStrict(x.add(ops.scalar(1))); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.log1p(x); }, { x: x }, grad);\n    };\n    UnaryOps.sqrt = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'sqrt');\n        var grad = function (dy) {\n            return { x: function () { return dy.divStrict(x.toFloat().sqrt().mul(ops.scalar(2))); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.sqrt(x); }, { x: x }, grad);\n    };\n    UnaryOps.rsqrt = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'rsqrt');\n        var grad = function (dy) {\n            return {\n                x: function () { return dy.divStrict(x.pow(ops.scalar(1.5)).mul(ops.scalar(2))).neg(); }\n            };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.rsqrt(x); }, { x: x }, grad);\n    };\n    UnaryOps.square = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'square');\n        var grad = function (dy) {\n            return { x: function () { return dy.mulStrict(x.toFloat().mul(ops.scalar(2))); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.square(x); }, { x: x }, grad);\n    };\n    UnaryOps.reciprocal = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'reciprocal');\n        var grad = function (dy) {\n            return { x: function () { return dy.divStrict(x.square().neg()); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.reciprocal(x); }, { x: x }, grad);\n    };\n    UnaryOps.abs = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'abs');\n        var grad = function (dy) {\n            return { x: function () { return dy.mulStrict(x.toFloat().step(-1)); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.abs(x); }, { x: x }, grad);\n    };\n    UnaryOps.clipByValue = function (x, clipValueMin, clipValueMax) {\n        util.assertArgumentsAreTensors({ x: x }, 'clipByValue');\n        util.assert((clipValueMin <= clipValueMax), \"Error in clip: min (\" + clipValueMin + \") must be \" +\n            (\"less than or equal to max (\" + clipValueMax + \").\"));\n        var grad = function (dy) {\n            return {\n                x: function () { return dy.where(x.greater(ops.scalar(clipValueMin))\n                    .logicalAnd(x.less(ops.scalar(clipValueMax))), ops_1.zerosLike(dy)); },\n            };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.clip(x, clipValueMin, clipValueMax); }, { x: x }, grad);\n    };\n    UnaryOps.relu = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'relu');\n        if (x.dtype === 'bool') {\n            return x.toInt();\n        }\n        var grad = function (dy) {\n            var stepRes = x.step();\n            return { x: function () { return dy.mulStrict(stepRes.toFloat()); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.relu(x); }, { x: x }, grad);\n    };\n    UnaryOps.elu = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'elu');\n        var grad = function (dy, saved) {\n            var y = saved[0];\n            return {\n                x: function () {\n                    return environment_1.ENV.engine.runKernel(function (backend) { return backend.eluDer(dy, y); }, { dy: dy, y: y });\n                }\n            };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend, save) { return save(backend.elu(x)); }, { x: x }, grad);\n    };\n    UnaryOps.selu = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'selu');\n        var grad = function (dy) {\n            return {\n                x: function () {\n                    var mask = x.greater(ops.scalar(0));\n                    var scaleAlpha = ops.scalar(selu_util.SELU_SCALEALPHA);\n                    var scale = ops.scalar(selu_util.SELU_SCALE);\n                    var greaterThanZeroDer = dy.mul(scale);\n                    var lessEqualZeroDer = dy.mul(scaleAlpha).mul(x.toFloat().exp());\n                    return ops.where(mask, greaterThanZeroDer, lessEqualZeroDer);\n                }\n            };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.selu(x); }, { x: x }, grad);\n    };\n    UnaryOps.leakyRelu = function (x, alpha) {\n        if (alpha === void 0) { alpha = 0.2; }\n        util.assertArgumentsAreTensors({ x: x }, 'leakyRelu');\n        return ops.maximum(ops.scalar(alpha).mul(x), x);\n    };\n    UnaryOps.prelu = function (x, alpha) {\n        util.assertArgumentsAreTensors({ x: x, alpha: alpha }, 'prelu');\n        var zero = ops.scalar(0);\n        return ops.maximum(zero, x).add(alpha.mul(ops.minimum(zero, x)));\n    };\n    UnaryOps.sigmoid = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'sigmoid');\n        var grad = function (dy, saved) {\n            var y = saved[0];\n            return { x: function () { return dy.mulStrict(y.mul(ops.scalar(1).sub(y))); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend, save) { return save(backend.sigmoid(x)); }, { x: x }, grad);\n    };\n    UnaryOps.logSigmoid = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'logSigmoid');\n        var grad = function (dy) {\n            return { x: function () { return dy.mulStrict(x.neg().sigmoid()); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.softplus(x.neg()).neg(); }, { x: x }, grad);\n    };\n    UnaryOps.softplus = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'softplus');\n        var grad = function (dy) {\n            return { x: function () { return dy.mulStrict(x.sigmoid()); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.softplus(x); }, { x: x }, grad);\n    };\n    UnaryOps.sin = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'sin');\n        var grad = function (dy) {\n            return { x: function () { return x.toFloat().cos().mulStrict(dy); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.sin(x); }, { x: x }, grad);\n    };\n    UnaryOps.cos = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'cos');\n        var grad = function (dy) {\n            return { x: function () { return x.toFloat().sin().neg().mulStrict(dy); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.cos(x); }, { x: x }, grad);\n    };\n    UnaryOps.tan = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'tan');\n        var grad = function (dy) {\n            return { x: function () { return dy.divStrict(x.cos().square()); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.tan(x); }, { x: x }, grad);\n    };\n    UnaryOps.asin = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'asin');\n        var grad = function (dy) {\n            return {\n                x: function () {\n                    return dy.divStrict(UnaryOps.sqrt(ops.scalar(1).sub(x.toFloat().square())));\n                }\n            };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.asin(x); }, { x: x }, grad);\n    };\n    UnaryOps.acos = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'acos');\n        var grad = function (dy) {\n            return {\n                x: function () {\n                    return dy.divStrict(UnaryOps.sqrt(ops.scalar(1).sub(x.toFloat().square())))\n                        .neg();\n                }\n            };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.acos(x); }, { x: x }, grad);\n    };\n    UnaryOps.atan = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'atan');\n        var grad = function (dy) {\n            return { x: function () { return dy.divStrict(ops.scalar(1).add(x.toFloat().square())); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.atan(x); }, { x: x }, grad);\n    };\n    UnaryOps.sinh = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'sinh');\n        var grad = function (dy) {\n            return { x: function () { return x.toFloat().cosh().mulStrict(dy); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.sinh(x); }, { x: x }, grad);\n    };\n    UnaryOps.cosh = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'cosh');\n        var grad = function (dy) {\n            return { x: function () { return x.toFloat().sinh().mulStrict(dy); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.cosh(x); }, { x: x }, grad);\n    };\n    UnaryOps.tanh = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'tanh');\n        var grad = function (dy, saved) {\n            var y = saved[0];\n            return { x: function () { return ops.scalar(1).sub(y.square()).mulStrict(dy); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend, save) { return save(backend.tanh(x)); }, { x: x }, grad);\n    };\n    UnaryOps.asinh = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'asinh');\n        var grad = function (dy) {\n            return {\n                x: function () {\n                    return dy.divStrict(UnaryOps.sqrt(ops.scalar(1).add(x.toFloat().square())));\n                }\n            };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.asinh(x); }, { x: x }, grad);\n    };\n    UnaryOps.acosh = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'acosh');\n        var grad = function (dy) {\n            return {\n                x: function () {\n                    return dy.divStrict(UnaryOps.sqrt(x.toFloat().square().sub(ops.scalar(1))));\n                }\n            };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.acosh(x); }, { x: x }, grad);\n    };\n    UnaryOps.atanh = function (x) {\n        util.assertArgumentsAreTensors({ x: x }, 'atanh');\n        var grad = function (dy) {\n            return { x: function () { return dy.divStrict(ops.scalar(1).sub(x.toFloat().square())); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.atanh(x); }, { x: x }, grad);\n    };\n    UnaryOps.erf = function (x) {\n        util.assert(x.dtype === 'int32' || x.dtype === 'float32', 'Input dtype must be `int32` or `float32`.');\n        if (x.dtype === 'int32') {\n            x = x.toFloat();\n        }\n        var grad = function (dy) {\n            return {\n                x: function () {\n                    return dy.mulStrict(ops.scalar(2 / Math.sqrt(Math.PI))\n                        .mul(x.square().neg().exp()));\n                }\n            };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.erf(x); }, { x: x }, grad);\n    };\n    UnaryOps.step = function (x, alpha) {\n        if (alpha === void 0) { alpha = 0.0; }\n        util.assertArgumentsAreTensors({ x: x }, 'step');\n        var grad = function (dy) {\n            return { x: function () { return ops.zerosLike(dy); } };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.step(x, alpha); }, { x: x }, grad);\n    };\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"neg\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"ceil\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"floor\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"sign\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"round\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"exp\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"expm1\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"log\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"log1p\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"sqrt\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"rsqrt\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"square\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"reciprocal\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"abs\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"clipByValue\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"relu\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"elu\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"selu\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"leakyRelu\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"prelu\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"sigmoid\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"logSigmoid\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"softplus\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"sin\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"cos\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"tan\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"asin\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"acos\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"atan\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"sinh\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"cosh\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"tanh\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"asinh\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"acosh\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"atanh\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"erf\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], UnaryOps, \"step\", null);\n    return UnaryOps;\n}());\nexports.UnaryOps = UnaryOps;\n","map":{"version":3,"file":"unary_ops.js","sourceRoot":"","sources":["../src/ops/unary_ops.ts"],"names":[],"mappings":";;;;;;;;AAiBA,8BAA2B;AAC3B,8CAAmC;AAEnC,8BAAgC;AAChC,yCAAsC;AACtC,2BAA6B;AAC7B,6BAAgC;AAChC,uCAAyC;AAEzC;IAAA;IAo2BA,CAAC;IAt1BQ,YAAG,GAAV,UAA6B,CAAI;QAC/B,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAE3C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,GAAG,EAAE,EAAR,CAAQ,EAAC,CAAC;QAC7B,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,EAAd,CAAc,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACpE,CAAC;IAcM,aAAI,GAAX,UAA8B,CAAI;QAChC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,MAAM,CAAC,CAAC;QAG5C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,GAAG,CAAC,SAAS,CAAC,EAAE,CAAC,EAAjB,CAAiB,EAAC,CAAC;QACtC,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,EAAf,CAAe,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACrE,CAAC;IAcM,cAAK,GAAZ,UAA+B,CAAI;QACjC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,OAAO,CAAC,CAAC;QAI7C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,GAAG,CAAC,SAAS,CAAC,EAAE,CAAC,EAAjB,CAAiB,EAAC,CAAC;QACtC,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,EAAhB,CAAgB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACtE,CAAC;IAcM,aAAI,GAAX,UAA8B,CAAI;QAChC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,MAAM,CAAC,CAAC;QAE5C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,GAAG,CAAC,SAAS,CAAC,EAAE,CAAC,EAAjB,CAAiB,EAAC,CAAC;QACtC,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,EAAf,CAAe,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACrE,CAAC;IAeM,cAAK,GAAZ,UAA+B,CAAI;QACjC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,OAAO,CAAC,CAAC;QAI7C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,GAAG,CAAC,SAAS,CAAC,EAAE,CAAC,EAAjB,CAAiB,EAAC,CAAC;QACtC,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,EAAhB,CAAgB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACtE,CAAC;IAcM,YAAG,GAAV,UAA6B,CAAI;QAC/B,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAE3C,IAAM,GAAG,GAAG,UAAC,EAAK,EAAE,KAAe;YAC1B,IAAA,YAAC,CAAU;YAClB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,CAAM,CAAC,EAApB,CAAoB,EAAC,CAAC;QACzC,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CACvB,UAAC,OAAO,EAAE,IAAI,IAAK,OAAA,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,EAApB,CAAoB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,GAAG,CAAC,CAAC;IACzD,CAAC;IAeM,cAAK,GAAZ,UAA+B,CAAI;QACjC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,OAAO,CAAC,CAAC;QAE7C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,CAAC,CAAC,GAAG,EAAE,CAAC,EAArB,CAAqB,EAAC,CAAC;QAC1C,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,EAAhB,CAAgB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACtE,CAAC;IAcM,YAAG,GAAV,UAA6B,CAAI;QAC/B,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAE3C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,EAAzB,CAAyB,EAAC,CAAC;QAC9C,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,EAAd,CAAc,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACpE,CAAC;IAeM,cAAK,GAAZ,UAA+B,CAAI;QACjC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,OAAO,CAAC,CAAC;QAE7C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,CAAC,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,EAAlC,CAAkC,EAAC,CAAC;QACvD,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,EAAhB,CAAgB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACtE,CAAC;IAcM,aAAI,GAAX,UAA8B,CAAI;QAChC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,MAAM,CAAC,CAAC;QAE5C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,IAAI,EAAE,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,EAAnD,CAAmD,EAAC,CAAC;QACxE,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,EAAf,CAAe,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACrE,CAAC;IAeM,cAAK,GAAZ,UAA+B,CAAI;QACjC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,OAAO,CAAC,CAAC;QAE7C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC;gBACL,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,CAAC,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,EAAE,EAA7D,CAA6D;aACvE,CAAC;QACJ,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,EAAhB,CAAgB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACtE,CAAC;IAcM,eAAM,GAAb,UAAgC,CAAI;QAClC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,QAAQ,CAAC,CAAC;QAE9C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,EAA5C,CAA4C,EAAC,CAAC;QACjE,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,EAAjB,CAAiB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACvE,CAAC;IAcM,mBAAU,GAAjB,UAAoC,CAAI;QACtC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,YAAY,CAAC,CAAC;QAElD,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC,EAA9B,CAA8B,EAAC,CAAC;QACnD,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,EAArB,CAAqB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IAC3E,CAAC;IAcM,YAAG,GAAV,UAA6B,CAAI;QAC/B,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAE3C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,EAAlC,CAAkC,EAAC,CAAC;QACvD,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,EAAd,CAAc,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACpE,CAAC;IAgBM,oBAAW,GAAlB,UACI,CAAI,EAAE,YAAoB,EAAE,YAAoB;QAClD,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,aAAa,CAAC,CAAC;QACnD,IAAI,CAAC,MAAM,CACP,CAAC,YAAY,IAAI,YAAY,CAAC,EAC9B,yBAAuB,YAAY,eAAY;aAC3C,gCAA8B,YAAY,OAAI,CAAA,CAAC,CAAC;QAExD,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC;gBAGL,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,KAAK,CACJ,CAAC,CAAC,OAAO,CAAC,GAAG,CAAC,MAAM,CAAC,YAAY,CAAC,CAAC;qBAC9B,UAAU,CAAC,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,MAAM,CAAC,YAAY,CAAC,CAAC,CAAC,EACjD,eAAS,CAAC,EAAE,CAAC,CAAM,EAHvB,CAGuB;aACjC,CAAC;QACJ,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CACvB,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,IAAI,CAAC,CAAC,EAAE,YAAY,EAAE,YAAY,CAAC,EAA3C,CAA2C,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACzE,CAAC;IAeM,aAAI,GAAX,UAA8B,CAAI;QAChC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,MAAM,CAAC,CAAC;QAE5C,EAAE,CAAC,CAAC,CAAC,CAAC,KAAK,KAAK,MAAM,CAAC,CAAC,CAAC;YACvB,MAAM,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC;QACnB,CAAC;QACD,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,IAAM,OAAO,GAAG,CAAC,CAAC,IAAI,EAAE,CAAC;YACzB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,OAAO,CAAC,OAAO,EAAE,CAAC,EAA/B,CAA+B,EAAC,CAAC;QACpD,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,EAAf,CAAe,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACrE,CAAC;IAcM,YAAG,GAAV,UAA6B,CAAI;QAC/B,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAE3C,IAAM,IAAI,GAAG,UAAC,EAAK,EAAE,KAAe;YAC3B,IAAA,YAAC,CAAU;YAClB,MAAM,CAAC;gBACL,CAAC,EAAE;oBACC,OAAA,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,MAAM,CAAC,EAAE,EAAE,CAAC,CAAC,EAArB,CAAqB,EAAE,EAAC,EAAE,IAAA,EAAE,CAAC,GAAA,EAAC,CAAM;gBAApE,CAAoE;aACzE,CAAC;QACJ,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CACvB,UAAC,OAAO,EAAE,IAAI,IAAK,OAAA,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,EAApB,CAAoB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IAC1D,CAAC;IAgBM,aAAI,GAAX,UAA8B,CAAI;QAChC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,MAAM,CAAC,CAAC;QAE5C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC;gBACL,CAAC,EAAE;oBACD,IAAM,IAAI,GAAG,CAAC,CAAC,OAAO,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC;oBAEtC,IAAM,UAAU,GAAG,GAAG,CAAC,MAAM,CAAC,SAAS,CAAC,eAAe,CAAC,CAAC;oBACzD,IAAM,KAAK,GAAG,GAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAU,CAAC,CAAC;oBAE/C,IAAM,kBAAkB,GAAG,EAAE,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC;oBACzC,IAAM,gBAAgB,GAAG,EAAE,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,GAAG,EAAE,CAAC,CAAC;oBAEnE,MAAM,CAAC,GAAG,CAAC,KAAK,CAAC,IAAI,EAAE,kBAAkB,EAAE,gBAAgB,CAAM,CAAC;gBACpE,CAAC;aACF,CAAC;QACJ,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,EAAf,CAAe,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACrE,CAAC;IAmBM,kBAAS,GAAhB,UAAmC,CAAI,EAAE,KAAW;QAAX,sBAAA,EAAA,WAAW;QAClD,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,WAAW,CAAC,CAAC;QAEjD,MAAM,CAAC,GAAG,CAAC,OAAO,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;IAClD,CAAC;IAkBM,cAAK,GAAZ,UAA+B,CAAI,EAAE,KAAQ;QAC3C,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAE,KAAK,OAAA,EAAC,EAAE,OAAO,CAAC,CAAC;QAEpD,IAAM,IAAI,GAAG,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC;QAC3B,MAAM,CAAC,GAAG,CAAC,OAAO,CAAC,IAAI,EAAE,CAAC,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,OAAO,CAAC,IAAI,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IACnE,CAAC;IAcM,gBAAO,GAAd,UAAiC,CAAI;QACnC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,SAAS,CAAC,CAAC;QAE/C,IAAM,IAAI,GAAG,UAAC,EAAK,EAAE,KAAe;YAC3B,IAAA,YAAC,CAAU;YAClB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,CAAC,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,EAAzC,CAAyC,EAAC,CAAC;QAC9D,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CACvB,UAAC,OAAO,EAAE,IAAI,IAAK,OAAA,IAAI,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,EAAxB,CAAwB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IAC9D,CAAC;IAeM,mBAAU,GAAjB,UAAoC,CAAI;QACtC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,YAAY,CAAC,CAAC;QAElD,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,CAAC,CAAC,GAAG,EAAE,CAAC,OAAO,EAAE,CAAC,EAA/B,CAA+B,EAAC,CAAC;QACpD,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CACvB,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,GAAG,EAAE,CAAC,CAAC,GAAG,EAAE,EAA/B,CAA+B,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IAC7D,CAAC;IAcM,iBAAQ,GAAf,UAAkC,CAAI;QACpC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,UAAU,CAAC,CAAC;QAEhD,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,EAAzB,CAAyB,EAAC,CAAC;QAC9C,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAnB,CAAmB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACzE,CAAC;IAcM,YAAG,GAAV,UAA6B,CAAI;QAC/B,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAE3C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,CAAC,CAAC,OAAO,EAAE,CAAC,GAAG,EAAE,CAAC,SAAS,CAAC,EAAE,CAAC,EAA/B,CAA+B,EAAC,CAAC;QACpD,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,EAAd,CAAc,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACpE,CAAC;IAcM,YAAG,GAAV,UAA6B,CAAI;QAC/B,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAE3C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,CAAC,CAAC,OAAO,EAAE,CAAC,GAAG,EAAE,CAAC,GAAG,EAAE,CAAC,SAAS,CAAC,EAAE,CAAC,EAArC,CAAqC,EAAC,CAAC;QAC1D,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,EAAd,CAAc,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACpE,CAAC;IAcM,YAAG,GAAV,UAA6B,CAAI;QAC/B,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAE3C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,CAAC,CAAC,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,EAA9B,CAA8B,EAAC,CAAC;QACnD,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,EAAd,CAAc,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACpE,CAAC;IAcM,aAAI,GAAX,UAA8B,CAAI;QAChC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,MAAM,CAAC,CAAC;QAE5C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC;gBACL,CAAC,EAAE;oBACC,OAAA,EAAE,CAAC,SAAS,CAAC,QAAQ,CAAC,IAAI,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,MAAM,EAAE,CAAC,CAAC,CAAC;gBAApE,CAAoE;aACzE,CAAC;QACJ,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,EAAf,CAAe,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACrE,CAAC;IAcM,aAAI,GAAX,UAA8B,CAAI;QAChC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,MAAM,CAAC,CAAC;QAE5C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC;gBACL,CAAC,EAAE;oBACC,OAAA,EAAE,CAAC,SAAS,CAAC,QAAQ,CAAC,IAAI,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,MAAM,EAAE,CAAC,CAAC,CAAC;yBAC/D,GAAG,EAAE;gBADV,CACU;aACf,CAAC;QACJ,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,EAAf,CAAe,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACrE,CAAC;IAcM,aAAI,GAAX,UAA8B,CAAI;QAChC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,MAAM,CAAC,CAAC;QAE5C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,MAAM,EAAE,CAAC,CAAC,EAArD,CAAqD,EAAC,CAAC;QAC1E,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,EAAf,CAAe,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACrE,CAAC;IAcM,aAAI,GAAX,UAA8B,CAAI;QAChC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,MAAM,CAAC,CAAC;QAE5C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,CAAC,CAAC,OAAO,EAAE,CAAC,IAAI,EAAE,CAAC,SAAS,CAAC,EAAE,CAAC,EAAhC,CAAgC,EAAC,CAAC;QACrD,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,EAAf,CAAe,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACrE,CAAC;IAcM,aAAI,GAAX,UAA8B,CAAI;QAChC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,MAAM,CAAC,CAAC;QAE5C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,CAAC,CAAC,OAAO,EAAE,CAAC,IAAI,EAAE,CAAC,SAAS,CAAC,EAAE,CAAC,EAAhC,CAAgC,EAAC,CAAC;QACrD,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,EAAf,CAAe,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACrE,CAAC;IAcM,aAAI,GAAX,UAA8B,CAAI;QAChC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,MAAM,CAAC,CAAC;QAE5C,IAAM,IAAI,GAAG,UAAC,EAAK,EAAE,KAAe;YAC3B,IAAA,YAAC,CAAU;YAClB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,CAAC,SAAS,CAAC,EAAE,CAAM,EAAhD,CAAgD,EAAC,CAAC;QACrE,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CACvB,UAAC,OAAO,EAAE,IAAI,IAAK,OAAA,IAAI,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,EAArB,CAAqB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IAC3D,CAAC;IAeM,cAAK,GAAZ,UAA+B,CAAI;QACjC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,OAAO,CAAC,CAAC;QAE7C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC;gBACL,CAAC,EAAE;oBACC,OAAA,EAAE,CAAC,SAAS,CAAC,QAAQ,CAAC,IAAI,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,MAAM,EAAE,CAAC,CAAC,CAAC;gBAApE,CAAoE;aACzE,CAAC;QACJ,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,EAAhB,CAAgB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACtE,CAAC;IAeM,cAAK,GAAZ,UAA+B,CAAI;QACjC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,OAAO,CAAC,CAAC;QAE7C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC;gBACL,CAAC,EAAE;oBACC,OAAA,EAAE,CAAC,SAAS,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,MAAM,EAAE,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;gBAApE,CAAoE;aACzE,CAAC;QACJ,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,EAAhB,CAAgB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACtE,CAAC;IAeM,cAAK,GAAZ,UAA+B,CAAI;QACjC,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,OAAO,CAAC,CAAC;QAE7C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,EAAE,CAAC,SAAS,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,MAAM,EAAE,CAAC,CAAC,EAArD,CAAqD,EAAC,CAAC;QAC1E,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,EAAhB,CAAgB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACtE,CAAC;IAeM,YAAG,GAAV,UAA6B,CAAI;QAC/B,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,KAAK,KAAK,OAAO,IAAI,CAAC,CAAC,KAAK,KAAK,SAAS,EACpD,2CAA2C,CAAC,CAAC;QAEjD,EAAE,CAAC,CAAC,CAAC,CAAC,KAAK,KAAK,OAAO,CAAC,CAAC,CAAC;YACxB,CAAC,GAAG,CAAC,CAAC,OAAO,EAAE,CAAC;QAClB,CAAC;QAED,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC;gBACL,CAAC,EAAE;oBACC,OAAA,EAAE,CAAC,SAAS,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,GAAC,IAAI,CAAC,IAAI,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;yBACxC,GAAG,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC,GAAG,EAAE,CAAC,CAAC;gBADjC,CACiC;aACtC,CAAC;QACJ,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,EAAd,CAAc,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IACpE,CAAC;IAeM,aAAI,GAAX,UAA8B,CAAI,EAAE,KAAW;QAAX,sBAAA,EAAA,WAAW;QAC7C,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAC,EAAE,MAAM,CAAC,CAAC;QAG5C,IAAM,IAAI,GAAG,UAAC,EAAK;YACjB,MAAM,CAAC,EAAC,CAAC,EAAE,cAAM,OAAA,GAAG,CAAC,SAAS,CAAC,EAAE,CAAC,EAAjB,CAAiB,EAAC,CAAC;QACtC,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,IAAI,CAAC,CAAC,EAAE,KAAK,CAAC,EAAtB,CAAsB,EAAE,EAAC,CAAC,GAAA,EAAC,EAAE,IAAI,CAAC,CAAC;IAC5E,CAAC;IAr1BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;6BAQT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BAST;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;+BAUT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BAQT;IAeD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;+BAUT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;6BAUT;IAeD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;+BAQT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;6BAQT;IAeD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;+BAQT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BAQT;IAeD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;+BAUT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;gCAQT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;oCAQT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;6BAQT;IAgBD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;qCAqBT;IAeD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BAYT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;6BAaT;IAgBD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BAoBT;IAmBD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;mCAKT;IAkBD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;+BAMT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;iCAUT;IAeD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;oCAST;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;kCAQT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;6BAQT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;6BAQT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;6BAQT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BAWT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BAYT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BAQT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BAQT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BAQT;IAcD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BAUT;IAeD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;+BAWT;IAeD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;+BAWT;IAeD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;+BAQT;IAeD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;6BAiBT;IAeD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BAST;IACH,eAAC;CAAA,AAp2BD,IAo2BC;AAp2BY,4BAAQ","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {doc} from '../doc';\nimport {ENV} from '../environment';\nimport {Tensor} from '../tensor';\nimport * as util from '../util';\nimport {operation} from './operation';\nimport * as ops from './ops';\nimport {zerosLike} from './ops';\nimport * as selu_util from './selu_util';\n\nexport class UnaryOps {\n  /**\n   * Computes `-1 * x` element-wise.\n   *\n   * ```js\n   * const x = tf.tensor2d([1, 2, -2, 0], [2, 2]);\n   *\n   * x.neg().print();  // or tf.neg(x)\n   * ```\n   *\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static neg<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'neg');\n\n    const grad = (dy: T) => {\n      return {x: () => dy.neg()};\n    };\n    return ENV.engine.runKernel(backend => backend.neg(x), {x}, grad);\n  }\n\n  /**\n   * Computes ceiling of input `Tensor` element-wise: `ceil(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([.6, 1.1, -3.3]);\n   *\n   * x.ceil().print();  // or tf.ceil(x)\n   * ```\n   * @param x The input Tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static ceil<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'ceil');\n\n    // TODO(manrajgrover): Return null for gradients when backprop supports it.\n    const grad = (dy: T) => {\n      return {x: () => ops.zerosLike(dy)};\n    };\n    return ENV.engine.runKernel(backend => backend.ceil(x), {x}, grad);\n  }\n\n  /**\n   * Computes floor of input `Tensor` element-wise: `floor(x)`.\n   *\n   * ```js\n   * const x = tf.tensor1d([.6, 1.1, -3.3]);\n   *\n   * x.floor().print();  // or tf.floor(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static floor<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'floor');\n\n    // TODO(nsthorat): Let gradients be null for cases where we want to stop\n    // backpropgation.\n    const grad = (dy: T) => {\n      return {x: () => ops.zerosLike(dy)};\n    };\n    return ENV.engine.runKernel(backend => backend.floor(x), {x}, grad);\n  }\n\n  /**\n   * Returns an element-wise indication of the sign of a number.\n   *\n   * ```js\n   * const x = tf.tensor1d([.6, 1.1, -3.3, NaN, 0]);\n   *\n   * x.sign().print();  // or tf.sign(x)\n   * ```\n   * @param x The input Tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static sign<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'sign');\n\n    const grad = (dy: T) => {\n      return {x: () => ops.zerosLike(dy)};\n    };\n    return ENV.engine.runKernel(backend => backend.sign(x), {x}, grad);\n  }\n\n  /**\n   * Computes round of input `Tensor` element-wise: `round(x)`.\n   * It implements banker's rounding.\n   *\n   * ```js\n   * const x = tf.tensor1d([.6, 1.1, -3.3]);\n   *\n   * x.round().print();  // or tf.round(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static round<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'round');\n\n    // TODO(nsthorat): Let gradients be null for cases where we want to stop\n    // backpropgation.\n    const grad = (dy: T) => {\n      return {x: () => ops.zerosLike(dy)};\n    };\n    return ENV.engine.runKernel(backend => backend.round(x), {x}, grad);\n  }\n\n  /**\n   * Computes exponential of the input `Tensor` element-wise. `e ^ x`\n   *\n   * ```js\n   * const x = tf.tensor1d([1, 2, -3]);\n   *\n   * x.exp().print();  // or tf.exp(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static exp<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'exp');\n\n    const bck = (dy: T, saved: Tensor[]) => {\n      const [y] = saved;\n      return {x: () => dy.mulStrict(y as T)};\n    };\n    return ENV.engine.runKernel(\n        (backend, save) => save(backend.exp(x)), {x}, bck);\n  }\n\n  /**\n   * Computes exponential of the input `Tensor` minus one element-wise.\n   * `e ^ x - 1`\n   *\n   * ```js\n   * const x = tf.tensor1d([1, 2, -3]);\n   *\n   * x.expm1().print();  // or tf.expm1(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static expm1<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'expm1');\n\n    const grad = (dy: T) => {\n      return {x: () => dy.mulStrict(x.exp())};\n    };\n    return ENV.engine.runKernel(backend => backend.expm1(x), {x}, grad);\n  }\n\n  /**\n   * Computes natural logarithm of the input `Tensor` element-wise: `ln(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([1, 2, Math.E]);\n   *\n   * x.log().print();  // or tf.log(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static log<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'log');\n\n    const grad = (dy: T) => {\n      return {x: () => dy.divStrict(x.toFloat())};\n    };\n    return ENV.engine.runKernel(backend => backend.log(x), {x}, grad);\n  }\n\n  /**\n   * Computes natural logarithm of the input `Tensor` plus one\n   * element-wise: `ln(1 + x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([1, 2, Math.E - 1]);\n   *\n   * x.log1p().print();  // or tf.log1p(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static log1p<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'log1p');\n\n    const grad = (dy: T) => {\n      return {x: () => dy.divStrict(x.add(ops.scalar(1)))};\n    };\n    return ENV.engine.runKernel(backend => backend.log1p(x), {x}, grad);\n  }\n\n  /**\n   * Computes square root of the input `Tensor` element-wise: `y = sqrt(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([1, 2, 4, -1]);\n   *\n   * x.sqrt().print();  // or tf.sqrt(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static sqrt<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'sqrt');\n\n    const grad = (dy: T) => {\n      return {x: () => dy.divStrict(x.toFloat().sqrt().mul(ops.scalar(2)))};\n    };\n    return ENV.engine.runKernel(backend => backend.sqrt(x), {x}, grad);\n  }\n\n  /**\n   * Computes reciprocal of square root of the input `Tensor` element-wise:\n   * `y = 1 / sqrt(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([1, 2, 4, -1]);\n   *\n   * x.rsqrt().print();  // or tf.rsqrt(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static rsqrt<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'rsqrt');\n\n    const grad = (dy: T) => {\n      return {\n        x: () => dy.divStrict(x.pow(ops.scalar(1.5)).mul(ops.scalar(2))).neg()\n      };\n    };\n    return ENV.engine.runKernel(backend => backend.rsqrt(x), {x}, grad);\n  }\n\n  /**\n   * Computes square of `x` element-wise: `x ^ 2`\n   *\n   * ```js\n   * const x = tf.tensor1d([1, 2, Math.sqrt(2), -1]);\n   *\n   * x.square().print();  // or tf.square(x)\n   * ```\n   * @param x The input Tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static square<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'square');\n\n    const grad = (dy: T) => {\n      return {x: () => dy.mulStrict(x.toFloat().mul(ops.scalar(2)))};\n    };\n    return ENV.engine.runKernel(backend => backend.square(x), {x}, grad);\n  }\n\n  /**\n   * Computes reciprocal of x element-wise: `1 / x`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, 1, 2]);\n   *\n   * x.reciprocal().print();  // or tf.reciprocal(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static reciprocal<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'reciprocal');\n\n    const grad = (dy: T) => {\n      return {x: () => dy.divStrict(x.square().neg())};\n    };\n    return ENV.engine.runKernel(backend => backend.reciprocal(x), {x}, grad);\n  }\n\n  /**\n   * Computes absolute value element-wise: `abs(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([-1, 2, -3, 4]);\n   *\n   * x.abs().print();  // or tf.abs(x)\n   * ```\n   * @param x The input `Tensor`.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static abs<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'abs');\n\n    const grad = (dy: T) => {\n      return {x: () => dy.mulStrict(x.toFloat().step(-1))};\n    };\n    return ENV.engine.runKernel(backend => backend.abs(x), {x}, grad);\n  }\n\n  /**\n   * Clips values element-wise. `max(min(x, clipValueMax), clipValueMin)`\n   *\n   * ```js\n   * const x = tf.tensor1d([-1, 2, -3, 4]);\n   *\n   * x.clipByValue(-2, 3).print();  // or tf.clipByValue(x, -2, 3)\n   * ```\n   * @param x The input tensor.\n   * @param clipValueMin Lower-bound of range to be clipped to.\n   * @param clipValueMax Upper-bound of range to be clipped to.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static clipByValue<T extends Tensor>(\n      x: T, clipValueMin: number, clipValueMax: number): T {\n    util.assertArgumentsAreTensors({x}, 'clipByValue');\n    util.assert(\n        (clipValueMin <= clipValueMax),\n        `Error in clip: min (${clipValueMin}) must be ` +\n            `less than or equal to max (${clipValueMax}).`);\n\n    const grad = (dy: T) => {\n      return {\n        // TODO(cais): Fix gradients for the case where x = min or x\n        // = max.\n        x: () => dy.where(\n                     x.greater(ops.scalar(clipValueMin))\n                         .logicalAnd(x.less(ops.scalar(clipValueMax))),\n                     zerosLike(dy)) as T,\n      };\n    };\n    return ENV.engine.runKernel(\n        backend => backend.clip(x, clipValueMin, clipValueMax), {x}, grad);\n  }\n\n  /**\n   * Computes rectified linear element-wise: `max(x, 0)`\n   *\n   * ```js\n   * const x = tf.tensor1d([-1, 2, -3, 4]);\n   *\n   * x.relu().print();  // or tf.relu(x)\n   * ```\n   * @param x The input tensor. If the dtype is `bool`, the output dtype will be\n   *     `int32'.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static relu<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'relu');\n\n    if (x.dtype === 'bool') {\n      return x.toInt();\n    }\n    const grad = (dy: T) => {\n      const stepRes = x.step();\n      return {x: () => dy.mulStrict(stepRes.toFloat())};\n    };\n    return ENV.engine.runKernel(backend => backend.relu(x), {x}, grad);\n  }\n\n  /**\n   * Computes exponential linear element-wise, `x > 0 ? e ^ x - 1 : 0`\n   *\n   * ```js\n   * const x = tf.tensor1d([-1, 1, -3, 2]);\n   *\n   * x.elu().print();  // or tf.elu(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static elu<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'elu');\n\n    const grad = (dy: T, saved: Tensor[]) => {\n      const [y] = saved;\n      return {\n        x: () =>\n            ENV.engine.runKernel(backend => backend.eluDer(dy, y), {dy, y}) as T\n      };\n    };\n    return ENV.engine.runKernel(\n        (backend, save) => save(backend.elu(x)), {x}, grad);\n  }\n\n  /**\n   * Computes scaled exponential linear element-wise.\n   *\n   * `x < 0 ? scale * alpha * (exp(x) - 1) : x`\n   *\n   * ```js\n   * const x = tf.tensor1d([-1, 2, -3, 4]);\n   *\n   * x.selu().print();  // or tf.selu(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static selu<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'selu');\n\n    const grad = (dy: T) => {\n      return {\n        x: () => {\n          const mask = x.greater(ops.scalar(0));\n\n          const scaleAlpha = ops.scalar(selu_util.SELU_SCALEALPHA);\n          const scale = ops.scalar(selu_util.SELU_SCALE);\n\n          const greaterThanZeroDer = dy.mul(scale);\n          const lessEqualZeroDer = dy.mul(scaleAlpha).mul(x.toFloat().exp());\n\n          return ops.where(mask, greaterThanZeroDer, lessEqualZeroDer) as T;\n        }\n      };\n    };\n    return ENV.engine.runKernel(backend => backend.selu(x), {x}, grad);\n  }\n\n  /**\n   * Computes leaky rectified linear element-wise.\n   *\n   * See\n   * [http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf](\n   *     http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)\n   *\n   * ```js\n   * const x = tf.tensor1d([-1, 2, -3, 4]);\n   *\n   * x.leakyRelu(0.1).print();  // or tf.leakyRelu(x, 0.1)\n   * ```\n   * @param x The input tensor.\n   * @param alpha The scaling factor for negative values, defaults to 0.2.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static leakyRelu<T extends Tensor>(x: T, alpha = 0.2): T {\n    util.assertArgumentsAreTensors({x}, 'leakyRelu');\n\n    return ops.maximum(ops.scalar(alpha).mul(x), x);\n  }\n\n  /**\n   * Computes leaky rectified linear element-wise with parametric alphas.\n   *\n   * `x < 0 ? alpha * x : f(x) = x`\n   *\n   * ```js\n   * const x = tf.tensor1d([-1, 2, -3, 4]);\n   * const alpha = tf.scalar(0.1);\n   *\n   * x.prelu(alpha).print();  // or tf.prelu(x, alpha)\n   * ```\n   * @param x The input tensor.\n   * @param alpha Scaling factor for negative values.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static prelu<T extends Tensor>(x: T, alpha: T): T {\n    util.assertArgumentsAreTensors({x, alpha}, 'prelu');\n\n    const zero = ops.scalar(0);\n    return ops.maximum(zero, x).add(alpha.mul(ops.minimum(zero, x)));\n  }\n\n  /**\n   * Computes sigmoid element-wise, `1 / (1 + exp(-x))`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, -1, 2, -3]);\n   *\n   * x.sigmoid().print();  // or tf.sigmoid(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static sigmoid<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'sigmoid');\n\n    const grad = (dy: T, saved: Tensor[]) => {\n      const [y] = saved;\n      return {x: () => dy.mulStrict(y.mul(ops.scalar(1).sub(y)))};\n    };\n    return ENV.engine.runKernel(\n        (backend, save) => save(backend.sigmoid(x)), {x}, grad);\n  }\n\n  /**\n   * Computes log sigmoid of the input `Tensor` element-wise:\n   * `logSigmoid(x)`. For numerical stability, we use `-tf.softplus(-x)`.\n   *\n   * ```js\n   * const x = tf.tensor1d([0, 1, -1, .7]);\n   *\n   * x.logSigmoid().print();  // or tf.logSigmoid(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static logSigmoid<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'logSigmoid');\n\n    const grad = (dy: T) => {\n      return {x: () => dy.mulStrict(x.neg().sigmoid())};\n    };\n    return ENV.engine.runKernel(\n        backend => backend.softplus(x.neg()).neg(), {x}, grad);\n  }\n\n  /**\n   * Computes softplus of the input `Tensor` element-wise: `log(exp(x) + 1)`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, 1, -1, .7]);\n   *\n   * x.softplus().print();  // or tf.softplus(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static softplus<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'softplus');\n\n    const grad = (dy: T) => {\n      return {x: () => dy.mulStrict(x.sigmoid())};\n    };\n    return ENV.engine.runKernel(backend => backend.softplus(x), {x}, grad);\n  }\n\n  /**\n   * Computes sin of the input Tensor element-wise: `sin(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, Math.PI / 2, Math.PI * 3 / 4]);\n   *\n   * x.sin().print();  // or tf.sin(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static sin<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'sin');\n\n    const grad = (dy: T) => {\n      return {x: () => x.toFloat().cos().mulStrict(dy)};\n    };\n    return ENV.engine.runKernel(backend => backend.sin(x), {x}, grad);\n  }\n\n  /**\n   * Computes cos of the input `Tensor` element-wise: `cos(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, Math.PI / 2, Math.PI * 3 / 4]);\n   *\n   * x.cos().print();  // or tf.cos(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static cos<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'cos');\n\n    const grad = (dy: T) => {\n      return {x: () => x.toFloat().sin().neg().mulStrict(dy)};\n    };\n    return ENV.engine.runKernel(backend => backend.cos(x), {x}, grad);\n  }\n\n  /**\n   * Computes tan of the input `Tensor` element-wise, `tan(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, Math.PI / 2, Math.PI * 3 / 4]);\n   *\n   * x.tan().print();  // or tf.tan(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static tan<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'tan');\n\n    const grad = (dy: T) => {\n      return {x: () => dy.divStrict(x.cos().square())};\n    };\n    return ENV.engine.runKernel(backend => backend.tan(x), {x}, grad);\n  }\n\n  /**\n   * Computes asin of the input `Tensor` element-wise: `asin(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, 1, -1, .7]);\n   *\n   * x.asin().print();  // or tf.asin(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static asin<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'asin');\n\n    const grad = (dy: T) => {\n      return {\n        x: () =>\n            dy.divStrict(UnaryOps.sqrt(ops.scalar(1).sub(x.toFloat().square())))\n      };\n    };\n    return ENV.engine.runKernel(backend => backend.asin(x), {x}, grad);\n  }\n\n  /**\n   * Computes acos of the input `Tensor` element-wise: `acos(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, 1, -1, .7]);\n   *\n   * x.acos().print();  // or tf.acos(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static acos<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'acos');\n\n    const grad = (dy: T) => {\n      return {\n        x: () =>\n            dy.divStrict(UnaryOps.sqrt(ops.scalar(1).sub(x.toFloat().square())))\n                .neg()\n      };\n    };\n    return ENV.engine.runKernel(backend => backend.acos(x), {x}, grad);\n  }\n\n  /**\n   * Computes atan of the input `Tensor` element-wise: `atan(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, 1, -1, .7]);\n   *\n   * x.atan().print();  // or tf.atan(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static atan<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'atan');\n\n    const grad = (dy: T) => {\n      return {x: () => dy.divStrict(ops.scalar(1).add(x.toFloat().square()))};\n    };\n    return ENV.engine.runKernel(backend => backend.atan(x), {x}, grad);\n  }\n\n  /**\n   * Computes hyperbolic sin of the input `Tensor` element-wise: `sinh(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, 1, -1, .7]);\n   *\n   * x.sinh().print();  // or tf.sinh(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static sinh<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'sinh');\n\n    const grad = (dy: T) => {\n      return {x: () => x.toFloat().cosh().mulStrict(dy)};\n    };\n    return ENV.engine.runKernel(backend => backend.sinh(x), {x}, grad);\n  }\n\n  /**\n   * Computes hyperbolic cos of the input `Tensor` element-wise: `cosh(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, 1, -1, .7]);\n   *\n   * x.cosh().print();  // or tf.cosh(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static cosh<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'cosh');\n\n    const grad = (dy: T) => {\n      return {x: () => x.toFloat().sinh().mulStrict(dy)};\n    };\n    return ENV.engine.runKernel(backend => backend.cosh(x), {x}, grad);\n  }\n\n  /**\n   * Computes hyperbolic tangent of the input `Tensor` element-wise: `tanh(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, 1, -1, 70]);\n   *\n   * x.tanh().print();  // or tf.tanh(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static tanh<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'tanh');\n\n    const grad = (dy: T, saved: Tensor[]) => {\n      const [y] = saved;\n      return {x: () => ops.scalar(1).sub(y.square()).mulStrict(dy) as T};\n    };\n    return ENV.engine.runKernel(\n        (backend, save) => save(backend.tanh(x)), {x}, grad);\n  }\n\n  /**\n   * Computes inverse hyperbolic sin of the input `Tensor` element-wise:\n   * `asinh(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, 1, -1, .7]);\n   *\n   * x.asinh().print();  // or tf.asinh(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static asinh<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'asinh');\n\n    const grad = (dy: T) => {\n      return {\n        x: () =>\n            dy.divStrict(UnaryOps.sqrt(ops.scalar(1).add(x.toFloat().square())))\n      };\n    };\n    return ENV.engine.runKernel(backend => backend.asinh(x), {x}, grad);\n  }\n\n  /**\n   * Computes the inverse hyperbolic cos of the input `Tensor` element-wise:\n   * `acosh(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([10, 1, 3, 5.7]);\n   *\n   * x.acosh().print();  // or tf.acosh(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static acosh<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'acosh');\n\n    const grad = (dy: T) => {\n      return {\n        x: () =>\n            dy.divStrict(UnaryOps.sqrt(x.toFloat().square().sub(ops.scalar(1))))\n      };\n    };\n    return ENV.engine.runKernel(backend => backend.acosh(x), {x}, grad);\n  }\n\n  /**\n   * Computes inverse hyperbolic tan of the input `Tensor` element-wise:\n   * `atanh(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, .1, -.1, .7]);\n   *\n   * x.atanh().print();  // or tf.atanh(x)\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static atanh<T extends Tensor>(x: T): T {\n    util.assertArgumentsAreTensors({x}, 'atanh');\n\n    const grad = (dy: T) => {\n      return {x: () => dy.divStrict(ops.scalar(1).sub(x.toFloat().square()))};\n    };\n    return ENV.engine.runKernel(backend => backend.atanh(x), {x}, grad);\n  }\n\n  /**\n   * Computes gause error function of the input `Tensor` element-wise:\n   * `erf(x)`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, .1, -.1, .7]);\n   *\n   * x.erf().print(); // or tf.erf(x);\n   * ```\n   * @param x The input tensor.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static erf<T extends Tensor>(x: T): T {\n    util.assert(x.dtype === 'int32' || x.dtype === 'float32',\n        'Input dtype must be `int32` or `float32`.');\n\n    if (x.dtype === 'int32') {\n      x = x.toFloat();\n    }\n\n    const grad = (dy: T) => {\n      return {\n        x: () =>\n            dy.mulStrict(ops.scalar(2/Math.sqrt(Math.PI))\n                .mul(x.square().neg().exp()))\n      };\n    };\n    return ENV.engine.runKernel(backend => backend.erf(x), {x}, grad);\n  }\n\n  /**\n   * Computes step of the input `Tensor` element-wise: `x > 0 ? 1 : alpha * x`\n   *\n   * ```js\n   * const x = tf.tensor1d([0, 2, -1, -3]);\n   *\n   * x.step(.5).print();  // or tf.step(x, .5)\n   * ```\n   * @param x The input tensor.\n   * @param alpha The gradient when input is negative.\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static step<T extends Tensor>(x: T, alpha = 0.0): T {\n    util.assertArgumentsAreTensors({x}, 'step');\n\n    // TODO(manrajgrover): Return null for gradients when backprop supports it.\n    const grad = (dy: T) => {\n      return {x: () => ops.zerosLike(dy)};\n    };\n    return ENV.engine.runKernel(backend => backend.step(x, alpha), {x}, grad);\n  }\n}\n"]}},"hash":"7183eabbdd11c187e6e2dd1298c1e159","cacheData":{"env":{}}}