{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/package.json","includedInParent":true,"mtime":1528810356568},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1528810356568},{"name":"./environment","loc":{"line":38,"column":28}},{"name":"./globals","loc":{"line":39,"column":24}},{"name":"./ops/ops","loc":{"line":40,"column":18}},{"name":"./profiler","loc":{"line":41,"column":25}},{"name":"./tape","loc":{"line":42,"column":21}},{"name":"./tensor","loc":{"line":43,"column":23}},{"name":"./util","loc":{"line":44,"column":19}}],"generated":{"js":"\"use strict\";\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = y[op[0] & 2 ? \"return\" : op[0] ? \"throw\" : \"next\"]) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [0, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar environment_1 = require(\"./environment\");\nvar globals_1 = require(\"./globals\");\nvar ops = require(\"./ops/ops\");\nvar profiler_1 = require(\"./profiler\");\nvar tape_1 = require(\"./tape\");\nvar tensor_1 = require(\"./tensor\");\nvar util = require(\"./util\");\nvar Engine = (function () {\n    function Engine(backend, safeMode) {\n        this.backend = backend;\n        this.safeMode = safeMode;\n        this.registeredVariables = {};\n        this.refCounter = new WeakMap();\n        this.nextTapeNodeId = 0;\n        this.numBytes = 0;\n        this.numTensors = 0;\n        this.numDataBuffers = 0;\n        this.gradientScopeCount = 0;\n        this.customGradientDepth = 0;\n        this.keepTensors = new Set();\n        this.activeScope = { track: [] };\n        this.scopeStack = [this.activeScope];\n        this.profiler = new profiler_1.Profiler(backend);\n    }\n    Engine.prototype.runKernel = function (forwardFunc, inputs, backwardsFunc) {\n        var _this = this;\n        var result;\n        var saved = [];\n        var saveFunc = function (x) {\n            saved.push(x);\n            return x;\n        };\n        var scopeName = this.activeScope.name;\n        this.customGradientDepth++;\n        if (!environment_1.ENV.get('DEBUG')) {\n            result = forwardFunc(this.backend, saveFunc);\n        }\n        else {\n            result = this.profiler.profileKernel(scopeName, function () { return forwardFunc(_this.backend, saveFunc); });\n        }\n        this.customGradientDepth--;\n        if (this.shouldRecord()) {\n            var tapeNode = {\n                id: this.nextTapeNodeId++,\n                name: scopeName,\n                inputs: inputs,\n                output: result,\n            };\n            if (backwardsFunc != null) {\n                tapeNode.gradient = function (dy) { return backwardsFunc(dy, saved); };\n            }\n            this.activeTape.push(tapeNode);\n        }\n        return result;\n    };\n    Engine.prototype.registerTensor = function (a) {\n        var refCount = this.refCounter.has(a.dataId) ? this.refCounter.get(a.dataId) : 0;\n        this.numTensors++;\n        if (refCount === 0) {\n            this.numDataBuffers++;\n            this.numBytes +=\n                util.sizeFromShape(a.shape) * util.bytesPerElement(a.dtype);\n            this.backend.register(a.dataId, a.shape, a.dtype);\n        }\n        this.refCounter.set(a.dataId, refCount + 1);\n        if (!(a instanceof tensor_1.Variable)) {\n            this.track(a);\n        }\n    };\n    Engine.prototype.registerVariable = function (v) {\n        if (this.registeredVariables[v.name] != null) {\n            throw new Error(\"Variable with name \" + v.name + \" was already registered\");\n        }\n        this.registeredVariables[v.name] = v;\n    };\n    Engine.prototype.disposeTensor = function (a) {\n        if (!this.refCounter.has(a.dataId)) {\n            return;\n        }\n        this.numTensors--;\n        var refCount = this.refCounter.get(a.dataId);\n        if (refCount <= 1) {\n            this.refCounter.delete(a.dataId);\n            this.backend.disposeData(a.dataId);\n            this.numDataBuffers--;\n            this.numBytes -=\n                util.sizeFromShape(a.shape) * util.bytesPerElement(a.dtype);\n        }\n        else {\n            this.refCounter.set(a.dataId, refCount - 1);\n        }\n    };\n    Engine.prototype.disposeVariables = function () {\n        for (var varName in this.registeredVariables) {\n            var v = this.registeredVariables[varName];\n            this.disposeTensor(v);\n            delete this.registeredVariables[varName];\n        }\n    };\n    Engine.prototype.memory = function () {\n        var info = this.backend.memory();\n        info.numTensors = this.numTensors;\n        info.numDataBuffers = this.numDataBuffers;\n        info.numBytes = this.numBytes;\n        return info;\n    };\n    Engine.prototype.shouldRecord = function () {\n        return this.activeTape != null && this.customGradientDepth === 0;\n    };\n    Engine.prototype.addTapeNode = function (inputs, result, gradientsFunc) {\n        var inputsMap = {};\n        inputs.forEach(function (input, idx) {\n            inputsMap[idx] = input;\n        });\n        var gradient = function (dy) {\n            var res = gradientsFunc(dy);\n            var resMap = {};\n            res.forEach(function (r, idx) {\n                resMap[idx] = function () { return r; };\n            });\n            return resMap;\n        };\n        var tapeNode = {\n            id: this.nextTapeNodeId++,\n            name: this.activeScope.name,\n            inputs: inputsMap,\n            output: result,\n            gradient: gradient\n        };\n        this.activeTape.push(tapeNode);\n    };\n    Engine.prototype.keep = function (result) {\n        if (this.scopeStack.length === 1 && environment_1.ENV.engine.safeMode) {\n            throw new Error('Safe mode is ON. Enclose all tensor operations inside tf.tidy(): ' +\n                'tf.tidy(() => {...}) to avoid memory leaks.');\n        }\n        this.keepTensors.add(result.id);\n        return result;\n    };\n    Engine.prototype.startScope = function (name, gradientsMode) {\n        if (gradientsMode === void 0) { gradientsMode = false; }\n        if (gradientsMode && this.gradientScopeCount === 0) {\n            this.activeTape = [];\n        }\n        if (gradientsMode) {\n            this.gradientScopeCount++;\n        }\n        var scopeInfo = { track: [] };\n        if (name) {\n            scopeInfo.name = name;\n        }\n        this.scopeStack.push(scopeInfo);\n        this.activeScope = scopeInfo;\n    };\n    Engine.prototype.endScope = function (result, gradientsMode) {\n        var _this = this;\n        if (gradientsMode === void 0) { gradientsMode = false; }\n        if (gradientsMode) {\n            this.gradientScopeCount--;\n            if (this.gradientScopeCount === 0) {\n                this.activeTape = null;\n            }\n        }\n        var tensorsToKeep = new Set(this.keepTensors);\n        var tensorsToTrackInParent = util.getTensorsInContainer(result);\n        tensorsToTrackInParent.forEach(function (tensor) { return tensorsToKeep.add(tensor.id); });\n        for (var i = 0; i < this.activeScope.track.length; i++) {\n            var tensor = this.activeScope.track[i];\n            if (tensorsToKeep.has(tensor.id)) {\n                continue;\n            }\n            if (this.activeTape != null) {\n                tensorsToTrackInParent.push(tensor);\n            }\n            else {\n                tensor.dispose();\n            }\n        }\n        var oldScope = this.scopeStack.pop();\n        this.activeScope = this.scopeStack.length === 0 ?\n            { track: [] } :\n            this.scopeStack[this.scopeStack.length - 1];\n        tensorsToTrackInParent.forEach(function (tensor) {\n            if (!_this.keepTensors.has(tensor.id) &&\n                util.isTensorInList(tensor, oldScope.track)) {\n                _this.track(tensor);\n            }\n        });\n    };\n    Engine.prototype.gradients = function (f, xs, dy, allowNoGradients) {\n        var _this = this;\n        if (allowNoGradients === void 0) { allowNoGradients = false; }\n        util.assert(xs.length > 0, 'gradients() received an empty list of xs.');\n        return globals_1.tidy('gradients', function () {\n            var y = f();\n            util.assert(y instanceof tensor_1.Tensor, 'The result y returned by f() must be a tensor.');\n            var filteredTape = tape_1.getFilteredNodesXToY(_this.activeTape, xs, y);\n            if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) {\n                throw new Error('Cannot compute gradient of y=f(x) with respect to x. Make sure ' +\n                    'that the f you passed encloses all operations that lead from x ' +\n                    'to y.');\n            }\n            var accumulatedGradientMap = {};\n            accumulatedGradientMap[y.id] = (dy == null) ? ops.ones(y.shape) : dy;\n            tape_1.backpropagateGradients(accumulatedGradientMap, filteredTape);\n            var grads = xs.map(function (x) { return accumulatedGradientMap[x.id]; });\n            return { value: y, grads: grads };\n        }, true);\n    };\n    Engine.prototype.customGrad = function (f) {\n        var _this = this;\n        util.assert(util.isFunction(f), 'The f passed in customGrad(f) must be a function.');\n        return function () {\n            var inputs = [];\n            for (var _i = 0; _i < arguments.length; _i++) {\n                inputs[_i] = arguments[_i];\n            }\n            util.assert(inputs.every(function (t) { return t instanceof tensor_1.Tensor; }), 'The args passed in customGrad(f)(x1, x2,...) must all be tensors');\n            _this.customGradientDepth++;\n            var gradientsFunc;\n            var gradientsMode = true;\n            var result = globals_1.tidy(f.name, function () {\n                var _a = f.apply(void 0, inputs), value = _a.value, gradFunc = _a.gradFunc;\n                util.assert(value instanceof tensor_1.Tensor, 'The function f passed in customGrad(f) must return an object ' +\n                    'where `obj.value` is a tensor');\n                util.assert(util.isFunction(gradFunc), 'The function f passed in customGrad(f) must return an object ' +\n                    'where `obj.gradFunc` is a function.');\n                gradientsFunc = gradFunc;\n                return value;\n            }, gradientsMode);\n            _this.customGradientDepth--;\n            if (_this.shouldRecord()) {\n                var gradFunc = function (dy) {\n                    var res = gradientsFunc(dy);\n                    var grads = Array.isArray(res) ? res : [res];\n                    util.assert(grads.length === inputs.length, 'The function f passed in customGrad(f) must return an object ' +\n                        'where `obj.gradFunc` is a function that returns the same ' +\n                        'number of tensors as inputs passed to f(...).');\n                    util.assert(grads.every(function (t) { return t instanceof tensor_1.Tensor; }), 'The function f passed in customGrad(f) must return an object ' +\n                        'where `obj.gradFunc` is a function that returns a list of ' +\n                        'only tensors.');\n                    return grads;\n                };\n                _this.addTapeNode(inputs, result, gradFunc);\n            }\n            return result;\n        };\n    };\n    Engine.prototype.write = function (dataId, values) {\n        this.backend.write(dataId, values);\n    };\n    Engine.prototype.readSync = function (dataId) {\n        return this.backend.readSync(dataId);\n    };\n    Engine.prototype.read = function (dataId) {\n        return this.backend.read(dataId);\n    };\n    Engine.prototype.fromPixels = function (pixels, numChannels) {\n        return this.backend.fromPixels(pixels, numChannels);\n    };\n    Engine.prototype.time = function (query) {\n        return __awaiter(this, void 0, void 0, function () {\n            var start, timingInfo;\n            return __generator(this, function (_a) {\n                switch (_a.label) {\n                    case 0:\n                        start = performance.now();\n                        return [4, this.backend.time(query)];\n                    case 1:\n                        timingInfo = _a.sent();\n                        timingInfo.wallMs = performance.now() - start;\n                        return [2, timingInfo];\n                }\n            });\n        });\n    };\n    Engine.prototype.track = function (result) {\n        if (this.scopeStack.length === 1 && this.safeMode) {\n            throw new Error('Safe mode is ON. Enclose all tensor operations inside tf.tidy(): ' +\n                'tf.tidy(() => {op();...}); to avoid memory leaks.');\n        }\n        this.activeScope.track.push(result);\n        return result;\n    };\n    return Engine;\n}());\nexports.Engine = Engine;\n","map":{"version":3,"file":"engine.js","sourceRoot":"","sources":["../src/engine.ts"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAiBA,6CAAkC;AAClC,qCAA+B;AAE/B,+BAAiC;AACjC,uCAAoC;AACpC,+BAAoE;AAEpE,mCAA4D;AAG5D,6BAA+B;AAwC/B;IAoBE,gBAAoB,OAAsB,EAAS,QAAiB;QAAhD,YAAO,GAAP,OAAO,CAAe;QAAS,aAAQ,GAAR,QAAQ,CAAS;QAlBpE,wBAAmB,GAAqB,EAAE,CAAC;QAEnC,eAAU,GAAG,IAAI,OAAO,EAAkB,CAAC;QAC3C,mBAAc,GAAG,CAAC,CAAC;QACnB,aAAQ,GAAG,CAAC,CAAC;QACb,eAAU,GAAG,CAAC,CAAC;QACf,mBAAc,GAAG,CAAC,CAAC;QAGnB,uBAAkB,GAAG,CAAC,CAAC;QACvB,wBAAmB,GAAG,CAAC,CAAC;QAKxB,gBAAW,GAAgB,IAAI,GAAG,EAAE,CAAC;QAK3C,IAAI,CAAC,WAAW,GAAG,EAAC,KAAK,EAAE,EAAE,EAAC,CAAC;QAC/B,IAAI,CAAC,UAAU,GAAG,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC;QACrC,IAAI,CAAC,QAAQ,GAAG,IAAI,mBAAQ,CAAC,OAAO,CAAC,CAAC;IACxC,CAAC;IAED,0BAAS,GAAT,UACI,WAA2B,EAC3B,MAAS,EACT,aAAwE;QAH5E,iBAsCC;QAjCC,IAAI,MAAS,CAAC;QACd,IAAM,KAAK,GAAa,EAAE,CAAC;QAC3B,IAAM,QAAQ,GAAG,UAAmB,CAAI;YACtC,KAAK,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;YACd,MAAM,CAAC,CAAC,CAAC;QACX,CAAC,CAAC;QACF,IAAM,SAAS,GAAG,IAAI,CAAC,WAAW,CAAC,IAAI,CAAC;QAGxC,IAAI,CAAC,mBAAmB,EAAE,CAAC;QAC3B,EAAE,CAAC,CAAC,CAAC,iBAAG,CAAC,GAAG,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;YACtB,MAAM,GAAG,WAAW,CAAC,IAAI,CAAC,OAAO,EAAE,QAAQ,CAAC,CAAC;QAC/C,CAAC;QAAC,IAAI,CAAC,CAAC;YACN,MAAM,GAAG,IAAI,CAAC,QAAQ,CAAC,aAAa,CAChC,SAAS,EAAE,cAAM,OAAA,WAAW,CAAC,KAAI,CAAC,OAAO,EAAE,QAAQ,CAAC,EAAnC,CAAmC,CAAC,CAAC;QAC5D,CAAC;QAED,IAAI,CAAC,mBAAmB,EAAE,CAAC;QAE3B,EAAE,CAAC,CAAC,IAAI,CAAC,YAAY,EAAE,CAAC,CAAC,CAAC;YACxB,IAAM,QAAQ,GAAa;gBACzB,EAAE,EAAE,IAAI,CAAC,cAAc,EAAE;gBACzB,IAAI,EAAE,SAAS;gBACf,MAAM,QAAA;gBACN,MAAM,EAAE,MAAM;aAEf,CAAC;YACF,EAAE,CAAC,CAAC,aAAa,IAAI,IAAI,CAAC,CAAC,CAAC;gBAC1B,QAAQ,CAAC,QAAQ,GAAG,UAAC,EAAK,IAAK,OAAA,aAAa,CAAC,EAAE,EAAE,KAAK,CAAC,EAAxB,CAAwB,CAAC;YAC1D,CAAC;YACD,IAAI,CAAC,UAAU,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;QACjC,CAAC;QACD,MAAM,CAAC,MAAM,CAAC;IAChB,CAAC;IAID,+BAAc,GAAd,UAAe,CAAkB;QAC/B,IAAM,QAAQ,GACV,IAAI,CAAC,UAAU,CAAC,GAAG,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,UAAU,CAAC,GAAG,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QACtE,IAAI,CAAC,UAAU,EAAE,CAAC;QAClB,EAAE,CAAC,CAAC,QAAQ,KAAK,CAAC,CAAC,CAAC,CAAC;YACnB,IAAI,CAAC,cAAc,EAAE,CAAC;YACtB,IAAI,CAAC,QAAQ;gBACT,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,KAAK,CAAC,GAAG,IAAI,CAAC,eAAe,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;YAChE,IAAI,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QACpD,CAAC;QACD,IAAI,CAAC,UAAU,CAAC,GAAG,CAAC,CAAC,CAAC,MAAM,EAAE,QAAQ,GAAG,CAAC,CAAC,CAAC;QAC5C,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,YAAY,iBAAQ,CAAC,CAAC,CAAC,CAAC;YAC7B,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;QAChB,CAAC;IACH,CAAC;IAED,iCAAgB,GAAhB,UAAiB,CAAW;QAC1B,EAAE,CAAC,CAAC,IAAI,CAAC,mBAAmB,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC;YAC7C,MAAM,IAAI,KAAK,CAAC,wBAAsB,CAAC,CAAC,IAAI,4BAAyB,CAAC,CAAC;QACzE,CAAC;QACD,IAAI,CAAC,mBAAmB,CAAC,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;IACvC,CAAC;IAED,8BAAa,GAAb,UAAc,CAAS;QACrB,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,UAAU,CAAC,GAAG,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC;YACnC,MAAM,CAAC;QACT,CAAC;QACD,IAAI,CAAC,UAAU,EAAE,CAAC;QAClB,IAAM,QAAQ,GAAG,IAAI,CAAC,UAAU,CAAC,GAAG,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC;QAC/C,EAAE,CAAC,CAAC,QAAQ,IAAI,CAAC,CAAC,CAAC,CAAC;YAClB,IAAI,CAAC,UAAU,CAAC,MAAM,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC;YACjC,IAAI,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC;YACnC,IAAI,CAAC,cAAc,EAAE,CAAC;YACtB,IAAI,CAAC,QAAQ;gBACT,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,KAAK,CAAC,GAAG,IAAI,CAAC,eAAe,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;QAClE,CAAC;QAAC,IAAI,CAAC,CAAC;YACN,IAAI,CAAC,UAAU,CAAC,GAAG,CAAC,CAAC,CAAC,MAAM,EAAE,QAAQ,GAAG,CAAC,CAAC,CAAC;QAC9C,CAAC;IAIH,CAAC;IAED,iCAAgB,GAAhB;QACE,GAAG,CAAC,CAAC,IAAM,OAAO,IAAI,IAAI,CAAC,mBAAmB,CAAC,CAAC,CAAC;YAC/C,IAAM,CAAC,GAAG,IAAI,CAAC,mBAAmB,CAAC,OAAO,CAAC,CAAC;YAC5C,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;YACtB,OAAO,IAAI,CAAC,mBAAmB,CAAC,OAAO,CAAC,CAAC;QAC3C,CAAC;IACH,CAAC;IAED,uBAAM,GAAN;QACE,IAAM,IAAI,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM,EAAgB,CAAC;QACjD,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,UAAU,CAAC;QAClC,IAAI,CAAC,cAAc,GAAG,IAAI,CAAC,cAAc,CAAC;QAC1C,IAAI,CAAC,QAAQ,GAAG,IAAI,CAAC,QAAQ,CAAC;QAC9B,MAAM,CAAC,IAAI,CAAC;IACd,CAAC;IAEO,6BAAY,GAApB;QACE,MAAM,CAAC,IAAI,CAAC,UAAU,IAAI,IAAI,IAAI,IAAI,CAAC,mBAAmB,KAAK,CAAC,CAAC;IACnE,CAAC;IAEO,4BAAW,GAAnB,UACI,MAAgB,EAAE,MAAc,EAChC,aAAuC;QACzC,IAAM,SAAS,GAAmB,EAAE,CAAC;QACrC,MAAM,CAAC,OAAO,CAAC,UAAC,KAAK,EAAE,GAAG;YACxB,SAAS,CAAC,GAAG,CAAC,GAAG,KAAK,CAAC;QACzB,CAAC,CAAC,CAAC;QAEH,IAAM,QAAQ,GAAG,UAAC,EAAU;YAC1B,IAAM,GAAG,GAAG,aAAa,CAAC,EAAE,CAAC,CAAC;YAC9B,IAAM,MAAM,GAAqB,EAAE,CAAC;YACpC,GAAG,CAAC,OAAO,CAAC,UAAC,CAAC,EAAE,GAAG;gBACjB,MAAM,CAAC,GAAG,CAAC,GAAG,cAAM,OAAA,CAAC,EAAD,CAAC,CAAC;YACxB,CAAC,CAAC,CAAC;YACH,MAAM,CAAC,MAAM,CAAC;QAChB,CAAC,CAAC;QAEF,IAAM,QAAQ,GAAa;YACzB,EAAE,EAAE,IAAI,CAAC,cAAc,EAAE;YACzB,IAAI,EAAE,IAAI,CAAC,WAAW,CAAC,IAAI;YAC3B,MAAM,EAAE,SAAS;YACjB,MAAM,EAAE,MAAM;YACd,QAAQ,UAAA;SACT,CAAC;QACF,IAAI,CAAC,UAAU,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;IACjC,CAAC;IAED,qBAAI,GAAJ,UAAuB,MAAS;QAC9B,EAAE,CAAC,CAAC,IAAI,CAAC,UAAU,CAAC,MAAM,KAAK,CAAC,IAAI,iBAAG,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC,CAAC;YACxD,MAAM,IAAI,KAAK,CACX,mEAAmE;gBACnE,6CAA6C,CAAC,CAAC;QACrD,CAAC;QACD,IAAI,CAAC,WAAW,CAAC,GAAG,CAAC,MAAM,CAAC,EAAE,CAAC,CAAC;QAChC,MAAM,CAAC,MAAM,CAAC;IAChB,CAAC;IAMD,2BAAU,GAAV,UAAW,IAAa,EAAE,aAAqB;QAArB,8BAAA,EAAA,qBAAqB;QAC7C,EAAE,CAAC,CAAC,aAAa,IAAI,IAAI,CAAC,kBAAkB,KAAK,CAAC,CAAC,CAAC,CAAC;YACnD,IAAI,CAAC,UAAU,GAAG,EAAE,CAAC;QACvB,CAAC;QACD,EAAE,CAAC,CAAC,aAAa,CAAC,CAAC,CAAC;YAClB,IAAI,CAAC,kBAAkB,EAAE,CAAC;QAC5B,CAAC;QAED,IAAM,SAAS,GAAe,EAAC,KAAK,EAAE,EAAE,EAAC,CAAC;QAC1C,EAAE,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC;YACT,SAAS,CAAC,IAAI,GAAG,IAAI,CAAC;QACxB,CAAC;QACD,IAAI,CAAC,UAAU,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;QAChC,IAAI,CAAC,WAAW,GAAG,SAAS,CAAC;IAC/B,CAAC;IAMD,yBAAQ,GAAR,UAAS,MAAuB,EAAE,aAAqB;QAAvD,iBAyCC;QAzCiC,8BAAA,EAAA,qBAAqB;QACrD,EAAE,CAAC,CAAC,aAAa,CAAC,CAAC,CAAC;YAClB,IAAI,CAAC,kBAAkB,EAAE,CAAC;YAC1B,EAAE,CAAC,CAAC,IAAI,CAAC,kBAAkB,KAAK,CAAC,CAAC,CAAC,CAAC;gBAClC,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC;YACzB,CAAC;QACH,CAAC;QAED,IAAM,aAAa,GAAG,IAAI,GAAG,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC;QAEhD,IAAM,sBAAsB,GAAG,IAAI,CAAC,qBAAqB,CAAC,MAAM,CAAC,CAAC;QAClE,sBAAsB,CAAC,OAAO,CAAC,UAAA,MAAM,IAAI,OAAA,aAAa,CAAC,GAAG,CAAC,MAAM,CAAC,EAAE,CAAC,EAA5B,CAA4B,CAAC,CAAC;QAGvE,GAAG,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,WAAW,CAAC,KAAK,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;YACvD,IAAM,MAAM,GAAG,IAAI,CAAC,WAAW,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;YACzC,EAAE,CAAC,CAAC,aAAa,CAAC,GAAG,CAAC,MAAM,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;gBACjC,QAAQ,CAAC;YACX,CAAC;YAED,EAAE,CAAC,CAAC,IAAI,CAAC,UAAU,IAAI,IAAI,CAAC,CAAC,CAAC;gBAC5B,sBAAsB,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;YACtC,CAAC;YAAC,IAAI,CAAC,CAAC;gBACN,MAAM,CAAC,OAAO,EAAE,CAAC;YACnB,CAAC;QACH,CAAC;QAED,IAAM,QAAQ,GAAG,IAAI,CAAC,UAAU,CAAC,GAAG,EAAE,CAAC;QACvC,IAAI,CAAC,WAAW,GAAG,IAAI,CAAC,UAAU,CAAC,MAAM,KAAK,CAAC,CAAC,CAAC;YAC7C,EAAC,KAAK,EAAE,EAAE,EAAC,CAAC,CAAC;YACb,IAAI,CAAC,UAAU,CAAC,IAAI,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;QAGhD,sBAAsB,CAAC,OAAO,CAAC,UAAA,MAAM;YAGnC,EAAE,CAAC,CAAC,CAAC,KAAI,CAAC,WAAW,CAAC,GAAG,CAAC,MAAM,CAAC,EAAE,CAAC;gBAChC,IAAI,CAAC,cAAc,CAAC,MAAM,EAAE,QAAQ,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;gBAChD,KAAI,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC;YACrB,CAAC;QACH,CAAC,CAAC,CAAC;IACL,CAAC;IAQD,0BAAS,GAAT,UACI,CAAU,EAAE,EAAY,EAAE,EAAM,EAChC,gBAAwB;QAF5B,iBA4BC;QA1BG,iCAAA,EAAA,wBAAwB;QAC1B,IAAI,CAAC,MAAM,CAAC,EAAE,CAAC,MAAM,GAAG,CAAC,EAAE,2CAA2C,CAAC,CAAC;QAExE,MAAM,CAAC,cAAI,CAAC,WAAW,EAAE;YACvB,IAAM,CAAC,GAAG,CAAC,EAAE,CAAC;YACd,IAAI,CAAC,MAAM,CACP,CAAC,YAAY,eAAM,EACnB,gDAAgD,CAAC,CAAC;YAEtD,IAAM,YAAY,GAAG,2BAAoB,CAAC,KAAI,CAAC,UAAU,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC;YAClE,EAAE,CAAC,CAAC,CAAC,gBAAgB,IAAI,YAAY,CAAC,MAAM,KAAK,CAAC,IAAI,EAAE,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;gBACpE,MAAM,IAAI,KAAK,CACX,iEAAiE;oBACjE,iEAAiE;oBACjE,OAAO,CAAC,CAAC;YACf,CAAC;YAED,IAAM,sBAAsB,GAAiC,EAAE,CAAC;YAChE,sBAAsB,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC;YAGrE,6BAAsB,CAAC,sBAAsB,EAAE,YAAY,CAAC,CAAC;YAE7D,IAAM,KAAK,GAAG,EAAE,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,sBAAsB,CAAC,CAAC,CAAC,EAAE,CAAC,EAA5B,CAA4B,CAAC,CAAC;YACxD,MAAM,CAAC,EAAC,KAAK,EAAE,CAAC,EAAE,KAAK,OAAA,EAAC,CAAC;QAC3B,CAAC,EAAE,IAAI,CAAqB,CAAC;IAC/B,CAAC;IAED,2BAAU,GAAV,UAA6B,CAAwB;QAArD,iBAiDC;QA/CC,IAAI,CAAC,MAAM,CACP,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC,EAClB,mDAAmD,CAAC,CAAC;QACzD,MAAM,CAAC;YAAC,gBAAmB;iBAAnB,UAAmB,EAAnB,qBAAmB,EAAnB,IAAmB;gBAAnB,2BAAmB;;YACzB,IAAI,CAAC,MAAM,CACP,MAAM,CAAC,KAAK,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,YAAY,eAAM,EAAnB,CAAmB,CAAC,EACtC,kEAAkE,CAAC,CAAC;YACxE,KAAI,CAAC,mBAAmB,EAAE,CAAC;YAE3B,IAAI,aAA2C,CAAC;YAChD,IAAM,aAAa,GAAG,IAAI,CAAC;YAC3B,IAAM,MAAM,GAAG,cAAI,CAAC,CAAC,CAAC,IAAI,EAAE;gBACpB,IAAA,4BAAgC,EAA/B,gBAAK,EAAE,sBAAQ,CAAiB;gBACvC,IAAI,CAAC,MAAM,CACP,KAAK,YAAY,eAAM,EACvB,+DAA+D;oBAC3D,+BAA+B,CAAC,CAAC;gBACzC,IAAI,CAAC,MAAM,CACP,IAAI,CAAC,UAAU,CAAC,QAAQ,CAAC,EACzB,+DAA+D;oBAC3D,qCAAqC,CAAC,CAAC;gBAC/C,aAAa,GAAG,QAAQ,CAAC;gBACzB,MAAM,CAAC,KAAK,CAAC;YACf,CAAC,EAAE,aAAa,CAAC,CAAC;YAElB,KAAI,CAAC,mBAAmB,EAAE,CAAC;YAE3B,EAAE,CAAC,CAAC,KAAI,CAAC,YAAY,EAAE,CAAC,CAAC,CAAC;gBACxB,IAAM,QAAQ,GAAG,UAAC,EAAK;oBACrB,IAAM,GAAG,GAAG,aAAa,CAAC,EAAE,CAAC,CAAC;oBAC9B,IAAM,KAAK,GAAa,KAAK,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC;oBACzD,IAAI,CAAC,MAAM,CACP,KAAK,CAAC,MAAM,KAAK,MAAM,CAAC,MAAM,EAC9B,+DAA+D;wBAC3D,2DAA2D;wBAC3D,+CAA+C,CAAC,CAAC;oBACzD,IAAI,CAAC,MAAM,CACP,KAAK,CAAC,KAAK,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,YAAY,eAAM,EAAnB,CAAmB,CAAC,EACrC,+DAA+D;wBAC3D,4DAA4D;wBAC5D,eAAe,CAAC,CAAC;oBACzB,MAAM,CAAC,KAAK,CAAC;gBACf,CAAC,CAAC;gBACF,KAAI,CAAC,WAAW,CAAC,MAAM,EAAE,MAAM,EAAE,QAAQ,CAAC,CAAC;YAC7C,CAAC;YACD,MAAM,CAAC,MAAM,CAAC;QAChB,CAAC,CAAC;IACJ,CAAC;IAGD,sBAAK,GAAL,UAAM,MAAc,EAAE,MAAkB;QACtC,IAAI,CAAC,OAAO,CAAC,KAAK,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;IACrC,CAAC;IACD,yBAAQ,GAAR,UAAS,MAAc;QACrB,MAAM,CAAC,IAAI,CAAC,OAAO,CAAC,QAAQ,CAAC,MAAM,CAAC,CAAC;IACvC,CAAC;IACD,qBAAI,GAAJ,UAAK,MAAc;QACjB,MAAM,CAAC,IAAI,CAAC,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;IACnC,CAAC;IACD,2BAAU,GAAV,UACI,MAAqE,EACrE,WAAmB;QACrB,MAAM,CAAC,IAAI,CAAC,OAAO,CAAC,UAAU,CAAC,MAAM,EAAE,WAAW,CAAC,CAAC;IACtD,CAAC;IACK,qBAAI,GAAV,UAAW,KAAiB;;;;;;wBACpB,KAAK,GAAG,WAAW,CAAC,GAAG,EAAE,CAAC;wBACb,WAAM,IAAI,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,EAAA;;wBAA3C,UAAU,GAAG,SAA4C;wBAC/D,UAAU,CAAC,MAAM,GAAG,WAAW,CAAC,GAAG,EAAE,GAAG,KAAK,CAAC;wBAC9C,WAAO,UAAU,EAAC;;;;KACnB;IAQO,sBAAK,GAAb,UAAgC,MAAS;QACvC,EAAE,CAAC,CAAC,IAAI,CAAC,UAAU,CAAC,MAAM,KAAK,CAAC,IAAI,IAAI,CAAC,QAAQ,CAAC,CAAC,CAAC;YAClD,MAAM,IAAI,KAAK,CACX,mEAAmE;gBACnE,mDAAmD,CAAC,CAAC;QAC3D,CAAC;QACD,IAAI,CAAC,WAAW,CAAC,KAAK,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;QACpC,MAAM,CAAC,MAAM,CAAC;IAChB,CAAC;IACH,aAAC;AAAD,CAAC,AAxWD,IAwWC;AAxWY,wBAAM","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENV} from './environment';\nimport {tidy} from './globals';\nimport {BackendTimingInfo, KernelBackend} from './kernels/backend';\nimport * as ops from './ops/ops';\nimport {Profiler} from './profiler';\nimport {backpropagateGradients, getFilteredNodesXToY} from './tape';\nimport {NamedGradientMap, TapeNode} from './tape';\nimport {DataId, Tensor, Tensor3D, Variable} from './tensor';\n// tslint:disable-next-line:max-line-length\nimport {NamedTensorMap, NamedVariableMap, TensorContainer, TypedArray} from './types';\nimport * as util from './util';\n\ninterface ScopeState {\n  track: Tensor[];\n  name?: string;\n}\n\n/**\n * A function that computes an output. The save function is for saving tensors\n * computed in the forward pass, that we need in the backwards pass.\n */\nexport type ForwardFunc<T extends Tensor> =\n    (backend: KernelBackend, save?: <S extends Tensor>(tensor: S) => S) => T;\n\n/**\n * @docalias (a: Tensor, b: Tensor,...) => {\n *   value: Tensor,\n *   gradFunc: (dy: Tensor) => Tensor|Tensor[]\n * }\n */\nexport type CustomGradientFunc<T extends Tensor> = (...args: Tensor[]) => {\n  value: T, gradFunc: (dy: T) => Tensor | Tensor[];\n};\n\nexport interface TensorManager {\n  registerTensor(a: Tensor): void;\n  registerVariable(v: Variable): void;\n  disposeTensor(a: Tensor): void;\n  memory(): {numDataBuffers: number; numBytes: number;};\n}\n\nexport type MemoryInfo = {\n  numTensors: number; numDataBuffers: number; numBytes: number;\n  unreliable?: boolean;\n};\n\nexport interface TimingInfo extends BackendTimingInfo {\n  wallMs: number;\n}\n\nexport class Engine implements TensorManager {\n  // Public since optimizers will use it.\n  registeredVariables: NamedVariableMap = {};\n\n  private refCounter = new WeakMap<DataId, number>();\n  private nextTapeNodeId = 0;\n  private numBytes = 0;\n  private numTensors = 0;\n  private numDataBuffers = 0;\n\n  private activeTape: TapeNode[];\n  private gradientScopeCount = 0;\n  private customGradientDepth = 0;\n\n  // Keep Tensors that parallel the tapes.\n  private activeScope: ScopeState;\n  private scopeStack: ScopeState[];\n  private keepTensors: Set<number> = new Set();\n  private profiler: Profiler;\n\n  constructor(private backend: KernelBackend, public safeMode: boolean) {\n    // Create a default outer scope.\n    this.activeScope = {track: []};\n    this.scopeStack = [this.activeScope];\n    this.profiler = new Profiler(backend);\n  }\n\n  runKernel<T extends Tensor, I extends NamedTensorMap>(\n      forwardFunc: ForwardFunc<T>,\n      inputs: I,\n      backwardsFunc?: (dy: T, saved: Tensor[]) => {[P in keyof I]: () => I[P]},\n      ): T {\n    let result: T;\n    const saved: Tensor[] = [];\n    const saveFunc = <T extends Tensor>(x: T): T => {\n      saved.push(x);\n      return x;\n    };\n    const scopeName = this.activeScope.name;\n\n    // Stop recording to a tape when running a kernel.\n    this.customGradientDepth++;\n    if (!ENV.get('DEBUG')) {\n      result = forwardFunc(this.backend, saveFunc);\n    } else {\n      result = this.profiler.profileKernel(\n          scopeName, () => forwardFunc(this.backend, saveFunc));\n    }\n    // Continue recording after the kernel is done.\n    this.customGradientDepth--;\n\n    if (this.shouldRecord()) {\n      const tapeNode: TapeNode = {\n        id: this.nextTapeNodeId++,\n        name: scopeName,\n        inputs,\n        output: result,\n\n      };\n      if (backwardsFunc != null) {\n        tapeNode.gradient = (dy: T) => backwardsFunc(dy, saved);\n      }\n      this.activeTape.push(tapeNode);\n    }\n    return result;\n  }\n\n  // TensorManager implementation.\n\n  registerTensor(a: Tensor|Variable): void {\n    const refCount =\n        this.refCounter.has(a.dataId) ? this.refCounter.get(a.dataId) : 0;\n    this.numTensors++;\n    if (refCount === 0) {\n      this.numDataBuffers++;\n      this.numBytes +=\n          util.sizeFromShape(a.shape) * util.bytesPerElement(a.dtype);\n      this.backend.register(a.dataId, a.shape, a.dtype);\n    }\n    this.refCounter.set(a.dataId, refCount + 1);\n    if (!(a instanceof Variable)) {\n      this.track(a);\n    }\n  }\n\n  registerVariable(v: Variable) {\n    if (this.registeredVariables[v.name] != null) {\n      throw new Error(`Variable with name ${v.name} was already registered`);\n    }\n    this.registeredVariables[v.name] = v;\n  }\n\n  disposeTensor(a: Tensor): void {\n    if (!this.refCounter.has(a.dataId)) {\n      return;\n    }\n    this.numTensors--;\n    const refCount = this.refCounter.get(a.dataId);\n    if (refCount <= 1) {\n      this.refCounter.delete(a.dataId);\n      this.backend.disposeData(a.dataId);\n      this.numDataBuffers--;\n      this.numBytes -=\n          util.sizeFromShape(a.shape) * util.bytesPerElement(a.dtype);\n    } else {\n      this.refCounter.set(a.dataId, refCount - 1);\n    }\n    // TODO(nsthorat): Construct an error and save the stack trace for\n    // debugging when in debug mode. Creating a stack trace is too expensive\n    // to do unconditionally.\n  }\n\n  disposeVariables(): void {\n    for (const varName in this.registeredVariables) {\n      const v = this.registeredVariables[varName];\n      this.disposeTensor(v);\n      delete this.registeredVariables[varName];\n    }\n  }\n\n  memory(): MemoryInfo {\n    const info = this.backend.memory() as MemoryInfo;\n    info.numTensors = this.numTensors;\n    info.numDataBuffers = this.numDataBuffers;\n    info.numBytes = this.numBytes;\n    return info;\n  }\n\n  private shouldRecord(): boolean {\n    return this.activeTape != null && this.customGradientDepth === 0;\n  }\n\n  private addTapeNode(\n      inputs: Tensor[], result: Tensor,\n      gradientsFunc: (dy: Tensor) => Tensor[]): void {\n    const inputsMap: NamedTensorMap = {};\n    inputs.forEach((input, idx) => {\n      inputsMap[idx] = input;\n    });\n\n    const gradient = (dy: Tensor) => {\n      const res = gradientsFunc(dy);\n      const resMap: NamedGradientMap = {};\n      res.forEach((r, idx) => {\n        resMap[idx] = () => r;\n      });\n      return resMap;\n    };\n\n    const tapeNode: TapeNode = {\n      id: this.nextTapeNodeId++,\n      name: this.activeScope.name,\n      inputs: inputsMap,\n      output: result,\n      gradient\n    };\n    this.activeTape.push(tapeNode);\n  }\n\n  keep<T extends Tensor>(result: T): T {\n    if (this.scopeStack.length === 1 && ENV.engine.safeMode) {\n      throw new Error(\n          'Safe mode is ON. Enclose all tensor operations inside tf.tidy(): ' +\n          'tf.tidy(() => {...}) to avoid memory leaks.');\n    }\n    this.keepTensors.add(result.id);\n    return result;\n  }\n\n  /**\n   * Start a scope. Use this with endScope() to achieve the same functionality\n   * as scope() without the need for a function closure.\n   */\n  startScope(name?: string, gradientsMode = false) {\n    if (gradientsMode && this.gradientScopeCount === 0) {\n      this.activeTape = [];\n    }\n    if (gradientsMode) {\n      this.gradientScopeCount++;\n    }\n\n    const scopeInfo: ScopeState = {track: []};\n    if (name) {\n      scopeInfo.name = name;\n    }\n    this.scopeStack.push(scopeInfo);\n    this.activeScope = scopeInfo;\n  }\n\n  /**\n   * End a scope. Use this with startScope() to achieve the same functionality\n   * as scope() without the need for a function closure.\n   */\n  endScope(result: TensorContainer, gradientsMode = false) {\n    if (gradientsMode) {\n      this.gradientScopeCount--;\n      if (this.gradientScopeCount === 0) {\n        this.activeTape = null;\n      }\n    }\n\n    const tensorsToKeep = new Set(this.keepTensors);\n\n    const tensorsToTrackInParent = util.getTensorsInContainer(result);\n    tensorsToTrackInParent.forEach(tensor => tensorsToKeep.add(tensor.id));\n\n    // Dispose the arrays tracked in this scope.\n    for (let i = 0; i < this.activeScope.track.length; i++) {\n      const tensor = this.activeScope.track[i];\n      if (tensorsToKeep.has(tensor.id)) {\n        continue;\n      }\n\n      if (this.activeTape != null) {\n        tensorsToTrackInParent.push(tensor);\n      } else {\n        tensor.dispose();\n      }\n    }\n\n    const oldScope = this.scopeStack.pop();\n    this.activeScope = this.scopeStack.length === 0 ?\n        {track: []} :\n        this.scopeStack[this.scopeStack.length - 1];\n\n    // Track the current result in the parent scope.\n    tensorsToTrackInParent.forEach(tensor => {\n      // Only track the tensor if was allocated in the inner scope and is not\n      // globally kept.\n      if (!this.keepTensors.has(tensor.id) &&\n          util.isTensorInList(tensor, oldScope.track)) {\n        this.track(tensor);\n      }\n    });\n  }\n\n  /**\n   * Returns gradients of `f` with respect to each of the `xs`. The gradients\n   * returned are of the same length as `xs`, but some might be null if `f` was\n   * not a function of that `x`. It also takes optional dy to multiply the\n   * gradient, which defaults to `1`.\n   */\n  gradients<T extends Tensor>(\n      f: () => T, xs: Tensor[], dy?: T,\n      allowNoGradients = false): {value: T, grads: Tensor[]} {\n    util.assert(xs.length > 0, 'gradients() received an empty list of xs.');\n\n    return tidy('gradients', () => {\n      const y = f();\n      util.assert(\n          y instanceof Tensor,\n          'The result y returned by f() must be a tensor.');\n      // Filter out the nodes that don't connect x => y.\n      const filteredTape = getFilteredNodesXToY(this.activeTape, xs, y);\n      if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) {\n        throw new Error(\n            'Cannot compute gradient of y=f(x) with respect to x. Make sure ' +\n            'that the f you passed encloses all operations that lead from x ' +\n            'to y.');\n      }\n\n      const accumulatedGradientMap: {[tensorId: number]: Tensor} = {};\n      accumulatedGradientMap[y.id] = (dy == null) ? ops.ones(y.shape) : dy;\n\n      // Backprop gradients through the filtered nodes.\n      backpropagateGradients(accumulatedGradientMap, filteredTape);\n\n      const grads = xs.map(x => accumulatedGradientMap[x.id]);\n      return {value: y, grads};\n    }, true /* gradientsMode */);\n  }\n\n  customGrad<T extends Tensor>(f: CustomGradientFunc<T>):\n      (...args: Tensor[]) => T {\n    util.assert(\n        util.isFunction(f),\n        'The f passed in customGrad(f) must be a function.');\n    return (...inputs: Tensor[]): T => {\n      util.assert(\n          inputs.every(t => t instanceof Tensor),\n          'The args passed in customGrad(f)(x1, x2,...) must all be tensors');\n      this.customGradientDepth++;\n\n      let gradientsFunc: (dy: T) => Tensor | Tensor[];\n      const gradientsMode = true;\n      const result = tidy(f.name, () => {\n        const {value, gradFunc} = f(...inputs);\n        util.assert(\n            value instanceof Tensor,\n            'The function f passed in customGrad(f) must return an object ' +\n                'where `obj.value` is a tensor');\n        util.assert(\n            util.isFunction(gradFunc),\n            'The function f passed in customGrad(f) must return an object ' +\n                'where `obj.gradFunc` is a function.');\n        gradientsFunc = gradFunc;\n        return value;\n      }, gradientsMode);\n\n      this.customGradientDepth--;\n\n      if (this.shouldRecord()) {\n        const gradFunc = (dy: T): Tensor[] => {\n          const res = gradientsFunc(dy);\n          const grads: Tensor[] = Array.isArray(res) ? res : [res];\n          util.assert(\n              grads.length === inputs.length,\n              'The function f passed in customGrad(f) must return an object ' +\n                  'where `obj.gradFunc` is a function that returns the same ' +\n                  'number of tensors as inputs passed to f(...).');\n          util.assert(\n              grads.every(t => t instanceof Tensor),\n              'The function f passed in customGrad(f) must return an object ' +\n                  'where `obj.gradFunc` is a function that returns a list of ' +\n                  'only tensors.');\n          return grads;\n        };\n        this.addTapeNode(inputs, result, gradFunc);\n      }\n      return result;\n    };\n  }\n\n  // Forwarding to backend.\n  write(dataId: DataId, values: TypedArray): void {\n    this.backend.write(dataId, values);\n  }\n  readSync(dataId: DataId): TypedArray {\n    return this.backend.readSync(dataId);\n  }\n  read(dataId: DataId): Promise<TypedArray> {\n    return this.backend.read(dataId);\n  }\n  fromPixels(\n      pixels: ImageData|HTMLImageElement|HTMLCanvasElement|HTMLVideoElement,\n      numChannels: number): Tensor3D {\n    return this.backend.fromPixels(pixels, numChannels);\n  }\n  async time(query: () => void): Promise<TimingInfo> {\n    const start = performance.now();\n    const timingInfo = await this.backend.time(query) as TimingInfo;\n    timingInfo.wallMs = performance.now() - start;\n    return timingInfo;\n  }\n\n  /**\n   * Tracks a Tensor in the current scope to be automatically cleaned up\n   * when the current scope ends, and returns the value.\n   *\n   * @param result The Tensor to track in the current scope.\n   */\n  private track<T extends Tensor>(result: T): T {\n    if (this.scopeStack.length === 1 && this.safeMode) {\n      throw new Error(\n          'Safe mode is ON. Enclose all tensor operations inside tf.tidy(): ' +\n          'tf.tidy(() => {op();...}); to avoid memory leaks.');\n    }\n    this.activeScope.track.push(result);\n    return result;\n  }\n}\n\n/** @docalias Function */\nexport type ScopeFn<T extends TensorContainer> = () => T;\n"]}},"hash":"12b1b89d3f45cdf8c02d3b60004b3e44","cacheData":{"env":{}}}