{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/package.json","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1524062920943},{"name":"../environment","loc":{"line":13,"column":28}},{"name":"../globals","loc":{"line":14,"column":24}},{"name":"../ops/ops","loc":{"line":15,"column":20}},{"name":"../serialization","loc":{"line":16,"column":30}},{"name":"./optimizer","loc":{"line":17,"column":26}}],"generated":{"js":"\"use strict\";\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = Object.setPrototypeOf ||\n        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n        function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar environment_1 = require(\"../environment\");\nvar globals_1 = require(\"../globals\");\nvar ops_1 = require(\"../ops/ops\");\nvar serialization_1 = require(\"../serialization\");\nvar optimizer_1 = require(\"./optimizer\");\nvar AdadeltaOptimizer = (function (_super) {\n    __extends(AdadeltaOptimizer, _super);\n    function AdadeltaOptimizer(learningRate, rho, epsilon) {\n        if (epsilon === void 0) { epsilon = 1e-8; }\n        var _this = _super.call(this) || this;\n        _this.learningRate = learningRate;\n        _this.rho = rho;\n        _this.epsilon = epsilon;\n        _this.accumulatedGrads = {};\n        _this.accumulatedUpdates = {};\n        _this.c = globals_1.keep(ops_1.scalar(-learningRate));\n        _this.epsilonScalar = globals_1.keep(ops_1.scalar(epsilon));\n        _this.rhoScalar = globals_1.keep(ops_1.scalar(rho));\n        _this.oneMinusRho = globals_1.keep(ops_1.scalar(1 - rho));\n        return _this;\n    }\n    AdadeltaOptimizer.prototype.applyGradients = function (variableGradients) {\n        var _this = this;\n        var _loop_1 = function (variableName) {\n            var value = environment_1.ENV.engine.registeredVariables[variableName];\n            if (this_1.accumulatedGrads[variableName] == null) {\n                var trainable_1 = false;\n                globals_1.tidy(function () {\n                    _this.accumulatedGrads[variableName] =\n                        ops_1.zerosLike(value).variable(trainable_1);\n                });\n            }\n            if (this_1.accumulatedUpdates[variableName] == null) {\n                var trainable_2 = false;\n                globals_1.tidy(function () {\n                    _this.accumulatedUpdates[variableName] =\n                        ops_1.zerosLike(value).variable(trainable_2);\n                });\n            }\n            var gradient = variableGradients[variableName];\n            var accumulatedGrad = this_1.accumulatedGrads[variableName];\n            var accumulatedUpdate = this_1.accumulatedUpdates[variableName];\n            globals_1.tidy(function () {\n                var newAccumulatedGrad = _this.rhoScalar.mul(accumulatedGrad)\n                    .add(_this.oneMinusRho.mul(gradient.square()));\n                var updates = accumulatedUpdate.add(_this.epsilonScalar)\n                    .sqrt()\n                    .div(accumulatedGrad.add(_this.epsilonScalar).sqrt())\n                    .mul(gradient);\n                var newAccumulatedUpdate = _this.rhoScalar.mul(accumulatedUpdate)\n                    .add(_this.oneMinusRho.mul(updates.square()));\n                _this.accumulatedGrads[variableName].assign(newAccumulatedGrad);\n                _this.accumulatedUpdates[variableName].assign(newAccumulatedUpdate);\n                var newValue = _this.c.mul(updates).add(value);\n                value.assign(newValue);\n            });\n        };\n        var this_1 = this;\n        for (var variableName in variableGradients) {\n            _loop_1(variableName);\n        }\n    };\n    AdadeltaOptimizer.prototype.dispose = function () {\n        var _this = this;\n        this.c.dispose();\n        this.epsilonScalar.dispose();\n        this.rhoScalar.dispose();\n        this.oneMinusRho.dispose();\n        if (this.accumulatedUpdates != null) {\n            Object.keys(this.accumulatedUpdates)\n                .forEach(function (name) { return _this.accumulatedUpdates[name].dispose(); });\n            Object.keys(this.accumulatedGrads)\n                .forEach(function (name) { return _this.accumulatedGrads[name].dispose(); });\n        }\n    };\n    AdadeltaOptimizer.prototype.getConfig = function () {\n        return {\n            learningRate: this.learningRate,\n            rho: this.rho,\n            epsilon: this.epsilon\n        };\n    };\n    AdadeltaOptimizer.fromConfig = function (cls, config) {\n        return new cls(config.learningRate, config.rho, config.epsilon);\n    };\n    AdadeltaOptimizer.className = 'AdadeltaOptimizer';\n    return AdadeltaOptimizer;\n}(optimizer_1.Optimizer));\nexports.AdadeltaOptimizer = AdadeltaOptimizer;\nserialization_1.SerializationMap.register(AdadeltaOptimizer);\n","map":{"version":3,"file":"adadelta_optimizer.js","sourceRoot":"","sources":["../src/optimizers/adadelta_optimizer.ts"],"names":[],"mappings":";;;;;;;;;;;;AAiBA,8CAAmC;AACnC,sCAAsC;AACtC,kCAA6C;AAE7C,kDAAqG;AAIrG,yCAAsC;AAGtC;IAAuC,qCAAS;IAU9C,2BACc,YAAoB,EAAY,GAAW,EAC3C,OAAc;QAAd,wBAAA,EAAA,cAAc;QAF5B,YAGE,iBAAO,SAKR;QAPa,kBAAY,GAAZ,YAAY,CAAQ;QAAY,SAAG,GAAH,GAAG,CAAQ;QAC3C,aAAO,GAAP,OAAO,CAAO;QALpB,sBAAgB,GAAqB,EAAE,CAAC;QACxC,wBAAkB,GAAqB,EAAE,CAAC;QAMhD,KAAI,CAAC,CAAC,GAAG,cAAI,CAAC,YAAM,CAAC,CAAC,YAAY,CAAC,CAAC,CAAC;QACrC,KAAI,CAAC,aAAa,GAAG,cAAI,CAAC,YAAM,CAAC,OAAO,CAAC,CAAC,CAAC;QAC3C,KAAI,CAAC,SAAS,GAAG,cAAI,CAAC,YAAM,CAAC,GAAG,CAAC,CAAC,CAAC;QACnC,KAAI,CAAC,WAAW,GAAG,cAAI,CAAC,YAAM,CAAC,CAAC,GAAG,GAAG,CAAC,CAAC,CAAC;;IAC3C,CAAC;IAED,0CAAc,GAAd,UAAe,iBAAmC;QAAlD,iBA2CC;gCA1CY,YAAY;YACrB,IAAM,KAAK,GAAG,iBAAG,CAAC,MAAM,CAAC,mBAAmB,CAAC,YAAY,CAAC,CAAC;YAC3D,EAAE,CAAC,CAAC,OAAK,gBAAgB,CAAC,YAAY,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC;gBAChD,IAAM,WAAS,GAAG,KAAK,CAAC;gBACxB,cAAI,CAAC;oBACH,KAAI,CAAC,gBAAgB,CAAC,YAAY,CAAC;wBAC/B,eAAS,CAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,WAAS,CAAC,CAAC;gBAC3C,CAAC,CAAC,CAAC;YACL,CAAC;YACD,EAAE,CAAC,CAAC,OAAK,kBAAkB,CAAC,YAAY,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC;gBAClD,IAAM,WAAS,GAAG,KAAK,CAAC;gBACxB,cAAI,CAAC;oBACH,KAAI,CAAC,kBAAkB,CAAC,YAAY,CAAC;wBACjC,eAAS,CAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,WAAS,CAAC,CAAC;gBAC3C,CAAC,CAAC,CAAC;YACL,CAAC;YAED,IAAM,QAAQ,GAAG,iBAAiB,CAAC,YAAY,CAAC,CAAC;YACjD,IAAM,eAAe,GAAG,OAAK,gBAAgB,CAAC,YAAY,CAAC,CAAC;YAC5D,IAAM,iBAAiB,GAAG,OAAK,kBAAkB,CAAC,YAAY,CAAC,CAAC;YAEhE,cAAI,CAAC;gBACH,IAAM,kBAAkB,GACpB,KAAI,CAAC,SAAS,CAAC,GAAG,CAAC,eAAe,CAAC;qBAC9B,GAAG,CAAC,KAAI,CAAC,WAAW,CAAC,GAAG,CAAC,QAAQ,CAAC,MAAM,EAAE,CAAC,CAAC,CAAC;gBAEtD,IAAM,OAAO,GAAG,iBAAiB,CAAC,GAAG,CAAC,KAAI,CAAC,aAAa,CAAC;qBACpC,IAAI,EAAE;qBACN,GAAG,CAAC,eAAe,CAAC,GAAG,CAAC,KAAI,CAAC,aAAa,CAAC,CAAC,IAAI,EAAE,CAAC;qBACnD,GAAG,CAAC,QAAQ,CAAC,CAAC;gBAEnC,IAAM,oBAAoB,GACtB,KAAI,CAAC,SAAS,CAAC,GAAG,CAAC,iBAAiB,CAAC;qBAChC,GAAG,CAAC,KAAI,CAAC,WAAW,CAAC,GAAG,CAAC,OAAO,CAAC,MAAM,EAAE,CAAC,CAAC,CAAC;gBAErD,KAAI,CAAC,gBAAgB,CAAC,YAAY,CAAC,CAAC,MAAM,CAAC,kBAAkB,CAAC,CAAC;gBAC/D,KAAI,CAAC,kBAAkB,CAAC,YAAY,CAAC,CAAC,MAAM,CAAC,oBAAoB,CAAC,CAAC;gBAEnE,IAAM,QAAQ,GAAG,KAAI,CAAC,CAAC,CAAC,GAAG,CAAC,OAAO,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC;gBAChD,KAAK,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;YACzB,CAAC,CAAC,CAAC;QACL,CAAC;;QAzCD,GAAG,CAAC,CAAC,IAAM,YAAY,IAAI,iBAAiB,CAAC;oBAAlC,YAAY;SAyCtB;IACH,CAAC;IAED,mCAAO,GAAP;QAAA,iBAWC;QAVC,IAAI,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC;QACjB,IAAI,CAAC,aAAa,CAAC,OAAO,EAAE,CAAC;QAC7B,IAAI,CAAC,SAAS,CAAC,OAAO,EAAE,CAAC;QACzB,IAAI,CAAC,WAAW,CAAC,OAAO,EAAE,CAAC;QAC3B,EAAE,CAAC,CAAC,IAAI,CAAC,kBAAkB,IAAI,IAAI,CAAC,CAAC,CAAC;YACpC,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,kBAAkB,CAAC;iBAC/B,OAAO,CAAC,UAAA,IAAI,IAAI,OAAA,KAAI,CAAC,kBAAkB,CAAC,IAAI,CAAC,CAAC,OAAO,EAAE,EAAvC,CAAuC,CAAC,CAAC;YAC9D,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,gBAAgB,CAAC;iBAC7B,OAAO,CAAC,UAAA,IAAI,IAAI,OAAA,KAAI,CAAC,gBAAgB,CAAC,IAAI,CAAC,CAAC,OAAO,EAAE,EAArC,CAAqC,CAAC,CAAC;QAC9D,CAAC;IACH,CAAC;IACD,qCAAS,GAAT;QACE,MAAM,CAAC;YACL,YAAY,EAAE,IAAI,CAAC,YAAY;YAC/B,GAAG,EAAE,IAAI,CAAC,GAAG;YACb,OAAO,EAAE,IAAI,CAAC,OAAO;SACtB,CAAC;IACJ,CAAC;IACM,4BAAU,GAAjB,UACI,GAA+B,EAAE,MAAkB;QACrD,MAAM,CAAC,IAAI,GAAG,CAAC,MAAM,CAAC,YAAY,EAAE,MAAM,CAAC,GAAG,EAAE,MAAM,CAAC,OAAO,CAAC,CAAC;IAClE,CAAC;IAtFM,2BAAS,GAAG,mBAAmB,CAAC;IAuFzC,wBAAC;CAAA,AAxFD,CAAuC,qBAAS,GAwF/C;AAxFY,8CAAiB;AAyF9B,gCAAgB,CAAC,QAAQ,CAAC,iBAAiB,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENV} from '../environment';\nimport {keep, tidy} from '../globals';\nimport {scalar, zerosLike} from '../ops/ops';\n// tslint:disable-next-line:max-line-length\nimport {ConfigDict, Serializable, SerializableConstructor, SerializationMap} from '../serialization';\nimport {Scalar} from '../tensor';\nimport {NamedVariableMap} from '../types';\n\nimport {Optimizer} from './optimizer';\n\n/** @doclink Optimizer */\nexport class AdadeltaOptimizer extends Optimizer {\n  static className = 'AdadeltaOptimizer';\n  private c: Scalar;\n  private epsilonScalar: Scalar;\n  private rhoScalar: Scalar;\n  private oneMinusRho: Scalar;\n\n  private accumulatedGrads: NamedVariableMap = {};\n  private accumulatedUpdates: NamedVariableMap = {};\n\n  constructor(\n      protected learningRate: number, protected rho: number,\n      protected epsilon = 1e-8) {\n    super();\n    this.c = keep(scalar(-learningRate));\n    this.epsilonScalar = keep(scalar(epsilon));\n    this.rhoScalar = keep(scalar(rho));\n    this.oneMinusRho = keep(scalar(1 - rho));\n  }\n\n  applyGradients(variableGradients: NamedVariableMap) {\n    for (const variableName in variableGradients) {\n      const value = ENV.engine.registeredVariables[variableName];\n      if (this.accumulatedGrads[variableName] == null) {\n        const trainable = false;\n        tidy(() => {\n          this.accumulatedGrads[variableName] =\n              zerosLike(value).variable(trainable);\n        });\n      }\n      if (this.accumulatedUpdates[variableName] == null) {\n        const trainable = false;\n        tidy(() => {\n          this.accumulatedUpdates[variableName] =\n              zerosLike(value).variable(trainable);\n        });\n      }\n\n      const gradient = variableGradients[variableName];\n      const accumulatedGrad = this.accumulatedGrads[variableName];\n      const accumulatedUpdate = this.accumulatedUpdates[variableName];\n\n      tidy(() => {\n        const newAccumulatedGrad =\n            this.rhoScalar.mul(accumulatedGrad)\n                .add(this.oneMinusRho.mul(gradient.square()));\n\n        const updates = accumulatedUpdate.add(this.epsilonScalar)\n                            .sqrt()\n                            .div(accumulatedGrad.add(this.epsilonScalar).sqrt())\n                            .mul(gradient);\n\n        const newAccumulatedUpdate =\n            this.rhoScalar.mul(accumulatedUpdate)\n                .add(this.oneMinusRho.mul(updates.square()));\n\n        this.accumulatedGrads[variableName].assign(newAccumulatedGrad);\n        this.accumulatedUpdates[variableName].assign(newAccumulatedUpdate);\n\n        const newValue = this.c.mul(updates).add(value);\n        value.assign(newValue);\n      });\n    }\n  }\n\n  dispose() {\n    this.c.dispose();\n    this.epsilonScalar.dispose();\n    this.rhoScalar.dispose();\n    this.oneMinusRho.dispose();\n    if (this.accumulatedUpdates != null) {\n      Object.keys(this.accumulatedUpdates)\n          .forEach(name => this.accumulatedUpdates[name].dispose());\n      Object.keys(this.accumulatedGrads)\n          .forEach(name => this.accumulatedGrads[name].dispose());\n    }\n  }\n  getConfig(): ConfigDict {\n    return {\n      learningRate: this.learningRate,\n      rho: this.rho,\n      epsilon: this.epsilon\n    };\n  }\n  static fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(config.learningRate, config.rho, config.epsilon);\n  }\n}\nSerializationMap.register(AdadeltaOptimizer);\n"]}},"hash":"91e4893880ac1f265b845b3a4f9d8139","cacheData":{"env":{}}}