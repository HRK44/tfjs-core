{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/package.json","includedInParent":true,"mtime":1528810356568},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1525096773813},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1528810356568},{"name":"../doc","loc":{"line":9,"column":20}},{"name":"../environment","loc":{"line":10,"column":28}},{"name":"../types","loc":{"line":11,"column":22}},{"name":"../util","loc":{"line":12,"column":19}},{"name":"./broadcast_util","loc":{"line":13,"column":29}},{"name":"./operation","loc":{"line":14,"column":26}},{"name":"./ops","loc":{"line":15,"column":20}}],"generated":{"js":"\"use strict\";\nvar __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar doc_1 = require(\"../doc\");\nvar environment_1 = require(\"../environment\");\nvar types_1 = require(\"../types\");\nvar util = require(\"../util\");\nvar broadcast_util = require(\"./broadcast_util\");\nvar operation_1 = require(\"./operation\");\nvar ops_1 = require(\"./ops\");\nvar BinaryOps = (function () {\n    function BinaryOps() {\n    }\n    BinaryOps.add = function (a, b) {\n        util.assertArgumentsAreTensors({ a: a, b: b }, 'add');\n        util.assertTypesMatch(a, b);\n        var outShape = broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        var der = function (dy) {\n            var derA = function () {\n                var res = dy;\n                var reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    res = res.sum(reduceAxes);\n                }\n                return res.reshape(a.shape);\n            };\n            var derB = function () {\n                var res = dy;\n                var reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    res = res.sum(reduceAxes);\n                }\n                return res.reshape(b.shape);\n            };\n            return { a: derA, b: derB };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.add(a, b); }, { a: a, b: b }, der);\n    };\n    BinaryOps.addStrict = function (a, b) {\n        util.assertShapesMatch(a.shape, b.shape, 'Error in addStrict: ');\n        return a.add(b);\n    };\n    BinaryOps.sub = function (a, b) {\n        util.assertArgumentsAreTensors({ a: a, b: b }, 'sub');\n        util.assertTypesMatch(a, b);\n        var outShape = broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        var der = function (dy) {\n            var derA = function () {\n                var res = dy;\n                var reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    res = res.sum(reduceAxes);\n                }\n                return res.reshape(a.shape);\n            };\n            var derB = function () {\n                var res = dy;\n                var reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    res = res.sum(reduceAxes);\n                }\n                return res.neg().reshape(b.shape);\n            };\n            return { a: derA, b: derB };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.subtract(a, b); }, { a: a, b: b }, der);\n    };\n    BinaryOps.subStrict = function (a, b) {\n        util.assertShapesMatch(a.shape, b.shape, 'Error in subStrict: ');\n        return a.sub(b);\n    };\n    BinaryOps.pow = function (base, exp) {\n        util.assertArgumentsAreTensors({ base: base, exp: exp }, 'pow');\n        var outShape = broadcast_util.assertAndGetBroadcastShape(base.shape, exp.shape);\n        base = base.cast(types_1.upcastType(base.dtype, exp.dtype));\n        exp = exp.cast(types_1.upcastType(base.dtype, exp.dtype));\n        var grad = function (dy, saved) {\n            var y = saved[0];\n            var derBase = function () {\n                var res = dy.mul(exp.toFloat().mul(y.div(base)));\n                var reduceAxes = broadcast_util.getReductionAxes(base.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    res = res.sum(reduceAxes);\n                }\n                return res.reshape(base.shape);\n            };\n            var derExp = function () {\n                var res = dy.mul(y.mul(base.log()).toFloat());\n                var reduceAxes = broadcast_util.getReductionAxes(exp.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    res = res.sum(reduceAxes);\n                }\n                return res.reshape(exp.shape);\n            };\n            return { base: derBase, exp: derExp };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend, save) { return save(backend.pow(base, exp)); }, { base: base, exp: exp }, grad);\n    };\n    BinaryOps.powStrict = function (base, exp) {\n        util.assertShapesMatch(base.shape, exp.shape, 'Error in powStrict: ');\n        return base.pow(exp);\n    };\n    BinaryOps.mul = function (a, b) {\n        util.assertArgumentsAreTensors({ a: a, b: b }, 'mul');\n        util.assertTypesMatch(a, b);\n        var outShape = broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        var der = function (dy) {\n            var derA = function () {\n                var res = dy.mul(b.toFloat());\n                var reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    return res.sum(reduceAxes).reshape(a.shape);\n                }\n                return res;\n            };\n            var derB = function () {\n                var res = dy.mul(a.toFloat());\n                var reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    return res.sum(reduceAxes).reshape(b.shape);\n                }\n                return res;\n            };\n            return { a: derA, b: derB };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.multiply(a, b); }, { a: a, b: b }, der);\n    };\n    BinaryOps.mulStrict = function (a, b) {\n        util.assertShapesMatch(a.shape, b.shape, 'Error in multiplyStrict: ');\n        return a.mul(b);\n    };\n    BinaryOps.div = function (a, b) {\n        util.assertArgumentsAreTensors({ a: a, b: b }, 'div');\n        util.assertTypesMatch(a, b);\n        var forwardFunc;\n        if (a.dtype === 'int32' && b.dtype === 'int32') {\n            return BinaryOps.floorDiv(a, b);\n        }\n        else {\n            forwardFunc = function (backend) { return backend.realDivide(a, b); };\n        }\n        var outShape = broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        var der = function (dy) {\n            var derA = function () {\n                var res = dy.div(b.toFloat());\n                var reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    return res.sum(reduceAxes).reshape(a.shape);\n                }\n                return res;\n            };\n            var derB = function () {\n                var res = dy.mul(a.toFloat());\n                var reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    res = res.sum(reduceAxes).reshape(b.shape);\n                }\n                var tmp = b.square();\n                return res.div(tmp.toFloat()).neg();\n            };\n            return { a: derA, b: derB };\n        };\n        return environment_1.ENV.engine.runKernel(forwardFunc, { a: a, b: b }, der);\n    };\n    BinaryOps.floorDiv = function (a, b) {\n        util.assertArgumentsAreTensors({ a: a, b: b }, 'floorDiv');\n        util.assertTypesMatch(a, b);\n        var forwardFunc = function (backend) { return backend.floorDiv(a, b); };\n        var outShape = broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        var der = function (dy) {\n            var derA = function () {\n                var res = dy.div(b.toFloat());\n                var reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    return res.sum(reduceAxes).reshape(a.shape);\n                }\n                return res;\n            };\n            var derB = function () {\n                var res = dy.mul(a.toFloat());\n                var reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    res = res.sum(reduceAxes).reshape(b.shape);\n                }\n                var tmp = b.square();\n                return res.div(tmp.toFloat()).neg();\n            };\n            return { a: derA, b: derB };\n        };\n        return environment_1.ENV.engine.runKernel(forwardFunc, { a: a, b: b }, der);\n    };\n    BinaryOps.divStrict = function (a, b) {\n        util.assertShapesMatch(a.shape, b.shape, 'Error in divideStrict: ');\n        return a.div(b);\n    };\n    BinaryOps.mod = function (a, b) {\n        util.assertArgumentsAreTensors({ a: a, b: b }, 'mod');\n        util.assertTypesMatch(a, b);\n        var outShape = broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        var der = function (dy) {\n            var derA = function () {\n                var reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    return dy.sum(reduceAxes).reshape(a.shape);\n                }\n                return dy;\n            };\n            var derB = function () {\n                var res = dy.mul(a.div(b).floor().neg());\n                var reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    return res.sum(reduceAxes).reshape(b.shape);\n                }\n                return res;\n            };\n            return { a: derA, b: derB };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.mod(a, b); }, { a: a, b: b }, der);\n    };\n    BinaryOps.modStrict = function (a, b) {\n        util.assertShapesMatch(a.shape, b.shape, 'Error in modStrict: ');\n        return a.mod(b);\n    };\n    BinaryOps.minimum = function (a, b) {\n        util.assertArgumentsAreTensors({ a: a, b: b }, 'minimum');\n        util.assertTypesMatch(a, b);\n        if (a.dtype === 'bool') {\n            a = a.toInt();\n        }\n        if (b.dtype === 'bool') {\n            b = b.toInt();\n        }\n        broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        var der = function (dy) {\n            var derA = function () { return dy.mul(a.lessEqual(b).toFloat()); };\n            var derB = function () { return dy.mul(a.greater(b).toFloat()); };\n            return { a: derA, b: derB };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.minimum(a, b); }, { a: a, b: b }, der);\n    };\n    BinaryOps.minimumStrict = function (a, b) {\n        util.assertShapesMatch(a.shape, b.shape, 'Error in minimumStrict: ');\n        return a.minimum(b);\n    };\n    BinaryOps.maximum = function (a, b) {\n        util.assertArgumentsAreTensors({ a: a, b: b }, 'maximum');\n        util.assertTypesMatch(a, b);\n        if (a.dtype === 'bool') {\n            a = a.toInt();\n        }\n        if (b.dtype === 'bool') {\n            b = b.toInt();\n        }\n        broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        var der = function (dy) {\n            var derA = function () { return dy.mul(a.greaterEqual(b).toFloat()); };\n            var derB = function () { return dy.mul(a.less(b).toFloat()); };\n            return { a: derA, b: derB };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.maximum(a, b); }, { a: a, b: b }, der);\n    };\n    BinaryOps.maximumStrict = function (a, b) {\n        util.assertShapesMatch(a.shape, b.shape, 'Error in minimumStrict: ');\n        return a.maximum(b);\n    };\n    BinaryOps.squaredDifference = function (a, b) {\n        util.assertArgumentsAreTensors({ a: a, b: b }, 'squaredDifference');\n        util.assertTypesMatch(a, b);\n        broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        var der = function (dy) {\n            var two = ops_1.scalar(2);\n            var derA = function () { return dy.mul(a.sub(b).mul(two)); };\n            var derB = function () { return dy.mul(b.sub(a).mul(two)); };\n            return { a: derA, b: derB };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.squaredDifference(a, b); }, { a: a, b: b }, der);\n    };\n    BinaryOps.squaredDifferenceStrict = function (a, b) {\n        util.assertShapesMatch(a.shape, b.shape, 'Error in squaredDifferenceStrict: ');\n        return a.squaredDifference(b);\n    };\n    BinaryOps.atan2 = function (a, b) {\n        util.assertArgumentsAreTensors({ a: a, b: b }, 'atan2');\n        util.assertTypesMatch(a, b);\n        var outShape = broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        var der = function (dy) {\n            var derA = function () {\n                var d = BinaryOps.add(ops_1.square(a), ops_1.square(b));\n                var res = dy.mul(b.div(d));\n                var reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    res = res.sum(reduceAxes);\n                }\n                return res.reshape(a.shape);\n            };\n            var derB = function () {\n                var d = BinaryOps.add(ops_1.square(a), ops_1.square(b));\n                var res = ops_1.neg(dy.mul(a.div(d)));\n                var reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n                if (reduceAxes.length > 0) {\n                    res = res.sum(reduceAxes);\n                }\n                return res.reshape(b.shape);\n            };\n            return { a: derA, b: derB };\n        };\n        return environment_1.ENV.engine.runKernel(function (backend) { return backend.atan2(a, b); }, { a: a, b: b }, der);\n    };\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Arithmetic' }),\n        operation_1.operation\n    ], BinaryOps, \"add\", null);\n    __decorate([\n        operation_1.operation\n    ], BinaryOps, \"addStrict\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Arithmetic' }),\n        operation_1.operation\n    ], BinaryOps, \"sub\", null);\n    __decorate([\n        operation_1.operation\n    ], BinaryOps, \"subStrict\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Arithmetic' }),\n        operation_1.operation\n    ], BinaryOps, \"pow\", null);\n    __decorate([\n        operation_1.operation\n    ], BinaryOps, \"powStrict\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Arithmetic' }),\n        operation_1.operation\n    ], BinaryOps, \"mul\", null);\n    __decorate([\n        operation_1.operation\n    ], BinaryOps, \"mulStrict\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Arithmetic' }),\n        operation_1.operation\n    ], BinaryOps, \"div\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Arithmetic' }),\n        operation_1.operation\n    ], BinaryOps, \"floorDiv\", null);\n    __decorate([\n        operation_1.operation\n    ], BinaryOps, \"divStrict\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Arithmetic' }),\n        operation_1.operation\n    ], BinaryOps, \"mod\", null);\n    __decorate([\n        operation_1.operation\n    ], BinaryOps, \"modStrict\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Arithmetic' }),\n        operation_1.operation\n    ], BinaryOps, \"minimum\", null);\n    __decorate([\n        operation_1.operation\n    ], BinaryOps, \"minimumStrict\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Arithmetic' }),\n        operation_1.operation\n    ], BinaryOps, \"maximum\", null);\n    __decorate([\n        operation_1.operation\n    ], BinaryOps, \"maximumStrict\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Arithmetic' }),\n        operation_1.operation\n    ], BinaryOps, \"squaredDifference\", null);\n    __decorate([\n        operation_1.operation\n    ], BinaryOps, \"squaredDifferenceStrict\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Basic math' }),\n        operation_1.operation\n    ], BinaryOps, \"atan2\", null);\n    return BinaryOps;\n}());\nexports.BinaryOps = BinaryOps;\n","map":{"version":3,"file":"binary_ops.js","sourceRoot":"","sources":["../src/ops/binary_ops.ts"],"names":[],"mappings":";;;;;;;;AAiBA,8BAA2B;AAC3B,8CAAmC;AAEnC,kCAAoC;AACpC,8BAAgC;AAEhC,iDAAmD;AACnD,yCAAsC;AACtC,6BAA0C;AAG1C;IAAA;IA2sBA,CAAC;IAjrBQ,aAAG,GAAV,UAA6B,CAAS,EAAE,CAAS;QAC/C,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAC9C,IAAI,CAAC,gBAAgB,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAE5B,IAAM,QAAQ,GACV,cAAc,CAAC,0BAA0B,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QAEhE,IAAM,GAAG,GAAG,UAAC,EAAU;YACrB,IAAM,IAAI,GAAG;gBACX,IAAI,GAAG,GAAG,EAAE,CAAC;gBACb,IAAM,UAAU,GAAG,cAAc,CAAC,gBAAgB,CAAC,CAAC,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBACtE,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,GAAG,GAAG,GAAG,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC;gBAC5B,CAAC;gBACD,MAAM,CAAC,GAAG,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;YAC9B,CAAC,CAAC;YACF,IAAM,IAAI,GAAG;gBACX,IAAI,GAAG,GAAG,EAAE,CAAC;gBACb,IAAM,UAAU,GAAG,cAAc,CAAC,gBAAgB,CAAC,CAAC,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBACtE,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,GAAG,GAAG,GAAG,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC;gBAC5B,CAAC;gBACD,MAAM,CAAC,GAAG,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;YAC9B,CAAC,CAAC;YACF,MAAM,CAAC,EAAC,CAAC,EAAE,IAAI,EAAE,CAAC,EAAE,IAAI,EAAC,CAAC;QAC5B,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,EAAjB,CAAiB,EAAE,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,GAAG,CAAM,CAAC;IAC9E,CAAC;IAWM,mBAAS,GAAhB,UAAmC,CAAI,EAAE,CAAI;QAC3C,IAAI,CAAC,iBAAiB,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,EAAE,sBAAsB,CAAC,CAAC;QACjE,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;IAClB,CAAC;IA4BM,aAAG,GAAV,UAA6B,CAAS,EAAE,CAAS;QAC/C,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAC9C,IAAI,CAAC,gBAAgB,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAE5B,IAAM,QAAQ,GACV,cAAc,CAAC,0BAA0B,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QAEhE,IAAM,GAAG,GAAG,UAAC,EAAU;YACrB,IAAM,IAAI,GAAG;gBACX,IAAI,GAAG,GAAG,EAAE,CAAC;gBACb,IAAM,UAAU,GAAG,cAAc,CAAC,gBAAgB,CAAC,CAAC,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBACtE,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,GAAG,GAAG,GAAG,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC;gBAC5B,CAAC;gBACD,MAAM,CAAC,GAAG,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;YAC9B,CAAC,CAAC;YACF,IAAM,IAAI,GAAG;gBACX,IAAI,GAAG,GAAG,EAAE,CAAC;gBACb,IAAM,UAAU,GAAG,cAAc,CAAC,gBAAgB,CAAC,CAAC,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBACtE,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,GAAG,GAAG,GAAG,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC;gBAC5B,CAAC;gBACD,MAAM,CAAC,GAAG,CAAC,GAAG,EAAE,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;YACpC,CAAC,CAAC;YACF,MAAM,CAAC,EAAC,CAAC,EAAE,IAAI,EAAE,CAAC,EAAE,IAAI,EAAC,CAAC;QAC5B,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAChB,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,QAAQ,CAAC,CAAC,EAAE,CAAC,CAAC,EAAtB,CAAsB,EAAE,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,GAAG,CAAM,CAAC;IAClE,CAAC;IAYM,mBAAS,GAAhB,UAAmC,CAAI,EAAE,CAAI;QAC3C,IAAI,CAAC,iBAAiB,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,EAAE,sBAAsB,CAAC,CAAC;QACjE,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;IAClB,CAAC;IA8BM,aAAG,GAAV,UAA6B,IAAO,EAAE,GAAW;QAC/C,IAAI,CAAC,yBAAyB,CAAC,EAAC,IAAI,MAAA,EAAE,GAAG,KAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAEnD,IAAM,QAAQ,GACV,cAAc,CAAC,0BAA0B,CAAC,IAAI,CAAC,KAAK,EAAE,GAAG,CAAC,KAAK,CAAC,CAAC;QACrE,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC,kBAAU,CAAC,IAAI,CAAC,KAAK,EAAE,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC;QACpD,GAAG,GAAG,GAAG,CAAC,IAAI,CAAC,kBAAU,CAAC,IAAI,CAAC,KAAK,EAAE,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC;QAClD,IAAM,IAAI,GAAG,UAAC,EAAU,EAAE,KAAe;YAChC,IAAA,YAAC,CAAU;YAClB,IAAM,OAAO,GAAG;gBACd,IAAI,GAAG,GAAG,EAAE,CAAC,GAAG,CAAC,GAAG,CAAC,OAAO,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;gBACjD,IAAM,UAAU,GACZ,cAAc,CAAC,gBAAgB,CAAC,IAAI,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBAC1D,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,GAAG,GAAG,GAAG,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC;gBAC5B,CAAC;gBACD,MAAM,CAAC,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAM,CAAC;YACtC,CAAC,CAAC;YACF,IAAM,MAAM,GAAG;gBACb,IAAI,GAAG,GAAG,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,GAAG,EAAE,CAAC,CAAC,OAAO,EAAE,CAAC,CAAC;gBAC9C,IAAM,UAAU,GAAG,cAAc,CAAC,gBAAgB,CAAC,GAAG,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBACxE,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,GAAG,GAAG,GAAG,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC;gBAC5B,CAAC;gBACD,MAAM,CAAC,GAAG,CAAC,OAAO,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC;YAChC,CAAC,CAAC;YACF,MAAM,CAAC,EAAC,IAAI,EAAE,OAAO,EAAE,GAAG,EAAE,MAAM,EAAC,CAAC;QACtC,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAChB,UAAC,OAAO,EAAE,IAAI,IAAK,OAAA,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,IAAI,EAAE,GAAG,CAAC,CAAC,EAA5B,CAA4B,EAAE,EAAC,IAAI,MAAA,EAAE,GAAG,KAAA,EAAC,EAC5D,IAAI,CAAM,CAAC;IACxB,CAAC;IAYM,mBAAS,GAAhB,UAAmC,IAAO,EAAE,GAAW;QACrD,IAAI,CAAC,iBAAiB,CAAC,IAAI,CAAC,KAAK,EAAE,GAAG,CAAC,KAAK,EAAE,sBAAsB,CAAC,CAAC;QACtE,MAAM,CAAC,IAAI,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC;IACvB,CAAC;IA2BM,aAAG,GAAV,UAA6B,CAAS,EAAE,CAAS;QAC/C,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAC9C,IAAI,CAAC,gBAAgB,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAE5B,IAAM,QAAQ,GACV,cAAc,CAAC,0BAA0B,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QAEhE,IAAM,GAAG,GAAG,UAAC,EAAU;YACrB,IAAM,IAAI,GAAG;gBACX,IAAM,GAAG,GAAG,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,CAAC;gBAChC,IAAM,UAAU,GAAG,cAAc,CAAC,gBAAgB,CAAC,CAAC,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBACtE,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,MAAM,CAAC,GAAG,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;gBAC9C,CAAC;gBACD,MAAM,CAAC,GAAG,CAAC;YACb,CAAC,CAAC;YACF,IAAM,IAAI,GAAG;gBACX,IAAM,GAAG,GAAG,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,CAAC;gBAChC,IAAM,UAAU,GAAG,cAAc,CAAC,gBAAgB,CAAC,CAAC,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBACtE,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,MAAM,CAAC,GAAG,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;gBAC9C,CAAC;gBACD,MAAM,CAAC,GAAG,CAAC;YACb,CAAC,CAAC;YACF,MAAM,CAAC,EAAC,CAAC,EAAE,IAAI,EAAE,CAAC,EAAE,IAAI,EAAC,CAAC;QAC5B,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAChB,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,QAAQ,CAAC,CAAC,EAAE,CAAC,CAAC,EAAtB,CAAsB,EAAE,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,GAAG,CAAM,CAAC;IAClE,CAAC;IAYM,mBAAS,GAAhB,UAAmC,CAAI,EAAE,CAAI;QAC3C,IAAI,CAAC,iBAAiB,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,EAAE,2BAA2B,CAAC,CAAC;QACtE,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAM,CAAC;IACvB,CAAC;IA6BM,aAAG,GAAV,UAA6B,CAAS,EAAE,CAAS;QAC/C,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAC9C,IAAI,CAAC,gBAAgB,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAE5B,IAAI,WAA+C,CAAC;QACpD,EAAE,CAAC,CAAC,CAAC,CAAC,KAAK,KAAK,OAAO,IAAI,CAAC,CAAC,KAAK,KAAK,OAAO,CAAC,CAAC,CAAC;YAC/C,MAAM,CAAC,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAClC,CAAC;QAAC,IAAI,CAAC,CAAC;YACN,WAAW,GAAG,UAAC,OAAsB,IAAK,OAAA,OAAO,CAAC,UAAU,CAAC,CAAC,EAAE,CAAC,CAAC,EAAxB,CAAwB,CAAC;QACrE,CAAC;QAED,IAAM,QAAQ,GACV,cAAc,CAAC,0BAA0B,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QAChE,IAAM,GAAG,GAAG,UAAC,EAAU;YACrB,IAAM,IAAI,GAAG;gBACX,IAAM,GAAG,GAAG,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,CAAC;gBAChC,IAAM,UAAU,GAAG,cAAc,CAAC,gBAAgB,CAAC,CAAC,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBACtE,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,MAAM,CAAC,GAAG,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;gBAC9C,CAAC;gBACD,MAAM,CAAC,GAAG,CAAC;YACb,CAAC,CAAC;YACF,IAAM,IAAI,GAAG;gBACX,IAAI,GAAG,GAAG,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,CAAC;gBAC9B,IAAM,UAAU,GAAG,cAAc,CAAC,gBAAgB,CAAC,CAAC,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBACtE,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,GAAG,GAAG,GAAG,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;gBAC7C,CAAC;gBACD,IAAM,GAAG,GAAG,CAAC,CAAC,MAAM,EAAY,CAAC;gBACjC,MAAM,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,OAAO,EAAE,CAAC,CAAC,GAAG,EAAY,CAAC;YAChD,CAAC,CAAC;YACF,MAAM,CAAC,EAAC,CAAC,EAAE,IAAI,EAAE,CAAC,EAAE,IAAI,EAAC,CAAC;QAC5B,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,WAAW,EAAE,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,GAAG,CAC/C,CAAC;IACR,CAAC;IA4BM,kBAAQ,GAAf,UAAkC,CAAS,EAAE,CAAS;QACpD,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,UAAU,CAAC,CAAC;QACnD,IAAI,CAAC,gBAAgB,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAE5B,IAAM,WAAW,GAAG,UAAC,OAAsB,IAAK,OAAA,OAAO,CAAC,QAAQ,CAAC,CAAC,EAAE,CAAC,CAAC,EAAtB,CAAsB,CAAC;QACvE,IAAM,QAAQ,GACV,cAAc,CAAC,0BAA0B,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QAChE,IAAM,GAAG,GAAG,UAAC,EAAU;YACrB,IAAM,IAAI,GAAG;gBACX,IAAM,GAAG,GAAG,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,CAAC;gBAChC,IAAM,UAAU,GAAG,cAAc,CAAC,gBAAgB,CAAC,CAAC,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBACtE,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,MAAM,CAAC,GAAG,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;gBAC9C,CAAC;gBACD,MAAM,CAAC,GAAG,CAAC;YACb,CAAC,CAAC;YACF,IAAM,IAAI,GAAG;gBACX,IAAI,GAAG,GAAG,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,CAAC;gBAC9B,IAAM,UAAU,GAAG,cAAc,CAAC,gBAAgB,CAAC,CAAC,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBACtE,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,GAAG,GAAG,GAAG,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;gBAC7C,CAAC;gBACD,IAAM,GAAG,GAAG,CAAC,CAAC,MAAM,EAAY,CAAC;gBACjC,MAAM,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,OAAO,EAAE,CAAC,CAAC,GAAG,EAAY,CAAC;YAChD,CAAC,CAAC;YACF,MAAM,CAAC,EAAC,CAAC,EAAE,IAAI,EAAE,CAAC,EAAE,IAAI,EAAC,CAAC;QAC5B,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,WAAW,EAAE,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,GAAG,CAC/C,CAAC;IACR,CAAC;IAUM,mBAAS,GAAhB,UAAmC,CAAI,EAAE,CAAI;QAC3C,IAAI,CAAC,iBAAiB,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,EAAE,yBAAyB,CAAC,CAAC;QACpE,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAM,CAAC;IACvB,CAAC;IA8BM,aAAG,GAAV,UAA6B,CAAS,EAAE,CAAS;QAC/C,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,KAAK,CAAC,CAAC;QAC9C,IAAI,CAAC,gBAAgB,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAE5B,IAAM,QAAQ,GACV,cAAc,CAAC,0BAA0B,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QAChE,IAAM,GAAG,GAAG,UAAC,EAAU;YACrB,IAAM,IAAI,GAAG;gBACX,IAAM,UAAU,GAAG,cAAc,CAAC,gBAAgB,CAAC,CAAC,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBACtE,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,MAAM,CAAC,EAAE,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;gBAC7C,CAAC;gBACD,MAAM,CAAC,EAAE,CAAC;YACZ,CAAC,CAAC;YACF,IAAM,IAAI,GAAG;gBACX,IAAM,GAAG,GAAG,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,GAAG,EAAE,CAAC,CAAC;gBAC3C,IAAM,UAAU,GAAG,cAAc,CAAC,gBAAgB,CAAC,CAAC,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBACtE,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,MAAM,CAAC,GAAG,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;gBAC9C,CAAC;gBACD,MAAM,CAAC,GAAG,CAAC;YACb,CAAC,CAAC;YACF,MAAM,CAAC,EAAC,CAAC,EAAE,IAAI,EAAE,CAAC,EAAE,IAAI,EAAC,CAAC;QAC5B,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,EAAjB,CAAiB,EAAE,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,GAAG,CAAM,CAAC;IAC9E,CAAC;IAUM,mBAAS,GAAhB,UAAmC,CAAI,EAAE,CAAI;QAC3C,IAAI,CAAC,iBAAiB,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,EAAE,sBAAsB,CAAC,CAAC;QACjE,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;IAClB,CAAC;IA6BM,iBAAO,GAAd,UAAiC,CAAS,EAAE,CAAS;QACnD,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,SAAS,CAAC,CAAC;QAClD,IAAI,CAAC,gBAAgB,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAE5B,EAAE,CAAC,CAAC,CAAC,CAAC,KAAK,KAAK,MAAM,CAAC,CAAC,CAAC;YACvB,CAAC,GAAG,CAAC,CAAC,KAAK,EAAE,CAAC;QAChB,CAAC;QACD,EAAE,CAAC,CAAC,CAAC,CAAC,KAAK,KAAK,MAAM,CAAC,CAAC,CAAC;YACvB,CAAC,GAAG,CAAC,CAAC,KAAK,EAAE,CAAC;QAChB,CAAC;QACD,cAAc,CAAC,0BAA0B,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QAC5D,IAAM,GAAG,GAAG,UAAC,EAAU;YACrB,IAAM,IAAI,GAAG,cAAM,OAAA,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,EAAhC,CAAgC,CAAC;YACpD,IAAM,IAAI,GAAG,cAAM,OAAA,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,EAA9B,CAA8B,CAAC;YAClD,MAAM,CAAC,EAAC,CAAC,EAAE,IAAI,EAAE,CAAC,EAAE,IAAI,EAAC,CAAC;QAC5B,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAChB,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,OAAO,CAAC,CAAC,EAAE,CAAC,CAAC,EAArB,CAAqB,EAAE,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,GAAG,CAAM,CAAC;IACjE,CAAC;IAUM,uBAAa,GAApB,UAAuC,CAAI,EAAE,CAAI;QAC/C,IAAI,CAAC,iBAAiB,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,EAAE,0BAA0B,CAAC,CAAC;QACrE,MAAM,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;IACtB,CAAC;IA6BM,iBAAO,GAAd,UAAiC,CAAS,EAAE,CAAS;QACnD,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,SAAS,CAAC,CAAC;QAClD,IAAI,CAAC,gBAAgB,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAE5B,EAAE,CAAC,CAAC,CAAC,CAAC,KAAK,KAAK,MAAM,CAAC,CAAC,CAAC;YACvB,CAAC,GAAG,CAAC,CAAC,KAAK,EAAE,CAAC;QAChB,CAAC;QACD,EAAE,CAAC,CAAC,CAAC,CAAC,KAAK,KAAK,MAAM,CAAC,CAAC,CAAC;YACvB,CAAC,GAAG,CAAC,CAAC,KAAK,EAAE,CAAC;QAChB,CAAC;QACD,cAAc,CAAC,0BAA0B,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QAC5D,IAAM,GAAG,GAAG,UAAC,EAAU;YACrB,IAAM,IAAI,GAAG,cAAM,OAAA,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,EAAnC,CAAmC,CAAC;YACvD,IAAM,IAAI,GAAG,cAAM,OAAA,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC,EAA3B,CAA2B,CAAC;YAC/C,MAAM,CAAC,EAAC,CAAC,EAAE,IAAI,EAAE,CAAC,EAAE,IAAI,EAAC,CAAC;QAC5B,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAChB,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,OAAO,CAAC,CAAC,EAAE,CAAC,CAAC,EAArB,CAAqB,EAAE,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,GAAG,CAAM,CAAC;IACjE,CAAC;IAUM,uBAAa,GAApB,UAAuC,CAAI,EAAE,CAAI;QAC/C,IAAI,CAAC,iBAAiB,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,EAAE,0BAA0B,CAAC,CAAC;QACrE,MAAM,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;IACtB,CAAC;IA8BM,2BAAiB,GAAxB,UAA2C,CAAS,EAAE,CAAS;QAC7D,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,mBAAmB,CAAC,CAAC;QAC5D,IAAI,CAAC,gBAAgB,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAE5B,cAAc,CAAC,0BAA0B,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QAC5D,IAAM,GAAG,GAAG,UAAC,EAAU;YACrB,IAAM,GAAG,GAAG,YAAM,CAAC,CAAC,CAAC,CAAC;YACtB,IAAM,IAAI,GAAG,cAAM,OAAA,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC,EAAzB,CAAyB,CAAC;YAC7C,IAAM,IAAI,GAAG,cAAM,OAAA,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC,EAAzB,CAAyB,CAAC;YAC7C,MAAM,CAAC,EAAC,CAAC,EAAE,IAAI,EAAE,CAAC,EAAE,IAAI,EAAC,CAAC;QAC5B,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAChB,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,iBAAiB,CAAC,CAAC,EAAE,CAAC,CAAC,EAA/B,CAA+B,EAAE,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,GAAG,CAAM,CAAC;IAC3E,CAAC;IAYM,iCAAuB,GAA9B,UAAiD,CAAI,EAAE,CAAI;QACzD,IAAI,CAAC,iBAAiB,CAClB,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,EAAE,oCAAoC,CAAC,CAAC;QAC5D,MAAM,CAAC,CAAC,CAAC,iBAAiB,CAAC,CAAC,CAAC,CAAC;IAChC,CAAC;IAmBM,eAAK,GAAZ,UAA+B,CAAS,EAAE,CAAS;QACjD,IAAI,CAAC,yBAAyB,CAAC,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,OAAO,CAAC,CAAC;QAChD,IAAI,CAAC,gBAAgB,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAE5B,IAAM,QAAQ,GACV,cAAc,CAAC,0BAA0B,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QAEhE,IAAM,GAAG,GAAG,UAAC,EAAU;YACrB,IAAM,IAAI,GAAG;gBACX,IAAM,CAAC,GAAG,SAAS,CAAC,GAAG,CAAC,YAAM,CAAC,CAAC,CAAC,EAAE,YAAM,CAAC,CAAC,CAAC,CAAC,CAAC;gBAC9C,IAAI,GAAG,GAAG,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC;gBAC3B,IAAM,UAAU,GAAG,cAAc,CAAC,gBAAgB,CAAC,CAAC,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBACtE,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,GAAG,GAAG,GAAG,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC;gBAC5B,CAAC;gBACD,MAAM,CAAC,GAAG,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;YAC9B,CAAC,CAAC;YACF,IAAM,IAAI,GAAG;gBACX,IAAM,CAAC,GAAG,SAAS,CAAC,GAAG,CAAC,YAAM,CAAC,CAAC,CAAC,EAAE,YAAM,CAAC,CAAC,CAAC,CAAM,CAAC;gBACnD,IAAI,GAAG,GAAG,SAAG,CAAC,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;gBAChC,IAAM,UAAU,GAAG,cAAc,CAAC,gBAAgB,CAAC,CAAC,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;gBACtE,EAAE,CAAC,CAAC,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;oBAC1B,GAAG,GAAG,GAAG,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC;gBAC5B,CAAC;gBACD,MAAM,CAAC,GAAG,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;YAC9B,CAAC,CAAC;YACF,MAAM,CAAC,EAAC,CAAC,EAAE,IAAI,EAAE,CAAC,EAAE,IAAI,EAAC,CAAC;QAC5B,CAAC,CAAC;QACF,MAAM,CAAC,iBAAG,CAAC,MAAM,CAAC,SAAS,CAAC,UAAA,OAAO,IAAI,OAAA,OAAO,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,EAAnB,CAAmB,EAAE,EAAC,CAAC,GAAA,EAAE,CAAC,GAAA,EAAC,EAAE,GAAG,CAClE,CAAC;IACR,CAAC;IAhrBD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BA4BT;IAWD;QADC,qBAAS;oCAIT;IA4BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BA6BT;IAYD;QADC,qBAAS;oCAIT;IA8BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BAgCT;IAYD;QADC,qBAAS;oCAIT;IA2BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BA6BT;IAYD;QADC,qBAAS;oCAIT;IA6BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BAoCT;IA4BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;mCA8BT;IAUD;QADC,qBAAS;oCAIT;IA8BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;8BA0BT;IAUD;QADC,qBAAS;oCAIT;IA6BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;kCAmBT;IAUD;QADC,qBAAS;wCAIT;IA6BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;kCAmBT;IAUD;QADC,qBAAS;wCAIT;IA8BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;4CAcT;IAYD;QADC,qBAAS;kDAKT;IAmBD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,YAAY,EAAC,CAAC;QACtD,qBAAS;gCA+BT;IACH,gBAAC;CAAA,AA3sBD,IA2sBC;AA3sBY,8BAAS","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {doc} from '../doc';\nimport {ENV} from '../environment';\nimport {Tensor} from '../tensor';\nimport {upcastType} from '../types';\nimport * as util from '../util';\n\nimport * as broadcast_util from './broadcast_util';\nimport {operation} from './operation';\nimport {neg, scalar, square} from './ops';\nimport {KernelBackend} from '../kernels/backend';\n\nexport class BinaryOps {\n  /**\n   * Adds two `Tensor`s element-wise, A + B. Supports broadcasting.\n   *\n   * We also expose `addStrict` which has the same signature as this op and\n   * asserts that `a` and `b` are the same shape (does not broadcast).\n   *\n   * ```js\n   * const a = tf.tensor1d([1, 2, 3, 4]);\n   * const b = tf.tensor1d([10, 20, 30, 40]);\n   *\n   * a.add(b).print();  // or tf.add(a, b)\n   * ```\n   *\n   * ```js\n   * // Broadcast add a with b.\n   * const a = tf.scalar(5);\n   * const b = tf.tensor1d([10, 20, 30, 40]);\n   *\n   * a.add(b).print();  // or tf.add(a, b)\n   * ```\n   * @param a The first `Tensor` to add.\n   * @param b The second `Tensor` to add. Must have the same type as `a`.\n   */\n  @doc({heading: 'Operations', subheading: 'Arithmetic'})\n  @operation\n  static add<T extends Tensor>(a: Tensor, b: Tensor): T {\n    util.assertArgumentsAreTensors({a, b}, 'add');\n    util.assertTypesMatch(a, b);\n\n    const outShape =\n        broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n\n    const der = (dy: Tensor) => {\n      const derA = () => {\n        let res = dy;\n        const reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n        if (reduceAxes.length > 0) {\n          res = res.sum(reduceAxes);\n        }\n        return res.reshape(a.shape);\n      };\n      const derB = () => {\n        let res = dy;\n        const reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n        if (reduceAxes.length > 0) {\n          res = res.sum(reduceAxes);\n        }\n        return res.reshape(b.shape);\n      };\n      return {a: derA, b: derB};\n    };\n    return ENV.engine.runKernel(backend => backend.add(a, b), {a, b}, der) as T;\n  }\n\n  /**\n   * Adds two `Tensor`s element-wise, A + B.\n   *\n   * Inputs must be the same shape. For broadcasting support, use add() instead.\n   *\n   * @param a The first Tensor to add element-wise.\n   * @param b The second Tensor to add element-wise.\n   */\n  @operation\n  static addStrict<T extends Tensor>(a: T, b: T): T {\n    util.assertShapesMatch(a.shape, b.shape, 'Error in addStrict: ');\n    return a.add(b);\n  }\n\n  /**\n   * Subtracts two `Tensor`s element-wise, A - B. Supports broadcasting.\n   *\n   * We also expose `subStrict` which has the same signature as this op and\n   * asserts that `a` and `b` are the same shape (does not broadcast).\n   *\n   * ```js\n   * const a = tf.tensor1d([10, 20, 30, 40]);\n   * const b = tf.tensor1d([1, 2, 3, 4]);\n   *\n   * a.sub(b).print();  // or tf.sub(a, b)\n   * ```\n   *\n   * ```js\n   * // Broadcast subtract a with b.\n   * const a = tf.tensor1d([10, 20, 30, 40]);\n   * const b = tf.scalar(5);\n   *\n   * a.sub(b).print();  // or tf.sub(a, b)\n   * ```\n   * @param a The first `Tensor` to subtract from.\n   * @param b The second `Tensor` to be subtracted. Must have the same dtype as\n   * `a`.\n   */\n  @doc({heading: 'Operations', subheading: 'Arithmetic'})\n  @operation\n  static sub<T extends Tensor>(a: Tensor, b: Tensor): T {\n    util.assertArgumentsAreTensors({a, b}, 'sub');\n    util.assertTypesMatch(a, b);\n\n    const outShape =\n        broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n\n    const der = (dy: Tensor) => {\n      const derA = () => {\n        let res = dy;\n        const reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n        if (reduceAxes.length > 0) {\n          res = res.sum(reduceAxes);\n        }\n        return res.reshape(a.shape);\n      };\n      const derB = () => {\n        let res = dy;\n        const reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n        if (reduceAxes.length > 0) {\n          res = res.sum(reduceAxes);\n        }\n        return res.neg().reshape(b.shape);\n      };\n      return {a: derA, b: derB};\n    };\n    return ENV.engine.runKernel(\n               backend => backend.subtract(a, b), {a, b}, der) as T;\n  }\n\n  /**\n   * Subtracts two `Tensor`s element-wise, A - B. Inputs must\n   * be the same shape.\n   *\n   * For broadcasting support, use sub() instead.\n   *\n   * @param a The first Tensor to subtract element-wise.\n   * @param b The second Tensor to subtract element-wise.\n   */\n  @operation\n  static subStrict<T extends Tensor>(a: T, b: T): T {\n    util.assertShapesMatch(a.shape, b.shape, 'Error in subStrict: ');\n    return a.sub(b);\n  }\n\n  /**\n   * Computes the power of one `Tensor` to another. Supports broadcasting.\n   *\n   * Given a `Tensor` x and a `Tensor` y, this operation computes x^y for\n   * corresponding elements in x and y. The result's dtype will be the upcasted\n   * type of the `base` and `exp` dtypes.\n   *\n   * ```js\n   * const a = tf.tensor([[2, 3], [4, 5]])\n   * const b = tf.tensor([[1, 2], [3, 0]]).toInt();\n   *\n   * a.pow(b).print();  // or tf.pow(a, b)\n   * ```\n   *\n   * ```js\n   * const a = tf.tensor([[1, 2], [3, 4]])\n   * const b = tf.tensor(2).toInt();\n   *\n   * a.pow(b).print();  // or tf.pow(a, b)\n   * ```\n   * We also expose `powStrict` which has the same signature as this op and\n   * asserts that `base` and `exp` are the same shape (does not broadcast).\n   *\n   * @param base The base `Tensor` to pow element-wise.\n   * @param exp The exponent `Tensor` to pow element-wise.\n   */\n  @doc({heading: 'Operations', subheading: 'Arithmetic'})\n  @operation\n  static pow<T extends Tensor>(base: T, exp: Tensor): T {\n    util.assertArgumentsAreTensors({base, exp}, 'pow');\n\n    const outShape =\n        broadcast_util.assertAndGetBroadcastShape(base.shape, exp.shape);\n    base = base.cast(upcastType(base.dtype, exp.dtype));\n    exp = exp.cast(upcastType(base.dtype, exp.dtype));\n    const grad = (dy: Tensor, saved: Tensor[]) => {\n      const [y] = saved;\n      const derBase = () => {\n        let res = dy.mul(exp.toFloat().mul(y.div(base)));\n        const reduceAxes =\n            broadcast_util.getReductionAxes(base.shape, outShape);\n        if (reduceAxes.length > 0) {\n          res = res.sum(reduceAxes);\n        }\n        return res.reshape(base.shape) as T;\n      };\n      const derExp = () => {\n        let res = dy.mul(y.mul(base.log()).toFloat());\n        const reduceAxes = broadcast_util.getReductionAxes(exp.shape, outShape);\n        if (reduceAxes.length > 0) {\n          res = res.sum(reduceAxes);\n        }\n        return res.reshape(exp.shape);\n      };\n      return {base: derBase, exp: derExp};\n    };\n    return ENV.engine.runKernel(\n               (backend, save) => save(backend.pow(base, exp)), {base, exp},\n               grad) as T;\n  }\n\n  /**\n   * Computes the power of one `Tensor` to another. Inputs must\n   * be the same shape.\n   *\n   * For broadcasting support, use pow() instead.\n   *\n   * @param base The base tensor to pow element-wise.\n   * @param exp The exponent tensor to pow element-wise.\n   */\n  @operation\n  static powStrict<T extends Tensor>(base: T, exp: Tensor): T {\n    util.assertShapesMatch(base.shape, exp.shape, 'Error in powStrict: ');\n    return base.pow(exp);\n  }\n\n  /**\n   * Multiplies two `Tensor`s element-wise, A * B. Supports broadcasting.\n   *\n   * We also expose `mulStrict` which has the same signature as this op and\n   * asserts that `a` and `b` are the same shape (does not broadcast).\n   *\n   * ```js\n   * const a = tf.tensor1d([1, 2, 3, 4]);\n   * const b = tf.tensor1d([2, 3, 4, 5]);\n   *\n   * a.mul(b).print();  // or tf.mul(a, b)\n   * ```\n   *\n   * ```js\n   * // Broadcast mul a with b.\n   * const a = tf.tensor1d([1, 2, 3, 4]);\n   * const b = tf.scalar(5);\n   *\n   * a.mul(b).print();  // or tf.mul(a, b)\n   * ```\n   * @param a The first tensor to multiply.\n   * @param b The second tensor to multiply. Must have the same dtype as `a`.\n   */\n  @doc({heading: 'Operations', subheading: 'Arithmetic'})\n  @operation\n  static mul<T extends Tensor>(a: Tensor, b: Tensor): T {\n    util.assertArgumentsAreTensors({a, b}, 'mul');\n    util.assertTypesMatch(a, b);\n\n    const outShape =\n        broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n\n    const der = (dy: Tensor) => {\n      const derA = () => {\n        const res = dy.mul(b.toFloat());\n        const reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n        if (reduceAxes.length > 0) {\n          return res.sum(reduceAxes).reshape(a.shape);\n        }\n        return res;\n      };\n      const derB = () => {\n        const res = dy.mul(a.toFloat());\n        const reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n        if (reduceAxes.length > 0) {\n          return res.sum(reduceAxes).reshape(b.shape);\n        }\n        return res;\n      };\n      return {a: derA, b: derB};\n    };\n    return ENV.engine.runKernel(\n               backend => backend.multiply(a, b), {a, b}, der) as T;\n  }\n\n  /**\n   * Multiplies two `Tensor`s element-wise, A * B.\n   *\n   * Inputs must be the same shape. For broadcasting support, use mul().\n   *\n   * @param a The first tensor to multiply.\n   * @param b The first tensor to multiply. Must have the same\n   *    dtype as `a`.\n   */\n  @operation\n  static mulStrict<T extends Tensor>(a: T, b: T): T {\n    util.assertShapesMatch(a.shape, b.shape, 'Error in multiplyStrict: ');\n    return a.mul(b) as T;\n  }\n\n  /**\n   * Divides two `Tensor`s element-wise, A / B. Supports broadcasting.\n   *\n   * We also expose `divStrict` which has the same signature as this op and\n   * asserts that `a` and `b` are the same shape (does not broadcast).\n   *\n   * ```js\n   * const a = tf.tensor1d([1, 4, 9, 16]);\n   * const b = tf.tensor1d([1, 2, 3, 4]);\n   *\n   * a.div(b).print();  // or tf.div(a, b)\n   * ```\n   *\n   * ```js\n   * // Broadcast div a with b.\n   * const a = tf.tensor1d([2, 4, 6, 8]);\n   * const b = tf.scalar(2);\n   *\n   * a.div(b).print();  // or tf.div(a, b)\n   * ```\n   *\n   * @param a The first tensor as the numerator.\n   * @param b The second tensor as the denominator. Must have the same dtype as\n   * `a`.\n   */\n  @doc({heading: 'Operations', subheading: 'Arithmetic'})\n  @operation\n  static div<T extends Tensor>(a: Tensor, b: Tensor): T {\n    util.assertArgumentsAreTensors({a, b}, 'div');\n    util.assertTypesMatch(a, b);\n\n    let forwardFunc: (backend: KernelBackend) => Tensor;\n    if (a.dtype === 'int32' && b.dtype === 'int32') {\n      return BinaryOps.floorDiv(a, b);\n    } else {\n      forwardFunc = (backend: KernelBackend) => backend.realDivide(a, b);\n    }\n\n    const outShape =\n        broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n    const der = (dy: Tensor) => {\n      const derA = () => {\n        const res = dy.div(b.toFloat());\n        const reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n        if (reduceAxes.length > 0) {\n          return res.sum(reduceAxes).reshape(a.shape);\n        }\n        return res;\n      };\n      const derB = () => {\n        let res = dy.mul(a.toFloat());\n        const reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n        if (reduceAxes.length > 0) {\n          res = res.sum(reduceAxes).reshape(b.shape);\n        }\n        const tmp = b.square() as Tensor;\n        return res.div(tmp.toFloat()).neg() as Tensor;\n      };\n      return {a: derA, b: derB};\n    };\n    return ENV.engine.runKernel(forwardFunc, {a, b}, der) as\n        T;\n  }\n\n  /**\n   * Divides two `Tensor`s element-wise, A / B. Supports broadcasting.\n   * The result is rounded with floor function.\n   *\n   *\n   * ```js\n   * const a = tf.tensor1d([1, 4, 9, 16]);\n   * const b = tf.tensor1d([1, 2, 3, 4]);\n   *\n   * a.floorDiv(b).print();  // or tf.div(a, b)\n   * ```\n   *\n   * ```js\n   * // Broadcast div a with b.\n   * const a = tf.tensor1d([2, 4, 6, 8]);\n   * const b = tf.scalar(2);\n   *\n   * a.floorDiv(b).print();  // or tf.floorDiv(a, b)\n   * ```\n   *\n   * @param a The first tensor as the numerator.\n   * @param b The second tensor as the denominator. Must have the same dtype as\n   * `a`.\n   */\n  @doc({heading: 'Operations', subheading: 'Arithmetic'})\n  @operation\n  static floorDiv<T extends Tensor>(a: Tensor, b: Tensor): T {\n    util.assertArgumentsAreTensors({a, b}, 'floorDiv');\n    util.assertTypesMatch(a, b);\n\n    const forwardFunc = (backend: KernelBackend) => backend.floorDiv(a, b);\n    const outShape =\n        broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n    const der = (dy: Tensor) => {\n      const derA = () => {\n        const res = dy.div(b.toFloat());\n        const reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n        if (reduceAxes.length > 0) {\n          return res.sum(reduceAxes).reshape(a.shape);\n        }\n        return res;\n      };\n      const derB = () => {\n        let res = dy.mul(a.toFloat());\n        const reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n        if (reduceAxes.length > 0) {\n          res = res.sum(reduceAxes).reshape(b.shape);\n        }\n        const tmp = b.square() as Tensor;\n        return res.div(tmp.toFloat()).neg() as Tensor;\n      };\n      return {a: derA, b: derB};\n    };\n    return ENV.engine.runKernel(forwardFunc, {a, b}, der) as\n        T;\n  }\n\n  /**\n   * Divides two `Tensor`s element-wise, A / B. Inputs must\n   * be the same shape.\n   *\n   * @param a The first tensor as the numerator for element-wise division.\n   * @param b The second tensor as the denominator for element-wise division.\n   */\n  @operation\n  static divStrict<T extends Tensor>(a: T, b: T): T {\n    util.assertShapesMatch(a.shape, b.shape, 'Error in divideStrict: ');\n    return a.div(b) as T;\n  }\n\n  /**\n   * Returns the mod of a and b element-wise.\n   * `floor(x / y) * y + mod(x, y) = x`\n   * Supports broadcasting.\n   *\n   * We also expose `modStrict` which has the same signature as this op and\n   * asserts that `a` and `b` are the same shape (does not broadcast).\n   *\n   * ```js\n   * const a = tf.tensor1d([1, 4, 3, 16]);\n   * const b = tf.tensor1d([1, 2, 9, 4]);\n   *\n   * a.mod(b).print();  // or tf.mod(a, b)\n   * ```\n   *\n   * ```js\n   * // Broadcast a mod b.\n   * const a = tf.tensor1d([2, 4, 6, 8]);\n   * const b = tf.scalar(5);\n   *\n   * a.mod(b).print();  // or tf.mod(a, b)\n   * ```\n   *\n   * @param a The first tensor.\n   * @param b The second tensor. Must have the same type as `a`.\n   */\n  @doc({heading: 'Operations', subheading: 'Arithmetic'})\n  @operation\n  static mod<T extends Tensor>(a: Tensor, b: Tensor): T {\n    util.assertArgumentsAreTensors({a, b}, 'mod');\n    util.assertTypesMatch(a, b);\n\n    const outShape =\n        broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n    const der = (dy: Tensor) => {\n      const derA = () => {\n        const reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n        if (reduceAxes.length > 0) {\n          return dy.sum(reduceAxes).reshape(a.shape);\n        }\n        return dy;\n      };\n      const derB = () => {\n        const res = dy.mul(a.div(b).floor().neg());\n        const reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n        if (reduceAxes.length > 0) {\n          return res.sum(reduceAxes).reshape(b.shape);\n        }\n        return res;\n      };\n      return {a: derA, b: derB};\n    };\n    return ENV.engine.runKernel(backend => backend.mod(a, b), {a, b}, der) as T;\n  }\n\n  /**\n   * Returns the mod of a and b (`a < b ? a : b`) element-wise. Inputs must\n   * be the same shape. For broadcasting support, use mod().\n   *\n   * @param a The first tensor.\n   * @param b The second tensor. Must have the same dtype as `a`.\n   */\n  @operation\n  static modStrict<T extends Tensor>(a: T, b: T): T {\n    util.assertShapesMatch(a.shape, b.shape, 'Error in modStrict: ');\n    return a.mod(b);\n  }\n\n  /**\n   * Returns the min of a and b (`a < b ? a : b`) element-wise.\n   * Supports broadcasting.\n   *\n   * We also expose `minimumStrict` which has the same signature as this op and\n   * asserts that `a` and `b` are the same shape (does not broadcast).\n   *\n   * ```js\n   * const a = tf.tensor1d([1, 4, 3, 16]);\n   * const b = tf.tensor1d([1, 2, 9, 4]);\n   *\n   * a.minimum(b).print();  // or tf.minimum(a, b)\n   * ```\n   *\n   * ```js\n   * // Broadcast minimum a with b.\n   * const a = tf.tensor1d([2, 4, 6, 8]);\n   * const b = tf.scalar(5);\n   *\n   * a.minimum(b).print();  // or tf.minimum(a, b)\n   * ```\n   *\n   * @param a The first tensor.\n   * @param b The second tensor. Must have the same type as `a`.\n   */\n  @doc({heading: 'Operations', subheading: 'Arithmetic'})\n  @operation\n  static minimum<T extends Tensor>(a: Tensor, b: Tensor): T {\n    util.assertArgumentsAreTensors({a, b}, 'minimum');\n    util.assertTypesMatch(a, b);\n\n    if (a.dtype === 'bool') {\n      a = a.toInt();\n    }\n    if (b.dtype === 'bool') {\n      b = b.toInt();\n    }\n    broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n    const der = (dy: Tensor) => {\n      const derA = () => dy.mul(a.lessEqual(b).toFloat());\n      const derB = () => dy.mul(a.greater(b).toFloat());\n      return {a: derA, b: derB};\n    };\n    return ENV.engine.runKernel(\n               backend => backend.minimum(a, b), {a, b}, der) as T;\n  }\n\n  /**\n   * Returns the min of a and b (`a < b ? a : b`) element-wise. Inputs must\n   * be the same shape. For broadcasting support, use minimum().\n   *\n   * @param a The first tensor.\n   * @param b The second tensor. Must have the same dtype as `a`.\n   */\n  @operation\n  static minimumStrict<T extends Tensor>(a: T, b: T): T {\n    util.assertShapesMatch(a.shape, b.shape, 'Error in minimumStrict: ');\n    return a.minimum(b);\n  }\n\n  /**\n   * Returns the max of a and b (`a > b ? a : b`) element-wise.\n   * Supports broadcasting.\n   *\n   * We also expose `maximumStrict` which has the same signature as this op and\n   * asserts that `a` and `b` are the same shape (does not broadcast).\n   *\n   * ```js\n   * const a = tf.tensor1d([1, 4, 3, 16]);\n   * const b = tf.tensor1d([1, 2, 9, 4]);\n   *\n   * a.maximum(b).print();  // or tf.maximum(a, b)\n   * ```\n   *\n   * ```js\n   * // Broadcast maximum a with b.\n   * const a = tf.tensor1d([2, 4, 6, 8]);\n   * const b = tf.scalar(5);\n   *\n   * a.maximum(b).print();  // or tf.maximum(a, b)\n   * ```\n   *\n   * @param a The first tensor.\n   * @param b The second tensor. Must have the same type as `a`.\n   */\n  @doc({heading: 'Operations', subheading: 'Arithmetic'})\n  @operation\n  static maximum<T extends Tensor>(a: Tensor, b: Tensor): T {\n    util.assertArgumentsAreTensors({a, b}, 'maximum');\n    util.assertTypesMatch(a, b);\n\n    if (a.dtype === 'bool') {\n      a = a.toInt();\n    }\n    if (b.dtype === 'bool') {\n      b = b.toInt();\n    }\n    broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n    const der = (dy: Tensor) => {\n      const derA = () => dy.mul(a.greaterEqual(b).toFloat());\n      const derB = () => dy.mul(a.less(b).toFloat());\n      return {a: derA, b: derB};\n    };\n    return ENV.engine.runKernel(\n               backend => backend.maximum(a, b), {a, b}, der) as T;\n  }\n\n  /**\n   * Returns the max of a and b (`a > b ? a : b`) element-wise. Inputs must\n   * be the same shape. For broadcasting support, use maximum().\n   *\n   * @param a The first tensor.\n   * @param b The second tensor. Must have the same dtype as `a`.\n   */\n  @operation\n  static maximumStrict<T extends Tensor>(a: T, b: T): T {\n    util.assertShapesMatch(a.shape, b.shape, 'Error in minimumStrict: ');\n    return a.maximum(b);\n  }\n\n  /**\n   * Returns (a - b) * (a - b) element-wise.\n   * Supports broadcasting.\n   *\n   * We also expose `squaredDifferenceStrict` which has the same signature as\n   * this op and asserts that `a` and `b` are the same shape (does not\n   * broadcast).\n   *\n   * ```js\n   * const a = tf.tensor1d([1, 4, 3, 16]);\n   * const b = tf.tensor1d([1, 2, 9, 4]);\n   *\n   * a.squaredDifference(b).print();  // or tf.squaredDifference(a, b)\n   * ```\n   *\n   * ```js\n   * // Broadcast squared difference  a with b.\n   * const a = tf.tensor1d([2, 4, 6, 8]);\n   * const b = tf.scalar(5);\n   *\n   * a.squaredDifference(b).print();  // or tf.squaredDifference(a, b)\n   * ```\n   *\n   * @param a The first tensor.\n   * @param b The second tensor. Must have the same type as `a`.\n   */\n  @doc({heading: 'Operations', subheading: 'Arithmetic'})\n  @operation\n  static squaredDifference<T extends Tensor>(a: Tensor, b: Tensor): T {\n    util.assertArgumentsAreTensors({a, b}, 'squaredDifference');\n    util.assertTypesMatch(a, b);\n\n    broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n    const der = (dy: Tensor) => {\n      const two = scalar(2);\n      const derA = () => dy.mul(a.sub(b).mul(two));\n      const derB = () => dy.mul(b.sub(a).mul(two));\n      return {a: derA, b: derB};\n    };\n    return ENV.engine.runKernel(\n               backend => backend.squaredDifference(a, b), {a, b}, der) as T;\n  }\n\n  /**\n   * Returns (a - b) * (a - b) element-wise.\n   *\n   * Inputs must be the same shape. For broadcasting support, use\n   * squaredDifference() instead.\n   *\n   * @param a The first tensor.\n   * @param b The second tensor. Must have the same type as `a`.\n   */\n  @operation\n  static squaredDifferenceStrict<T extends Tensor>(a: T, b: T): T {\n    util.assertShapesMatch(\n        a.shape, b.shape, 'Error in squaredDifferenceStrict: ');\n    return a.squaredDifference(b);\n  }\n\n  /*\n   * Computes arctangent of `Tensor`s a / b element-wise: `atan2(a, b)`.\n   * Supports broadcasting.\n   *\n   * ```js\n   * const a = tf.tensor1d([1.0, 1.0, -1.0, .7]);\n   * const b = tf.tensor1d([2.0, 13.0, 3.5, .21]);\n   *\n   * tf.atan2(a, b).print()\n   *```\n   *\n   * @param a The first tensor.\n   * @param b The second tensor. Must have the same dtype as `a`.\n   *\n   */\n  @doc({heading: 'Operations', subheading: 'Basic math'})\n  @operation\n  static atan2<T extends Tensor>(a: Tensor, b: Tensor): T {\n    util.assertArgumentsAreTensors({a, b}, 'atan2');\n    util.assertTypesMatch(a, b);\n\n    const outShape =\n        broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n\n    const der = (dy: Tensor) => {\n      const derA = () => {\n        const d = BinaryOps.add(square(a), square(b));\n        let res = dy.mul(b.div(d));\n        const reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n        if (reduceAxes.length > 0) {\n          res = res.sum(reduceAxes);\n        }\n        return res.reshape(a.shape);\n      };\n      const derB = () => {\n        const d = BinaryOps.add(square(a), square(b)) as T;\n        let res = neg(dy.mul(a.div(d)));\n        const reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n        if (reduceAxes.length > 0) {\n          res = res.sum(reduceAxes);\n        }\n        return res.reshape(b.shape);\n      };\n      return {a: derA, b: derB};\n    };\n    return ENV.engine.runKernel(backend => backend.atan2(a, b), {a, b}, der) as\n        T;\n  }\n}\n"]}},"hash":"1acf669de509380d6bae970c3379ef44","cacheData":{"env":{}}}